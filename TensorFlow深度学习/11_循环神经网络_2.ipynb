{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 导入matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 导入TF相关模块\n",
    "from tensorflow.keras import layers, losses, optimizers, Sequential\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 循环神经网络\n",
    "卷积神经网络利用数据的局部相关性和权值共享的思想大大减少了网络的参数量，非常适合于图片这种具有空间(Spatial)局部相关性的数据。\n",
    "\n",
    "自然界的信号除了具有空间维度之外，还有一个时间(Temporal)维度，比如文本、语音信号、股市参数等。这类数据并不一定具有局部相关性，同时数据在时间维度上的长度也是可变的，卷积神经网络并不擅长处理此类数据。\n",
    "\n",
    "本章介绍的循环神经网络可以较好地解决此类问题。\n",
    "\n",
    "### 11.7 RNN短时记忆\n",
    "循环神经网络除了训练困难，还有一个更严重的问题，那就是`短时记忆`(Short-term memory)。考虑一个长句子：\n",
    "\n",
    "> 今天天气太美好了，尽管路上发生了一件不愉快的事情，…，我马上调整好状态，开开心心地准备迎接美好的一天。\n",
    "\n",
    "根据我们的理解，之所以能够`开开心心地准备迎接美好的一天`，在于句子最开始处点名了`今天天气太美好了`。可见人类是能够很好地理解长句子的，但是，循环神经网络在处理较长的句子时，往往只能够理解有限长度内的信息，而对于位于较长范围类的有用信息往往不能够很好的利用起来。我们把这种现象叫做`短时记忆`。\n",
    "\n",
    "针对这个问题，1997年，`Jürgen Schmidhuber`提出了`长短时记忆网络`(Long Short-Term Memory，简称`LSTM`)。LSTM相对于基础的RNN网络，记忆能力更强，更擅长处理较长的序列信号数据。LSTM广泛应用在序列预测、NLP等任务中，几乎取代了基础的RNN模型。\n",
    "\n",
    "\n",
    "## 11.8 LSTM原理\n",
    "基础的RNN网络结构如`图11.13`所示，上一个时间戳的状态向量$h_{t-1}$与当前时间戳的输入$x_t$经过线性变换后，通过激活函数`tanh`后得到新的状态向量$h_t$。相对于基础的RNN网络只有一个状态向量$h_t$ ，`LSTM`新增了一个状态向量$C_t$，同时引入了门控(Gate)机制，通过门控单元来控制信息的遗忘和刷新，如`图11.14`所示。\n",
    "\n",
    "<img src=\"images/11_14.png\" style=\"width:450px;\"/>\n",
    "\n",
    "在`LSTM`中，有两个状态向量$c$和$h$，其中$c$作为`LSTM`的内部状态向量，可以理解为`LSTM`的内存状态向量`Memory`，而$h$表示`LSTM`的输出向量。相对于基础的RNN来说，`LSTM`把内部`Memory`和输出分开为两个变量，同时利用三个门控：\n",
    "+ 输入门(Input Gate)\n",
    "+ 遗忘门(Forget Gate)\n",
    "+ 输出门(Output Gate)\n",
    "\n",
    "控制内部信息的流动。\n",
    "\n",
    "门控机制可以理解为控制数据流通量的一种手段，类比于水阀门：当水阀门全部打开时，水流畅通无阻地通过；当水阀门全部关闭时，水流完全被隔断。在`LSTM`中，阀门开和程度利用门控值向量$g$表示，如`图11.15`所示。\n",
    "\n",
    "<img src=\"images/11_15.png\" style=\"width:250px;\"/>\n",
    "\n",
    "通过$\\sigma{(g)}$激活函数将门控制压缩到$[0,1]$之间区间：\n",
    "+ 当$\\sigma{(g)} = 0$时，门控全部关闭，输出$o = 0$\n",
    "+ 当$\\sigma{(g)} = 1$时，门控全部打开，输出$o = x$\n",
    "\n",
    "通过门控机制可以较好地控制数据的流量程度。\n",
    "\n",
    "### 11.8.1 遗忘门\n",
    "遗忘门作用于`LSTM`状态向量$c$上面，用于控制上一个时间戳的记忆$c_{t-1}$对当前时间戳的影响。遗忘门的控制变量$g_{f}$由\n",
    "+ $g_f = \\sigma(W_f[h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "产生，如`图11.16`所示。\n",
    "\n",
    "<img src=\"images/11_16.png\" style=\"width:350px;\"/>\n",
    "\n",
    "其中$W_f$和$b_f$为遗忘门的参数张量，可由反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数：\n",
    "+ 当门控$g_{f} = 1$时，遗忘门全部打开，`LSTM`接受上一个状态$c_{t-1}$的所有信息\n",
    "+ 当门控$g_{f} = 0$时，遗忘门关闭，`LSTM`直接忽略$c_{t-1}$，输出为0的向量\n",
    "\n",
    "这也是遗忘门的名字由来。\n",
    "\n",
    "经过遗忘门后，`LSTM`的状态向量变为$g_fc_{t−1}$。\n",
    "\n",
    "### 11.8.2 输入门\n",
    "输入门用于控制`LSTM`对输入的接收程度。首先通过对当前时间戳的输入$x_t$和上一个时间戳的输出$h_{t-1}$做非线性变换得到新的输入向量$\\widetilde{c}_t$:\n",
    "+ $\\widetilde{c}_t = \\tanh{(W_c[h_{t-1}, x_t] + b_c)}$\n",
    "\n",
    "其中$W_c$和$b_c$为输入门的参数，需要通过反向传播算法自动优化，`tanh`为激活函数，用于将输入标准化到$[−1,1]$区间。$\\widetilde{c}_t$并不会全部刷新进入`LSTM`的`Memory`，而是通过输入门控制接受输入的量。输入门的控制变量同样来自于输入$x_t$和输出$h_{t-1}$：\n",
    "+ $g_i = \\sigma(W_i[h_{t-1}, x_t] + b_i)$\n",
    "\n",
    "其中$W_i$和$b_i$为输入门的参数，需要通过反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数。\n",
    "\n",
    "输入门控制变量$g_i$决定了`LSTM`对当前时间戳的新输入$\\widetilde{c}_t$的接受程度，如`图11.17`所示：\n",
    "\n",
    "<img src=\"images/11_17.png\" style=\"width:350px;\"/>\n",
    "\n",
    "+ 当$g_i = 0$时，`LSTM`不接受任何的新输入$\\widetilde{c}_t$\n",
    "+ 当$g_i = 1$时，`LSTM`全部接受新输入$\\widetilde{c}_t$\n",
    "\n",
    "经过输入门后，待写入`Memory`的向量为$g_i\\widetilde{c}_t$。\n",
    "\n",
    "### 11.8.3 刷新Memory\n",
    "在遗忘门和输入门的控制下，`LSTM`有选择地读取了上一个时间戳的记忆$c_{t-1}$和当前时间戳的新输入$\\widetilde{c}_t$，状态向量$c_t$的刷新方式为：\n",
    "+ $c_t = g_i\\widetilde{c}_t + g_fc_{t-1}$\n",
    "\n",
    "得到的新状态向量$c_t$即为当前时间戳的状态向量，如`图11.17`所示。\n",
    "\n",
    "### 11.8.4 输出门\n",
    "`LSTM`的内部状态向量$c_{t}$并不会直接用于输出，这一点和基础的RNN不一样。基础的RNN网络的状态向量既用于记忆，又用于输出，所以基础的RNN可以理解为状态向量$c$和输出向量$h$是同一个对象。在`LSTM`内部，状态向量并不会全部输出，而是在输出门的作用下有选择地输出。输出门的门控变量$g_o$为：\n",
    "+ $g_o = \\sigma(W_O[h_{t-1},x_t]+b_o)$\n",
    "\n",
    "其中$W_O$和$b_o$为输出门的参数，同样需要通过反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数：\n",
    "+ 当输出门$g_o=0$时，输出关闭，`LSTM`的内部记忆完全被隔断，无法用作输出，此时输出为0的向量\n",
    "+ 当输出门$g_o=1$时，输出完全打开，`LSTM`的状态向量$c_t$全部用于输出\n",
    "\n",
    "`LSTM`的输出由：\n",
    "+ $h_t = g_o\\cdot\\tanh(c_t)$\n",
    "\n",
    "产生，即内存向量$c_t$经过`tanh`激活函数后与输入门作用，得到`LSTM`的输出。由于$g_o \\in [0,1]，\\tanh(c_t) \\in [−1,1]$，因此`LSTM`的输出$t \\in [−1,1]$。\n",
    "\n",
    "<img src=\"images/11_18.png\" style=\"width:350px;\"/>\n",
    "\n",
    "### 11.8.5 小结\n",
    "`LSTM`虽然状态向量和门控数量较多，计算流程相对复杂。但是由于每个门控功能清晰明确，每个状态的作用也比较好理解。这里将典型的门控行为列举出来，并解释其代码的`LSTM`行为，如`表11.1`所示。\n",
    "\n",
    "<img src=\"images/t_11_01.png\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.9 LSTM层使用方法\n",
    "`TensorFlow`有两种方式实现`LSTM`网络：\n",
    "+ 使用`LSTMCell`手动完成时间戳上面的循环运算\n",
    "+ 通过`LSTM`层方式一步完成前向运算\n",
    "\n",
    "### 11.9.1 LSTMCell\n",
    "`LSTMCell`的用法和`SimpleRNNCell`基本一致，区别在于`LSTM`的状态变量有两 个，即$[h_t,c_t]$，其中$h_t$为`cell`的输出，$c_t$为`cell`的更新后的状态。\n",
    "\n",
    "首先新建一个状态向量长度$h=64$的`LSTMCell`，其中状态向量$c_t$和输出向量$h_t$的长度都为$h$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140247601251888, 140247601251888, 140247601252560)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([2,80,100])\n",
    "# 得到一个时间戳的输入\n",
    "xt = x[:,0,:] \n",
    "# 创建LSTM Cell\n",
    "cell = layers.LSTMCell(64) \n",
    "# 初始化状态和输出\n",
    "state = [tf.zeros([2,64]),tf.zeros([2,64])]\n",
    "# 前向计算\n",
    "out, state = cell(xt, state) \n",
    "# 查看返回元素的 id\n",
    "id(out),id(state[0]),id(state[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，返回的输出`out`和$c_t$的`id`是相同的，这与基础的`RNN`初衷一致，都是为了格式的统一。 通过在时间戳上展开循环运算，即可完成一次层的前向传播，写法与基础的`RNN`一样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在序列长度维度上解开，循环送入LSTMCell单元\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    # 前向计算\n",
    "    out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出可以仅使用最后一个时间戳上的输出，也可以聚合所有时间戳上的输出向量。\n",
    "\n",
    "### 11.9.2 LSTM层\n",
    "\n",
    "通过`layers.LSTM`层可以方便的一次完成整个序列的运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一层LSTM层，内存向量长度为64\n",
    "layer = layers.LSTM(64)\n",
    "# 序列通过LSTM层，默认返回最后一个时间戳的输出h\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过`LSTM`层前向传播后，默认只会返回最后一个时间戳的输出，如果需要返回每个时间戳上面的输出，需要设置`return_sequences=True`标志："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 80, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建LSTM层时，设置返回每个时间戳上的输出\n",
    "layer = layers.LSTM(64, return_sequences=True)\n",
    "# 前向计算，每个时间戳上的输出自动进行了concat，拼成一个张量\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`out`的`shape`是$[2,80,64]$，包含了80个时间戳。\n",
    "\n",
    "对于多层神经网络，可以通过`Sequential`容器包裹多层`LSTM`层，并设置所有非末层网络`return_sequences=True`，这是因为非末层的`LSTM`层需要上一层在所有时间戳的输出作为输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = keras.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "    layers.LSTM(64)\n",
    "])\n",
    "\n",
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.10 GRU简介\n",
    "`LSTM`具有更长的记忆能力，不容易出现梯度弥散现象，在大部分序列任务上面都取得了比基础的RNN模型更好的性能表现。但是`LSTM`结构相对较复杂，计算代价较高，模型参数量较大。因此，科学家们尝试简化`LSTM`内部的计算流程，特别是减少门控数量。研究发现，遗忘门是`LSTM`中最重要的门控，甚至发现只有遗忘门的简化版网络在多个基准数据集上面优于标准`LSTM`网络。\n",
    "\n",
    "在众多的简化版`LSTM`中，`门控循环网络`(Gated Recurrent Unit，简称`GRU`)应用最广泛。`GRU`把内部状态向量和输出向量合并，统一为状态向量，门控数量也减少到2个：`复位门`(Reset Gate)和`更新门`(Update Gate)，如`图11.19`：\n",
    "\n",
    "<img src=\"images/11_19.png\" style=\"width:300px;\"/>\n",
    "\n",
    "### 11.10.1 复位门\n",
    "复位门用于控制上一个时间戳的状态$h_{t-1}$进入`GRU`的量。门控向量$g_r$由当前时间戳输入$x_t$和上一时间戳状态$h_{t-1}$变换得到，关系如下：\n",
    "+ $g_r = \\sigma(W_r[h_{t-1},x_t]+b_r)$\n",
    "\n",
    "其中$W_r$和$b_r$为复位门的参数，由反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数。门控向量$g_r$只控制状态$h_{t-1}$，而不会控制输入$x_t$：\n",
    "+  $\\tilde{h_t} = \\tanh(W_h[g_rh_{t-1},x_t]+b_h)$\n",
    "\n",
    "当$g_r = 0$时，新输入$\\tilde{h_t}$全部来自于输入$x_t$，不接受$h_{t-1}$，此时相当于复位$h_{t-1}$。当$g_r = 1$时，$h_{t-1}$和输入$x_t$共同产生新输入$\\tilde{h_t}$，如`图11.20`所示。\n",
    "\n",
    "<img src=\"images/11_20.png\" style=\"width:350px;\"/>\n",
    "\n",
    "### 11.10.2 更新门\n",
    "更新门用控制上一时间戳状态$h_{t-1}$和新输入$\\tilde{h_t}$对新状态向量$h_t$的影响程度。更新门控向量$g_z$由\n",
    "+ $g_z = \\sigma(W_z[h_{t-1}, x_t] + b_z)$\n",
    "\n",
    "得到，其中$W_z$和$b_z$为更新门的参数，由反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数。$g_z$用与控制新输入$\\tilde{h_t}$信号，$1-g_z$用于控制状态$h_{t-1}$信号：\n",
    "+ $h_t = (1-g_z)h_{t-1}+g_z\\tilde{h_t}$\n",
    "\n",
    "可以看到，$\\tilde{h_t}$和$h_{t-1}$对$h_t$的更新量处于相互竞争、此消彼长的状态：\n",
    "+ 当更新门$g_z=0$时，$h_t$全部来自上一时间戳状态$h_{t-1}$\n",
    "+ 当更新门$g_z = 1$时，$h_t$全部来自新输入$\\tilde{h_t}$\n",
    "\n",
    "<img src=\"images/11_21.png\" style=\"width:350px;\"/>\n",
    "\n",
    "### 11.10.3 GRU使用方法\n",
    "`TensorFlow`中，也有`Cell`方式和层方式实现`GRU`网络。`GRUCell`和`SimpleRNNCell`非常类似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化状态向量，GRU只有一个\n",
    "h = [tf.zeros([2,64])]\n",
    "# 新建GRU Cell，向量长度为64\n",
    "cell = layers.GRUCell(64)\n",
    "# 在时间戳维度上解开，循环通过cell\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, h = cell(xt, h)\n",
    "# 输出形状\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`layers.GRU`类可以方便创建一层`GRU`网络层，通过`Sequential`容器可以堆叠多层`GRU`层的网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = keras.Sequential([\n",
    "    layers.GRU(64, return_sequences=True),\n",
    "    layers.GRU(64)\n",
    "])\n",
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.11 LSTM/GRU情感分类问题再战\n",
    "首先准备数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) 218 (25000,)\n",
      "(25000,) 68 (25000,)\n"
     ]
    }
   ],
   "source": [
    "# 批量大小\n",
    "batchsz = 128 \n",
    "# 词汇表大小N_vocab\n",
    "total_words = 10000 \n",
    "# 句子最大长度s，大于的句子部分将截断，小于的将填充\n",
    "max_review_len = 80 \n",
    "# 词向量特征长度f\n",
    "embedding_len = 100 \n",
    "# 加载IMDB数据集，此处的数据采用数字编码，一个数字代表一个单词\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n",
    "print(x_train.shape, len(x_train[0]), y_train.shape)\n",
    "print(x_test.shape, len(x_test[0]), y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 80) tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "# 截断和填充句子，使得等长，此处长句子保留句子后面的部分，短句子在前面填充\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n",
    "\n",
    "# 构建数据集，打散，批量，并丢掉最后一个不够batchsz的batch\n",
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.batch(batchsz, drop_remainder=True)\n",
    "print('x_train shape:', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字编码表\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# for k,v in word_index.items():\n",
    "#     print(k,v)\n",
    "#%%\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.11.1 LSTM模型\n",
    "首先是`Cell`方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5087 - accuracy: 0.6572Epoch 1/30\n",
      "195/195 [==============================] - 12s 63ms/step - loss: 0.4145 - accuracy: 0.8305\n",
      "195/195 [==============================] - 56s 285ms/step - loss: 0.5092 - accuracy: 0.6581 - val_loss: 0.4145 - val_accuracy: 0.8305\n",
      "Epoch 2/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8531Epoch 1/30\n",
      "195/195 [==============================] - 9s 48ms/step - loss: 0.3701 - accuracy: 0.8382\n",
      "195/195 [==============================] - 35s 181ms/step - loss: 0.3480 - accuracy: 0.8531 - val_loss: 0.3701 - val_accuracy: 0.8382\n",
      "Epoch 3/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.8725Epoch 1/30\n",
      "195/195 [==============================] - 9s 48ms/step - loss: 0.3525 - accuracy: 0.8419\n",
      "195/195 [==============================] - 36s 184ms/step - loss: 0.3094 - accuracy: 0.8726 - val_loss: 0.3525 - val_accuracy: 0.8419\n",
      "Epoch 4/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8891Epoch 1/30\n",
      "195/195 [==============================] - 9s 47ms/step - loss: 0.3672 - accuracy: 0.8470\n",
      "195/195 [==============================] - 36s 184ms/step - loss: 0.2834 - accuracy: 0.8891 - val_loss: 0.3672 - val_accuracy: 0.8470\n",
      "Epoch 5/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2627 - accuracy: 0.8955Epoch 1/30\n",
      "195/195 [==============================] - 9s 47ms/step - loss: 0.3694 - accuracy: 0.8429\n",
      "195/195 [==============================] - 36s 184ms/step - loss: 0.2624 - accuracy: 0.8955 - val_loss: 0.3694 - val_accuracy: 0.8429\n",
      "Epoch 6/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2451 - accuracy: 0.9066Epoch 1/30\n",
      "195/195 [==============================] - 10s 49ms/step - loss: 0.3519 - accuracy: 0.8446\n",
      "195/195 [==============================] - 36s 187ms/step - loss: 0.2455 - accuracy: 0.9066 - val_loss: 0.3519 - val_accuracy: 0.8446\n",
      "Epoch 7/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.9101Epoch 1/30\n",
      "195/195 [==============================] - 9s 46ms/step - loss: 0.3852 - accuracy: 0.8402\n",
      "195/195 [==============================] - 36s 183ms/step - loss: 0.2319 - accuracy: 0.9101 - val_loss: 0.3852 - val_accuracy: 0.8402\n",
      "Epoch 8/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2157 - accuracy: 0.9189Epoch 1/30\n",
      "195/195 [==============================] - 9s 47ms/step - loss: 0.3899 - accuracy: 0.8431\n",
      "195/195 [==============================] - 35s 181ms/step - loss: 0.2159 - accuracy: 0.9189 - val_loss: 0.3899 - val_accuracy: 0.8431\n",
      "Epoch 9/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9248Epoch 1/30\n",
      "195/195 [==============================] - 9s 48ms/step - loss: 0.3949 - accuracy: 0.8391\n",
      "195/195 [==============================] - 36s 184ms/step - loss: 0.2062 - accuracy: 0.9248 - val_loss: 0.3949 - val_accuracy: 0.8391\n",
      "Epoch 10/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1946 - accuracy: 0.9280Epoch 1/30\n",
      "195/195 [==============================] - 10s 49ms/step - loss: 0.4140 - accuracy: 0.8370\n",
      "195/195 [==============================] - 36s 186ms/step - loss: 0.1944 - accuracy: 0.9280 - val_loss: 0.4140 - val_accuracy: 0.8370\n",
      "Epoch 11/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1886 - accuracy: 0.9310Epoch 1/30\n",
      "195/195 [==============================] - 9s 47ms/step - loss: 0.4591 - accuracy: 0.8286\n",
      "195/195 [==============================] - 36s 185ms/step - loss: 0.1886 - accuracy: 0.9310 - val_loss: 0.4591 - val_accuracy: 0.8286\n",
      "Epoch 12/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1785 - accuracy: 0.9343Epoch 1/30\n",
      "195/195 [==============================] - 9s 46ms/step - loss: 0.4963 - accuracy: 0.8260\n",
      "195/195 [==============================] - 35s 180ms/step - loss: 0.1791 - accuracy: 0.9343 - val_loss: 0.4963 - val_accuracy: 0.8260\n",
      "Epoch 13/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1656 - accuracy: 0.9417Epoch 1/30\n",
      "195/195 [==============================] - 9s 47ms/step - loss: 0.4548 - accuracy: 0.8312\n",
      "195/195 [==============================] - 35s 182ms/step - loss: 0.1656 - accuracy: 0.9417 - val_loss: 0.4548 - val_accuracy: 0.8312\n",
      "Epoch 14/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9410Epoch 1/30\n",
      "195/195 [==============================] - 9s 47ms/step - loss: 0.5269 - accuracy: 0.8209\n",
      "195/195 [==============================] - 35s 181ms/step - loss: 0.1584 - accuracy: 0.9410 - val_loss: 0.5269 - val_accuracy: 0.8209\n",
      "Epoch 15/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1487 - accuracy: 0.9454Epoch 1/30\n",
      "195/195 [==============================] - 10s 49ms/step - loss: 0.5267 - accuracy: 0.8213\n",
      "195/195 [==============================] - 37s 188ms/step - loss: 0.1485 - accuracy: 0.9454 - val_loss: 0.5267 - val_accuracy: 0.8213\n",
      "Epoch 16/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1355 - accuracy: 0.9517Epoch 1/30\n",
      "195/195 [==============================] - 9s 47ms/step - loss: 0.5335 - accuracy: 0.8287\n",
      "195/195 [==============================] - 36s 186ms/step - loss: 0.1352 - accuracy: 0.9517 - val_loss: 0.5335 - val_accuracy: 0.8287\n",
      "Epoch 17/30\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9533Epoch 1/30\n",
      "  9/195 [>.............................] - ETA: 8s - loss: 0.6508 - accuracy: 0.8238"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2f239a03c7a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-2f239a03c7a8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                   experimental_run_tf_function = False)\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# 训练和验证\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m# 测试\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1209\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MyRNN(keras.Model):\n",
    "    # Cell方式构建多层网络\n",
    "    def __init__(self, units):\n",
    "        super(MyRNN, self).__init__()\n",
    "        # [b, 64]，构建Cell初始化状态向量，重复使用\n",
    "        self.state0 = [tf.zeros([batchsz, units]),tf.zeros([batchsz, units])]\n",
    "        self.state1 = [tf.zeros([batchsz, units]),tf.zeros([batchsz, units])]\n",
    "        # 词向量编码 [b, 80] => [b, 80, 100]\n",
    "        self.embedding = layers.Embedding(total_words, embedding_len, input_length=max_review_len)\n",
    "        # 构建2个Cell\n",
    "        self.rnn_cell0 = layers.LSTMCell(units, dropout=0.5)\n",
    "        self.rnn_cell1 = layers.LSTMCell(units, dropout=0.5)\n",
    "        # 构建分类网络，用于将CELL的输出特征进行分类，2分类\n",
    "        # [b, 80, 100] => [b, 64] => [b, 1]\n",
    "        self.outlayer = Sequential([\n",
    "        \tlayers.Dense(units),\n",
    "        \tlayers.Dropout(rate=0.5),\n",
    "        \tlayers.ReLU(),\n",
    "        \tlayers.Dense(1)])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs # [b, 80]\n",
    "        # embedding: [b, 80] => [b, 80, 100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
    "        state0 = self.state0\n",
    "        state1 = self.state1\n",
    "        for word in tf.unstack(x, axis=1): # word: [b, 100] \n",
    "            out0, state0 = self.rnn_cell0(word, state0, training) \n",
    "            out1, state1 = self.rnn_cell1(out0, state1, training)\n",
    "        # 末层最后一个输出作为分类网络的输入: [b, 64] => [b, 1]\n",
    "        x = self.outlayer(out1,training)\n",
    "        # p(y is pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob\n",
    "\n",
    "def main():\n",
    "    units = 64 # RNN状态向量长度f\n",
    "    epochs = 30 # 训练epochs\n",
    "\n",
    "    model = MyRNN(units)\n",
    "    # 装配\n",
    "    # Keras在编译模型阶段，区分训练状态和非训练状态，二者的逻辑是不一样的。\n",
    "    # 比如说，模型使用了Dropout，在训练时要随机失活部分神经元，而在正式运行，所有神经元都要保留的。\n",
    "    # 如果是训练状态，则在编译参数里增加一个选项即可：\n",
    "    # experimental_run_tf_function = False\n",
    "    # 否则会报错：\n",
    "    # _SymbolicException: Inputs to eager execution function cannot be Keras symbolic...\n",
    "    model.compile(optimizer = optimizers.RMSprop(0.001),\n",
    "                  loss = losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function = False)\n",
    "    # 训练和验证\n",
    "    model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "    # 测试\n",
    "    model.evaluate(db_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.11.2 GRU模型\n",
    "首先是`Cell`方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(keras.Model):\n",
    "    # Cell方式构建多层网络\n",
    "    def __init__(self, units):\n",
    "        super(MyRNN, self).__init__()\n",
    "        # [b, 64]，构建Cell初始化状态向量，重复使用\n",
    "        self.state0 = [tf.zeros([batchsz, units])]\n",
    "        self.state1 = [tf.zeros([batchsz, units])]\n",
    "        # 词向量编码 [b, 80] => [b, 80, 100]\n",
    "        self.embedding = layers.Embedding(total_words, embedding_len, input_length=max_review_len)\n",
    "        # 构建2个Cell\n",
    "        self.rnn_cell0 = layers.GRUCell(units, dropout=0.5)\n",
    "        self.rnn_cell1 = layers.GRUCell(units, dropout=0.5)\n",
    "        # 构建分类网络，用于将CELL的输出特征进行分类，2分类\n",
    "        # [b, 80, 100] => [b, 64] => [b, 1]\n",
    "        self.outlayer = Sequential([\n",
    "        \tlayers.Dense(units),\n",
    "        \tlayers.Dropout(rate=0.5),\n",
    "        \tlayers.ReLU(),\n",
    "        \tlayers.Dense(1)])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs # [b, 80]\n",
    "        # embedding: [b, 80] => [b, 80, 100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
    "        state0 = self.state0\n",
    "        state1 = self.state1\n",
    "        for word in tf.unstack(x, axis=1): # word: [b, 100] \n",
    "            out0, state0 = self.rnn_cell0(word, state0, training) \n",
    "            out1, state1 = self.rnn_cell1(out0, state1, training)\n",
    "        # 末层最后一个输出作为分类网络的输入: [b, 64] => [b, 1]\n",
    "        x = self.outlayer(out1, training)\n",
    "        # p(y is pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob\n",
    "\n",
    "def main():\n",
    "    units = 64 # RNN状态向量长度f\n",
    "    epochs = 30 # 训练epochs\n",
    "    model = MyRNN(units)\n",
    "    # 装配\n",
    "    model.compile(optimizer = optimizers.RMSprop(0.001),\n",
    "                  loss = losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function = False)\n",
    "    # 训练和验证\n",
    "    model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "    # 测试\n",
    "    model.evaluate(db_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.12 预训练的词向量\n",
    "在情感分类任务时，`Embedding`层是从零开始训练的。实际上，对于文本处理任务来说，领域知识大部分是共享的，因此我们能够利用在其它任务上训练好的词向量。\n",
    "\n",
    "我们以预训练的`GloVe`词向量为例，演示如何利用预训练的词向量模型提升任务性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "GLOVE_DIR = r'/home/alex/datasets/glove'\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "len(embeddings_index), len(embeddings_index.keys()), len(word_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GloVe.6B`版本共存储了40万个词汇的向量表。前面实战中我们只考虑最多1万个常见的词汇，我们根据词汇的数字编码表依次从GloVe模型中获取其词向量，并写入对应位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = total_words\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, embedding_len))\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # print(word,embedding_vector)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在获得了词汇表数据后，利用词汇表初始化`Embedding`层即可，并设置`Embedding`层不参与梯度优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(keras.Model):\n",
    "    # Cell方式构建多层网络\n",
    "    def __init__(self, units):\n",
    "        super(MyRNN, self).__init__() \n",
    "        # 词向量编码 [b, 80] => [b, 80, 100]\n",
    "        self.embedding = layers.Embedding(total_words, embedding_len,\n",
    "                                          input_length=max_review_len,\n",
    "                                          trainable=False)\n",
    "        self.embedding.build(input_shape=(None,max_review_len))\n",
    "        # self.embedding.set_weights([embedding_matrix])\n",
    "        # 构建RNN\n",
    "        self.rnn = keras.Sequential([\n",
    "            layers.LSTM(units, dropout=0.5, return_sequences=True),\n",
    "            layers.LSTM(units, dropout=0.5)\n",
    "        ])\n",
    "        # 构建分类网络，用于将CELL的输出特征进行分类，2分类\n",
    "        # [b, 80, 100] => [b, 64] => [b, 1]\n",
    "        self.outlayer = Sequential([\n",
    "            layers.Dense(32),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.ReLU(),\n",
    "            layers.Dense(1)])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs # [b, 80]\n",
    "        # embedding: [b, 80] => [b, 80, 100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
    "        x = self.rnn(x)\n",
    "        # 末层最后一个输出作为分类网络的输入: [b, 64] => [b, 1]\n",
    "        x = self.outlayer(x,training)\n",
    "        # p(y is pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob\n",
    "\n",
    "def main():\n",
    "    units = 512 # RNN状态向量长度f\n",
    "    epochs = 50 # 训练epochs\n",
    "\n",
    "    model = MyRNN(units)\n",
    "    # 装配\n",
    "    model.compile(optimizer = optimizers.Adam(0.001),\n",
    "                  loss = losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    # 训练和验证\n",
    "    model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "    # 测试\n",
    "    model.evaluate(db_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它部分均保持一致。我们可以简单地比较通过预训练的 GloVe 模型初始化的 Embedding 层的训练结果和随机初始化的 Embedding 层的训练结果，在训练完 50 个 Epochs 后，预训 练模型的准确率达到了 84.7%，提升了约 2%。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
