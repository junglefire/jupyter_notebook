{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 回归问题\n",
    "\n",
    "##### 1. 数据准备\n",
    "我们使用手写数据集`MNIST`，它包含了`0~9`共10种数字的手写图片，每种数字一共有7000张图片，采集自不同书写风格的真实手写图片，一共70000张图片。其中60000张图片作为训练集$\\mathbb{D}^{\\mathrm{train}}$用来训练模型，剩下10000张图片作为测试集$\\mathbb{D}^{\\mathrm{test}}$用来预测或者测试，训练集和测试集共同组成了整个`MNIST`数据集。\n",
    "\n",
    "考虑到手写数字图片包含的信息比较简单，每张图片均被缩放到$28\\times 28$的大小，同时只保留了灰度信息，因此，每张图片可以用$[h,w,1]$形状的张量来表示。\n",
    "\n",
    "我们利用`TensorFlow`自动在线下载`MNIST`数据集，并转换为`Numpy`数组格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([60000, 28, 28]), TensorShape([60000, 10]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x, y), (x_val, y_val) = datasets.mnist.load_data() # 加载 MNIST 数据集\n",
    "\n",
    "x = 2*tf.convert_to_tensor(x, dtype=tf.float32)/255.-1 # 转换为浮点张量，并缩放到-1~1\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32) # 转换为整形张量\n",
    "y = tf.one_hot(y, depth=10) # one-hot 编码\n",
    "\n",
    "(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一张图片的计算流程是通用的，我们在计算的过程中可以一次进行多张图片的计算，充分利用CPU或GPU的并行计算能力。我们用形状为$[h,w]$的矩阵来表示一张图片，对于多张图片来说，我们在前面添加一个数量维度(Dimension)，使用形状为$[b,h,w]$的张量来表示，其中$b$代表了批量(Batch Size)；多张彩色图片可以使用形状为$[b,h,w,c]$的张量来表示，其中$c$表示通道数量(Channel)，彩色图片$c=3$。通过`TensorFlow`的`Dataset`对象可以方便完成模型的批量训练，只需要调用`batch()`函数即可构建带`batch`功能的数据集对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28), (None, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)) # 构建数据集对象\n",
    "train_dataset = train_dataset.batch(512) # 批量训练\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 模型构建\n",
    "我们使用2层的全连接模型，公式如下：\n",
    "+ $h_1 = \\mathrm{ReLU}(W_1x + b_1)$\n",
    "+ $h_2 = \\mathrm{ReLU}(W_2x + b_2)$\n",
    "+ $o = W_3h_2 + b_3$\n",
    "\n",
    "对于第一层模型来说，它接受的输入$x \\in \\mathcal{R}^{784}$，输出$h_1 \\in \\mathcal{R}^{256}$设计为长度为256的向量，我们不需要显式地编写$h_1 = \\mathrm{ReLU}(W_1x + b_1)$的计算逻辑，在`TensorFlow`中通过一行代码即可实现：\n",
    "```python\n",
    "# 创建一层网络，设置输出节点数为256，激活函数类型为ReLU \n",
    "layers.Dense(256, activation='relu')\n",
    "```\n",
    "\n",
    "对于3层网络我们也可以快速搭建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用Sequential容器封装3个网络层，前网络层的输出默认作为下一层的输入\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu'), # 隐藏层 1\n",
    "    layers.Dense(128, activation='relu'), # 隐藏层 2\n",
    "    layers.Dense(10)]) # 输出层，输出节点数为 10\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定输入$x$，调用`model(x)`得到模型输出$o$后，通过`MSE`损失函数计算当前的误差$\\mathcal{L}$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape: # 构建梯度记录环境\n",
    "    # 打平操作，[b, 28, 28] => [b, 784]\n",
    "    x = tf.reshape(x, (-1, 28*28))\n",
    "    # Step1. 得到模型输出 output [b, 784] => [b, 10]\n",
    "    out = model(x)\n",
    "    # [b] => [b, 10]\n",
    "    y_onehot = y\n",
    "    # 计算差的平方和，[b, 10]\n",
    "    loss = tf.square(out-y_onehot)\n",
    "    # 计算每个样本的平均误差，[b]\n",
    "    loss = tf.reduce_sum(loss)/x.shape[0]\n",
    "# Step3. 计算参数的梯度 w1, w2, w3, b1, b2, b3\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "# w' = w - lr * grad，更新网络参数\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是一个神经网络模型的构造过程，这里省略了测试部分，后面章节会涉及。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
