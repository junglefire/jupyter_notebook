{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 导入Tensorflow相关模块\n",
    "import tensorflow.keras.datasets as datasets\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. TensorFlow基础\n",
    "`TensorFlow`是一个面向深度学习算法的科学计算库，内部数据保存在`张量`(Tensor)对象上，所有的运算操作(Operation，简称OP)也都是基于张量对象进行的。复杂的神经网络算法本质上就是各种张量相乘、相加等基本运算操作的组合，在深入学习深度学习算法之前，熟练掌握`TensorFlow`张量的基础操作方法十分重要。\n",
    "\n",
    "## 4.1 数据类型\n",
    "\n",
    "### 4.1.1 数值类型\n",
    "数值类型的张量是 TensorFlow 的主要数据载体，根据维度数来区分，可分为：\n",
    "+ **标量**(Scalar)：单个的实数，如1.2, 3.4等，维度(Dimension)数为0，`shape`为$[]$\n",
    "+ **向量**(Vector)：$n$个实数的有序集合，通过中括号包裹，如`[1.2]`，`[1.2,3.4]`等，维度数为1，长度不定，`shape`为$[n]$\n",
    "+ **矩阵**(Matrix)：$n$行$m$列实数的有序集合，如`[[1,2],[3,4]]`，维度数为2，每个维度上的长度不定，`shape`为$[n,m]$\n",
    "+ **张量**(Tensor)：所有维度数$dim > 2$的数组统称为张量。张量的每个维度也作`轴`(Axis)，一般维度代表了具体的物理含义，比如`Shape`为$[2,32,32,3]$的张量共有4维，如果表示图片数据的话，每个维度/轴代表的含义分别是图片数量、图片高度、图片宽度、图片通道数，其中2代表了2张图片，32代表了高、宽也是为32，3代表了`RGB`共3个通道。张量的维度数以及每个维度所代表的具体物理含义需要由用户自行定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(float, tensorflow.python.framework.ops.EagerTensor, True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1.2 # python 语言方式创建标量\n",
    "# 如果要使用TF提供的功能函数，须通过TF规定的方式去创建张量，\n",
    "# 而不能使用Python语言的标准变量创建方式\n",
    "aa = tf.constant(1.2)\n",
    "type(a), type(aa), tf.is_tensor(aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与标量不同，向量的定义须通过`List`容器传给`tf.constant()`函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1. , 2.3, 4. ], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1, 2.3, 4.]) # 创建一个元素的向量\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的方法，定义矩阵的实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1,2],[3,4]])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 字符串类型\n",
    "除了丰富的数值类型张量外，`TensorFlow`还支持字符串类型的数据。在`tf.strings`模块中，提供了常见的字符串类型的工具函数，如小写化`lower()`、拼接`join()`、长度`length()`、切分`split()`等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'Hello, Deep Learning.'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant('Hello, Deep Learning.')\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 布尔类型\n",
    "为了方便表达比较运算操作的结果，`TensorFlow`还支持布尔类型的张量。布尔类型的张量只需要传入`Python`语言的布尔类型数据，转换成`TensorFlow`内部布尔型即可。\n",
    "\n",
    "需要注意的是，`TensorFlow`的布尔类型和`Python`语言的布尔类型并不等价，不能通用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=bool, numpy=True>,\n",
       " False,\n",
       " <tf.Tensor: shape=(), dtype=bool, numpy=True>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(True)\n",
    "a, a is True, a == True #仅数值比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 数值精度\n",
    "对于数值类型的张量，可以保存为不同字节长度的精度。常用的精度类型有`tf.int16`、`tf.int32`、`tf.int64`、`tf.float16`、`tf.float32`、`tf.float64`等，其中`tf.float64`即为`tf.double`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=3.1415927>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.141592653589793>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_32 = tf.constant(np.pi, dtype=tf.float32)\n",
    "pi_64 = tf.constant(np.pi, dtype=tf.float64)\n",
    "pi_32, pi_64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于大部分深度学习算法，一般使用`tf.int32`和`tf.float32`可满足大部分场合的运算精度要求，部分对精度要求较高的算法，如强化学习某些算法，可以选择使用`tf.int64`和`tf.float64`精度保存张量。\n",
    "\n",
    "### 4.2.1 读取精度\n",
    "通过访问张量的`dtype`成员属性可以判断张量的保存精度。\n",
    "\n",
    "### 4.2.2 类型转换\n",
    "系统的每个模块使用的数据类型、数值精度可能各不相同，对于不符合要求的张量的类型及精度，需要通过`tf.cast`函数进行转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float16, numpy=3.14>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.140625>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_16 = tf.constant(np.pi, dtype=tf.float16)\n",
    "pi_64 = tf.cast(pi_16, tf.double)\n",
    "pi_16, pi_64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 待优化张量\n",
    "为了区分需要计算梯度信息的张量与不需要计算梯度信息的张量，`TensorFlow`增加了一种专门的数据类型来支持梯度信息的记录：`tf.Variable`。\n",
    "\n",
    "`tf.Variable`类型在普通的张量类型基础上添加了`name`、`trainable`等属性来支持计算图的构建。由于梯度运算会消耗大量的计算资源，而且会自动更新相关参数，对于不需要的优化的张量，如神经网络的输入$X$，不需要通过`tf.Variable`封装；相反，对于需要计算梯度并优化的张量，如神经网络层的$W$和$b$，需要通过`tf.Variable`包裹以便`TensorFlow`跟踪相关梯度信息。\n",
    "\n",
    "通过`tf.Variable()`函数可以将普通张量转换为待优化张量，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Variable:0', True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([-1, 0, 1, 2])\n",
    "aa = tf.Variable(a)\n",
    "aa.name, aa.trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`name`和`trainable`属性是`Variable`特有的属性，`name`属性用于命名计算图中的变量，这套命名体系是`TensorFlow`内部维护的，一般不需要用户关注；`trainable`属性表征当前张量是否需要被优化，创建`Variable`对象时是默认启用优化标志，可以设置`trainable=False`来设置张量不需要优化。\n",
    "\n",
    "待优化张量可视为普通张量的特殊类型，普通张量其实也可以通过`GradientTape.watch()`方法临时加入跟踪梯度信息的列表，从而支持自动求导功能。\n",
    "\n",
    "除了通过普通张量方式创建`Variable`，也可以直接创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Variable:0', True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.Variable([[1,2],[3,4]])\n",
    "a.name, a.trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 创建张量\n",
    "在`TensorFlow`中，可以通过多种方式创建张量。\n",
    "\n",
    "### 4.4.1 从数组、列表对象创建\n",
    "通过`tf.convert_to_tensor`函数可以创建新`Tensor`，并将保存在`Python`列表对象或者`Numpy`数组对象中的数据导入到新`Tensor`中，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
       " array([[1., 2.],\n",
       "        [3., 4.]])>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.convert_to_tensor([1,2.])\n",
    "b = tf.convert_to_tensor(np.array([[1,2.],[3,4]]))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **注意**\n",
    "> Numpy浮点数数组默认使用64位精度保存数据，转换到`Tensor`类型时精度为`tf.float64`，可以在需要的时候将其转换为`tf.float32`类型\n",
    "\n",
    "`tf.constant()`和`tf.convert_to_tensor()`都能够自动的把`Numpy`数组或者`Python`列表数据类型转化为`Tensor`类型，这两个API命名来自`TensorFlow 1.x`的命名习惯，在`TensorFlow 2`中函数的名字并不是很贴切，使用其一即可。\n",
    "\n",
    "### 4.4.2 创建全0或全1张量\n",
    "将张量创建为全0或者全1数据是非常常见的张量初始化手段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([]),tf.ones([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([1]),tf.ones([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0., 0.],\n",
       "        [0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([2,2]),tf.ones([2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tf.zeros_like`，`tf.ones_like`可以方便地新建与某个张量`shape`一致，且内容为全0或全1的张量。\n",
    "\n",
    "### 4.4.3 创建自定义数值张量\n",
    "除了初始化为全0或全1的张量之外，有时也需要全部初始化为某个自定义数值的张量。\n",
    "\n",
    "通过`tf.fill(shape, value)`可以创建全为自定义数值`value`的张量，形状由`shape`参数指定："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[99, 99],\n",
       "       [99, 99]], dtype=int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fill([2,2], 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 创建已知分布的张量\n",
    "正态分布(Normal Distribution，或Gaussian Distribution)和均匀分布(Uniform Distribution)是最常见的分布之一，创建采样自这2种分布的张量非常有用，比如在卷积神经网络中，卷积核张量$W$初始化为正态分布有利于网络的训练；在对抗生成网络中，隐藏变量$z$一般采样自均匀分布。\n",
    "\n",
    "通过`tf.random.normal(shape, mean=0.0, stddev=1.0)`可以创建形状为`shape`、均值为`mean`、标准差为`stddev`的正态分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.5458537, 3.7866337],\n",
       "       [2.8747766, 2.3216863]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal([2,2], mean=1,stddev=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)`可以创建采样自$[\\mathrm{minval}, \\mathrm{maxval})$区间的均匀分布的张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[8.20524  , 5.618274 ],\n",
       "       [4.3096056, 1.7808759]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform([2,2], maxval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要均匀采样整形类型的数据，必须指定采样区间的最大值`maxval`参数，同时指定数据类型为`tf.int*`型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 0, 31],\n",
       "       [63,  1]], dtype=int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform([2,2],maxval=100,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.5 创建序列\n",
    "可以通过`tf.range()`函数创建一段连续的整型序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 2, 4, 6, 8], dtype=int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10, delta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([10, 12, 14, 16, 18], dtype=int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10, 20, delta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 张量的典型应用\n",
    "本节介绍每种维度数下张量的典型应用，让读者在看到每种张量时，能够直观地联想到它主要的物理意义和用途，对后续张量的维度变换等一系列抽象操作的学习打下基础。\n",
    "\n",
    "### 4.5.1 标量\n",
    "标量就是一个简单的数字，维度数为0，`shape`为$[]$。标量的一些典型用途是误差值的表示、各种测量指标的表示，比如准确度(Accuracy)、精度(Precision)和召回率(Recall)等。\n",
    "\n",
    "以均方差误差函数为例，经过`tf.keras.losses.mse`(或`tf.keras.losses.MSE`，两者相同功能)返回每个样本上的误差值，最后取误差的均值作为当前`Batch`的误差，它是一个标量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.29233792>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tf.random.uniform([4,10]) #随机模拟网络输出\n",
    "y = tf.constant([2,3,2,0]) # 随机构造样本真实标签\n",
    "y = tf.one_hot(y, depth=10) # one-hot 编码\n",
    "\n",
    "loss = tf.keras.losses.mse(y, out) # 计算每个样本的 MSE\n",
    "loss = tf.reduce_mean(loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 向量\n",
    "向量是一种非常常见的数据载体，如在全连接层和卷积神经网络层中，偏置张量$b$就使用向量来表示。如`图4.2`所示，每个全连接层的输出节点都添加了一个偏置值，把所有输出节点的偏置表示成向量形式：$b = [b_1, b_2]^T$。\n",
    "\n",
    "<img src=\"images/04_02.png\" style=\"width:350px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[ 0.47742736, -0.14441332],\n",
       "       [ 1.5571787 ,  1.2649426 ],\n",
       "       [ 0.13402967, -1.9965774 ],\n",
       "       [-1.3444796 , -2.251197  ]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z=wx,模拟获得激活函数的输入z\n",
    "z = tf.random.normal([4,2])\n",
    "b = tf.zeros([2])\n",
    "z = z + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过高层接口类`Dense()`方式创建的网络层，张量$W$和$b$存储在类的内部，由类自动创建并管理。可以通过全连接层的`bias`成员变量查看偏置变量$b$，例如创建输入节点数为4，输出节点数为3的线性层网络，那么它的偏置向量$b$的长度应为3："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一层`Wx+b`，输出节点为3\n",
    "fc = layers.Dense(3) \n",
    "# 通过build函数创建`W,b`张量，输入节点为4\n",
    "fc.build(input_shape=(2,4))\n",
    "fc.bias # 查看偏置向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，类的偏置成员`bias`为长度为3的向量，初始化为全0，这也是偏置$b$的默认初始化方案。同时偏置向量$b$的类型为`Variable`，这是因为$W$和$b$都是待优化参数。\n",
    "\n",
    "### 4.5.3 矩阵\n",
    "矩阵也是非常常见的张量类型，比如全连接层的批量输入张量$X$的形状为$[b, d_{in}]$，其中$b$表示输入样本的个数，即`Batch Size`，$d_{in}$表示输入特征的长度。例如特征长度为4，一共包含2个样本的输入可以表示为矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令全连接层的输出节点数为3，则它的权值张量$W$的`shape`为$[4,3]$，我们利用张量$X$、$W$和向量$b$可以直接实现一个网络层，代码如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.1310693,  1.1310693,  1.1310693],\n",
       "       [-0.9165219, -0.9165219, -0.9165219]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.ones([4,3]) #定义W张量\n",
    "b = tf.zeros([3]) #定义b张量\n",
    "o = x@w+b #X@W+b运算\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$X$和$W$张量均是矩阵，上述代码实现了一个线性变换的网络层，激活函数为空。一般地，$\\sigma(X@W + b)$网络层称为`全连接层`，在`TensorFlow`中可以通过`Dense`类直接实现，特别地，当激活函数$\\sigma$为空时，全连接层也称为`线性层`。我们通过`Dense`类创建输入4个节 点，输出3个节点的网络层，并通过全连接层的`kernel`成员名查看其权值矩阵$W$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
       "array([[ 0.07998121,  0.24727356, -0.08657426],\n",
       "       [-0.88830286,  0.10295928,  0.5879053 ],\n",
       "       [-0.69401145,  0.35744727,  0.07975996],\n",
       "       [ 0.53344214,  0.26444352,  0.04773366]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = layers.Dense(3) #定义全连接层的输出节点为3\n",
    "fc.build(input_shape=(2,4)) #定义全连接层的输入节点为4\n",
    "fc.kernel # 查看权值矩阵W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 三维张量\n",
    "三维的张量一个典型应用是表示序列信号，它的格式是\n",
    "+ $X = [\\mathrm{batch\\_size}, \\mathrm{sequence\\_len}, \\mathrm{feature\\_len}]$\n",
    "\n",
    "考虑自然语言处理中句子的表示，如评价句子的是否为正面情绪的情感分类任务网络，如`图4.3`所示。\n",
    "\n",
    "<img src=\"images/04_03.png\" style=\"width:400px;\"/>\n",
    "\n",
    "为了能够方便字符串被神经网络处理，一般将单词通过嵌入层(Embedding Layer)编码为固定长度的向量，比如`what`编码为某个长度3的向量，那么2个等长(单词数量为5)的句子序列可以表示为`shape`为$[2,5,3]$的3维张量，其中2表示句子个数，5表示单词数量，3表示单词向量的长度。\n",
    "\n",
    "### 4.5.5 四维张量\n",
    "大于四维的张量一般应用的比较少，如在元学习(Meta Learning)中会采用五维的张量表示方法，理解方法与三、四维张量类似，不再赘述。\n",
    "\n",
    "四维张量在卷积神经网络中应用非常广泛，它用于保存`特征图`(Feature maps)数据，格式一般定义为\n",
    "+ $[b,h,w,c]$\n",
    "\n",
    "其中$b$表示输入样本的数量，$h,w$分别表示特征图的高/宽，$c$表示特征图的通道数。图片数据是特征图的一种，对于含有`RGB`的3个通道的彩色图片，每张图片包含了$h$行列像素点，每个点需要3个数值表示`RGB`通道的颜色强度，因此一张图片可以表示为$[h,w,3]$。\n",
    "\n",
    "神经网络中一般并行计算多个输入以提高计算效率，故$b$张图片的张量可表示为$[b,h,w,3]$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 30, 30, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建32x32的彩色图片输入，个数为4\n",
    "x = tf.random.normal([4,32,32,3])\n",
    "# 创建卷积神经网络\n",
    "layer = layers.Conv2D(16,kernel_size=3)\n",
    "out = layer(x) # 前向计算\n",
    "out.shape # 输出大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中卷积核张量也是4维张量，可以通过`kernel`成员变量访问："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 3, 3, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 索引与切片\n",
    "\n",
    "### 4.6.1 索引\n",
    "`TensorFlow`支持基本的$[i][j]\\dots$标准索引方式，也支持通过逗号分隔索引号的索引方式。假设$X$为4张$32\\times 32$大小的彩色图片，`shape`为$[4,32,32,3]$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.68077874, -1.2214446 ,  2.3251214 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.68077874, -1.2214446 ,  2.3251214 ], dtype=float32)>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([4,32,32,3])\n",
    "# 取第1张图片，第2行，第3列的数据\n",
    "x[0][1][2], x[0,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 切片\n",
    "通过`start: end: step`切片方式可以方便地提取一段数据。切片的用法如下表：\n",
    "\n",
    "<img src=\"images/t_04_01.png\" style=\"width:500px;\"/>\n",
    "\n",
    "特别地，`step`可以为负数，例如，当`step=−1`时，`start: end: −1`表示从`start`开始，逆序读取至`end`结束(不包含`end`)，索引号$\\mathrm{end} \\le \\mathrm{start}$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int32, numpy=array([8, 7, 6, 5, 4, 3, 2, 1], dtype=int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(9)\n",
    "x[8:0:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆序取全部元素，实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=int32, numpy=array([8, 7, 6, 5, 4, 3, 2, 1, 0], dtype=int32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当张量的维度数量较多时，不需要采样的维度一般用单冒号`:`表示采样所有元素。考虑$[4,32,32,3]$的图片张量，当需要读取`G`通道上的数据时，前面所有维度全部提取，此时需要写为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 32, 32])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([4,32,32,3])\n",
    "# 取G通道数据\n",
    "x[:,:,:,1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了避免出现像$[:,:,:,1]$这样过多冒号的情况，可以使用`⋯`符号表示取多个维度上所有的数据，其中维度的数量需根据规则自动推断：当切片方式出现`⋯`符号时，`⋯`符号左边的维度将自动对齐到最左边，`⋯`符号右边的维度将自动对齐到最右边，此时系统再自动推断`⋯`符号代表的维度数量，它的切片方式总结如`表4.2`所示：\n",
    "\n",
    "<img src=\"images/t_04_02.png\" style=\"width:500px;\"/>\n",
    "\n",
    "读取第`1~2`张图片的`G/B`通道数据，实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 32, 32, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 高宽维度全部采集\n",
    "x[0:2,...,1:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取最后2张图片，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 32, 32, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 高、宽、通道维度全部采集，等价于x[2:]\n",
    "x[2:,...].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 维度变换\n",
    "基本的维度变换操作函数包含了改变视图`reshape`、插入新维度`expand_dims`，删除维度`squeeze`、交换维度`transpose`、复制数据`tile`等函数。\n",
    "\n",
    "### 4.7.1 改变视图\n",
    "我们先来认识一下张量的`存储`(Storage)和`视图`(View)的概念：\n",
    "+ 张量的视图就是我们理解张量的方式，比如`shape`为$[2,4,4,3]$的张量$A$，我 们从逻辑上可以理解为2张图片，每张图片4行4列，每个位置有`RGB`表示颜色的3个通道的数据\n",
    "+ 张量的存储体现在张量在内存上保存为一段连续的内存区域\n",
    "\n",
    "对于同样的存储，我们可以有不同的理解方式，比如上述张量$A$，我们可以在不改变张量的存储下，将张量$A$理解为2个样本，每个样本的特征为长度48的向量。同一个存储，从不同的角度观察数据，可以产生不同的视图，这就是存储与视图的关系。\n",
    "\n",
    "改变视图操作在提供便捷性的同时，也会带来很多逻辑隐患，这主要的原因是改变视图操作的默认前提是存储不需要改变，否则改变视图操作就是非法的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 4, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.range(96)\n",
    "# 改变`x`的视图，获得`4D`张量，存储并未改变\n",
    "x=tf.reshape(x,[2,4,4,3])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2 增、删维度\n",
    "增加一个长度为1的维度相当于给原有的数据添加一个新维度的概念，维度长度为1，故数据并不需要改变，仅仅是改变数据的理解方式，因此它其实可以理解为改变视图的一种特殊方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[0,1],[2,3]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tf.expand_dims(x, axis)`可在指定的`axis`轴前可以插入一个新的维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\n",
       "array([[[0],\n",
       "        [1]],\n",
       "\n",
       "       [[2],\n",
       "        [3]]], dtype=int32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.expand_dims(x,axis=2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，插入一个新维度后，数据的存储顺序并没有改变，仅仅是改变了数据的视图。\n",
    "\n",
    "需要注意的是，`axis`为正时，表示在当前维度之前插入一个新维度；为负时，表示当前维度之后插入一个新的维度：\n",
    "\n",
    "<img src=\"images/04_06.png\" style=\"width:230px;\"/>\n",
    "\n",
    "删除维度是增加维度的逆操作，删除维度只能删除长度为1的维度，也不会改变张量的存储："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.squeeze(x, axis=2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不指定维度参数`axis`，`tf.squeeze(x)`会默认删除所有长度为1的维度，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 1), dtype=int32, numpy=\n",
       "array([[[[0],\n",
       "         [1]],\n",
       "\n",
       "        [[2],\n",
       "         [3]]]], dtype=int32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[[0,1],[2,3]]])\n",
    "x = tf.expand_dims(x,axis=3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.squeeze(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3 交换维度\n",
    "在实现算法逻辑时，在保持维度顺序不变的条件下，仅仅改变张量的理解方式是不够的，有时需要直接调整的存储顺序，即交换维度(Transpose)。通过交换维度操作，改变了张量的存储顺序，同时也改变了张量的视图。\n",
    "\n",
    "例如，`TensorFlow`中图片张量的默认存储格式是通道后行格式：$[b,h,w,c]$，但是`PyTorch`是通道先行格式：$[b,c,h,w]$，因此需要交换维度。`tf.transpose(x, perm)`函数完成维度交换操作，其中参数`perm`表示新维度的顺序`List`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 32, 32])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([2,32,32,3])\n",
    "x1 = tf.transpose(x,perm=[0,3,1,2])\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.4 复制数据\n",
    "`tf.tile(x, multiples)`函数完成数据在指定维度上的复制操作，`multiples`指定了每个维度上面的复制倍数，对应位置为1表明不复制，为2表明新长度为原来长度的2倍，即数据复制一份，以此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(4)\n",
    "# 创建2行2列矩阵\n",
    "x = tf.reshape(x,[2,2])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，在列维度复制1份数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
       "array([[0, 1, 0, 1],\n",
       "       [2, 3, 2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.tile(x,multiples=[1,2])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后在行维度复制1份数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "array([[0, 1, 0, 1],\n",
       "       [2, 3, 2, 3],\n",
       "       [0, 1, 0, 1],\n",
       "       [2, 3, 2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.tile(x,multiples=[2,1])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **注意**\n",
    "> \n",
    "> `tf.tile`会创建一个新的张量来保存复制后的张量，复制操作涉及大量数据的读写IO运算，计算代价相对较高。神经网络中不同`shape`之间的张量运算操作十分频繁，那么有没有轻量级的复制操作呢？这就是接下来要介绍的`Broadcasting`操作\n",
    "\n",
    "## 4.8 Broadcasting\n",
    "`Broadcasting`称为`广播机制`(或`自动扩展机制`)，它是一种轻量级的张量复制手段，在逻辑上扩展张量数据的形状，但是只会在需要时才会执行实际存储复制操作。对于大部分场景，`Broadcasting`机制都能通过优化手段避免实际复制数据而完成逻辑运算，从而相对于`tf.tile`函数，减少了大量计算代价。\n",
    "\n",
    "对于所有长度为1的维度，`Broadcasting`的效果和`tf.tile`一样，都能在此维度上逻辑复制数据若干份，区别在于`tf.tile`会创建一个新的张量，执行复制IO操作，并保存复制后的张量数据，而`Broadcasting`并不会立即复制数据，它会在逻辑上改变张量的形状，使得视图上变成了复制后的形状。\n",
    "\n",
    "`Broadcasting`会通过深度学习框架的优化手段避免实际复制数据而完成逻辑运算，至于怎么实现的用户不必关心，对于用户来说，`Broadcasting`和`tf.tile`复制的最终效果是一样的，操作对用户透明，但是`Broadcasting`机制节省了大量计算资源。\n",
    "\n",
    "那么有了`Broadcasting`机制后，所有`shape`不一致的张量是不是都可以直接完成运算？显然，所有的运算都需要在正确逻辑下进行，`Broadcasting`机制并不会扰乱正常的计算逻辑，它只会针对于最常见的场景自动完成增加维度并复制数据的功能，提高开发效率和运行效率。这种最常见的场景是什么呢？这就要说到`Broadcasting`设计的核心思想。\n",
    "\n",
    "`Broadcasting`机制的核心思想是普适性，即同一份数据能普遍适合于其他位置。在验证普适性之前，需要先将张量`shape`靠右对齐，然后进行普适性判断：对于长度为1的维 度，默认这个数据普遍适合于当前维度的其他位置；对于不存在的维度，则在增加新维度后默认当前数据也是普适于新维度的，从而可以扩展为更多维度数、任意长度的张量形状。\n",
    "\n",
    "考虑`shape`为$[w,1]$的张量$A$，需要扩展为`shape`：$[b,h,w,c]$。如`图4.7`所示，第一行为欲扩展的`shape`，第二行为现有`shape`：\n",
    "\n",
    "<img src=\"images/04_07.png\" style=\"width:300px;\"/>\n",
    "\n",
    "首先将2个`shape`靠右对齐，对于通道维度$c$，张量的现长度为1，则默认此数据同样适合当前维度的其他位置，将数据在逻辑上复制$c−1$份，长度变为 $c$；对于不存在的$b$和$h$维度，则自动插入新维度，新维度长度为1，同时默认当前的数据普适于新维度的其他位置，即对于其它的图片、其它的行来说，与当前的这一行的数据完全一致。这样将数据$b$和$h$维度的长度自动扩展为$b$和$h$，如`图4.8`所示：\n",
    "\n",
    "<img src=\"images/04_08.png\" style=\"width:550px;\"/>\n",
    "\n",
    "通过`tf.broadcast_to(x, new_shape)`函数可以显式地执行自动扩展功能，将现有`shape`扩张为`new_shape`，实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [2]], dtype=int32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.range(3)\n",
    "A = tf.reshape(A, [3,-1])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 3, 3), dtype=int32, numpy=\n",
       "array([[[[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [2, 2, 2]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [2, 2, 2]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [2, 2, 2]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [2, 2, 2]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [2, 2, 2]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [2, 2, 2]]]], dtype=int32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = tf.broadcast_to(A, [2,3,3,3])\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在普适性原则的指导下，`Broadcasting`机制变得直观、好理解，它的设计是非常符合人的思维模式。\n",
    "\n",
    "我们来考虑不满足普适性原则的例子，如`图4.9`所示：\n",
    "\n",
    "<img src=\"images/04_09.png\" style=\"width:270px;\"/>\n",
    "\n",
    "在$c$维度上，张量已经有2个特征数据，新`shape`对应维度的长度为$c$, ($c \\ne 2$，如$c = 3$)，那么当前维度上的这2个特征无法普适到其它位置，故不满足普适性原则，无法应用`Broadcasting`机制，将会触发错误。\n",
    "\n",
    "在进行张量运算时，有些运算在处理不同`shape`的张量时，会隐式地自动调用`Broadcasting`机制，如`+、-、*、/`等运算等，将参与运算的张量`Broadcasting`成一个公共`shape`，再进行相应的计算。`图4.10`演示了3种不同`shape`下的张量$A、B$相加的例子：\n",
    "\n",
    "<img src=\"images/04_10.png\" style=\"width:400px;\"/>\n",
    "\n",
    "\n",
    "## 4.9 数学运算\n",
    "\n",
    "### 4.9.1 加、减、乘、除\n",
    "分别通过`tf.add`、`tf.subtract`、`tf.multiply`、`tf.divide`函数实现，`TensorFlow`也重载了`+、 − 、 ∗ 、/`运算符。\n",
    "\n",
    "整除和余除分别通过`//`和`%`运算符实现。\n",
    "\n",
    "### 4.9.2 乘方运算\n",
    "通过`tf.pow(x, a)`可以方便地完成$y = x^a$运算。\n",
    "\n",
    "当指数为$\\frac{1}{a}$时，完成$y = \\sqrt[a]{x}$运算。\n",
    "\n",
    "对于常见的平方和平方根运算，可以使用`tf.square(x)`和`tf.sqrt(x)`实现。\n",
    "\n",
    "### 4.9.3 指数和对数运算\n",
    "对于自然指数$e^x$，可以通过`tf.exp(x)`实现。\n",
    "\n",
    "自然对数$\\log{e}$可以通过`tf.math.log(x)`实现。\n",
    "\n",
    "### 4.9.4 矩阵相乘运算\n",
    "通过@运算符可以实现矩阵相乘，还可以通过`tf.matmul(a, b)`函数实现。\n",
    "\n",
    "需要注意的是，`TensorFlow`中的矩阵相乘可以使用批量方式，也就是张量$A$和$B$的维度数可以大于2。当张量$A$和$B$的维度数大于2时，`TensorFlow`会选择$A$和$B$的最后两个维度进行矩阵相乘，前面所有的维度都视作`Batch`维度。\n",
    "\n",
    "根据矩阵相乘的定义，$A$和$B$能够矩阵相乘的条件是，$A$的倒数第一个维度长度(列)和$B$的倒数第二个维度长度(行)必须相等："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 3, 28, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal([4,3,28,32])\n",
    "b = tf.random.normal([4,3,32,2])\n",
    "c = a@b # 批量形式的矩阵相乘\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵相乘函数同样支持自动`Broadcasting`机制："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 28, 16])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal([4,28,32])\n",
    "b = tf.random.normal([32,16])\n",
    "c = a@b\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 前向传播实战\n",
    "本节我们将利用已经学到的知识去完成三层神经网络的实现：\n",
    "+ $\\mathrm{out} = \\mathit{ReLU}\\bigg\\{\\mathit{ReLU}\\big\\{\\mathit{ReLU}[X@W_1 + b_1]@W_2 + b_2\\big\\}@W + b\\bigg\\}$\n",
    "\n",
    "我们采用的数据集是`MNIST`手写数字图片集，输入节点数为784，第一层的输出节点数是256，第二层的输出节点数是128，第三层的输出节点是10，也就是当前样本属于10类别的概率。\n",
    "\n",
    "完整代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载MNIST数据集并标准化\n",
    "def load_data():\n",
    "    (x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
    "    # 转换为浮点张量， 并缩放到-1~1\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "    # 转换为整形张量\n",
    "    y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "    # one-hot 编码\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    # 改变视图， [b, 28, 28] => [b, 28*28]\n",
    "    x = tf.reshape(x, (-1, 28 * 28))\n",
    "    # 构建数据集对象\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    # 批量训练\n",
    "    train_dataset = train_dataset.batch(200)\n",
    "    return train_dataset\n",
    "\n",
    "# 初始化模型参数\n",
    "def init_paramaters():\n",
    "    # 每层的张量都需要被优化，故使用Variable类型，并使用截断的正太分布初始化权值张量\n",
    "    # 偏置向量初始化为 0 即可\n",
    "    # 第一层的参数\n",
    "    w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    # 第二层的参数\n",
    "    w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    # 第三层的参数\n",
    "    w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    return w1, b1, w2, b2, w3, b3\n",
    "\n",
    "\n",
    "def train_epoch(epoch, train_dataset, w1, b1, w2, b2, w3, b3, lr=0.001):\n",
    "    for step, (x, y) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 第一层计算， [b, 784]@[784, 256]+[256] => [b, 256]+[256] => [b,256]+[b, 256]\n",
    "            h1 = x @ w1 + tf.broadcast_to(b1, (x.shape[0], 256))\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # 第二层计算， [b, 256] => [b, 128]\n",
    "            h2 = h1 @ w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # 输出层计算， [b, 128] => [b, 10]\n",
    "            out = h2 @ w3 + b3\n",
    "            # 计算网络输出与标签之间的均方差，mse=mean(sum(y-out)^2)\n",
    "            # [b, 10]\n",
    "            loss = tf.square(y - out)\n",
    "            # 误差标量， mean: scalar\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            # 自动梯度，需要求梯度的张量有[w1, b1, w2, b2, w3, b3]\n",
    "            grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        # 梯度更新， assign_sub将当前值减去参数值，原地更新\n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        w2.assign_sub(lr * grads[2])\n",
    "        b2.assign_sub(lr * grads[3])\n",
    "        w3.assign_sub(lr * grads[4])\n",
    "        b3.assign_sub(lr * grads[5])\n",
    "    print('epoch:', epoch, ', loss:', loss.numpy())\n",
    "    return loss.numpy()\n",
    "\n",
    "\n",
    "def train(epochs):\n",
    "    losses = []\n",
    "    train_dataset = load_data()\n",
    "    w1, b1, w2, b2, w3, b3 = init_paramaters()\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(epoch, train_dataset, w1, b1, w2, b2, w3, b3, lr=0.001)\n",
    "        losses.append(loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , loss: 0.17441195\n",
      "epoch: 1 , loss: 0.1534687\n",
      "epoch: 2 , loss: 0.13826548\n",
      "epoch: 3 , loss: 0.12692253\n",
      "epoch: 4 , loss: 0.11815366\n",
      "epoch: 5 , loss: 0.111196354\n",
      "epoch: 6 , loss: 0.105570585\n",
      "epoch: 7 , loss: 0.10087141\n",
      "epoch: 8 , loss: 0.096876554\n",
      "epoch: 9 , loss: 0.09341132\n",
      "epoch: 10 , loss: 0.09034189\n",
      "epoch: 11 , loss: 0.08761643\n",
      "epoch: 12 , loss: 0.08518828\n",
      "epoch: 13 , loss: 0.083006196\n",
      "epoch: 14 , loss: 0.08102085\n",
      "epoch: 15 , loss: 0.07920984\n",
      "epoch: 16 , loss: 0.077544436\n",
      "epoch: 17 , loss: 0.076011084\n",
      "epoch: 18 , loss: 0.07459002\n",
      "epoch: 19 , loss: 0.07326108\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "losses = train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络训练 误差值的变化曲线："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5xVZdn/8c/FWUHO0wyCCCipoII4Ip5NSyH7iflDwSMgYGk+T6VYFKWGaaiZpqmpecRE07SIVDxk2VMeGAyRg8hIoGOcPSCPIgxczx/3GtkOe8+e09prz+zv+/Var1l7Hfa+Zs+e+c697rXuZe6OiIhIdS2SLkBERPKTAkJERNJSQIiISFoKCBERSUsBISIiabVKuoDG0r17d+/Tp0/SZYiINCnz5s1b7+5F6dY1m4Do06cPZWVlSZchItKkmNnKTOt0iElERNJSQIiISFoKCBERSavZ9EGIiNTF1q1bqaioYPPmzUmXkhPt2rWjV69etG7dutb7KCBEpCBVVFSw22670adPH8ws6XJi5e5s2LCBiooK+vbtW+v9CvoQU0kJmO08lZQkXZmIxG3z5s1069at2YcDgJnRrVu3OreWCjog1qyp23IRaV4KIRyq1Od7LeiAEBGRzBQQIiIJ2LBhA4MHD2bw4MGUlJTQs2fPzx5v2bKlVs8xfvx4li5dGluN6qQWEcmipCT9oefiYli9un7P2a1bN+bPnw/AFVdcQYcOHZg8efLntnF33J0WLdL/L3/PPffU78VrSS0IEZEsctlfWV5ezoABAzjrrLMYOHAgq1at4vzzz6e0tJSBAwcybdq0z7Y98sgjmT9/PpWVlXTu3JkpU6YwaNAgDjvsMNauXdvgWgq6BVFcnPm/AhEpHN/5DkT/zNfZscemXz54MNx4Y/2e84033uD++++ntLQUgOnTp9O1a1cqKyv50pe+xKhRoxgwYMDn9vnwww855phjmD59OhdffDF33303U6ZMqV8BkYJuQaxeDe5huuqqsOzNN+vfZBQRaQx77bXXZ+EAMHPmTIYMGcKQIUNYsmQJixcv3mmfXXbZhREjRgBw8MEHs2LFigbXUdAtiFTjxsFll8Hdd8PPfpZ0NSKSS9n+06/pDNG//rVRSwGgffv2n80vW7aMX/7yl7zyyit07tyZs88+O+31DG3atPlsvmXLllRWVja4joJuQaTafXc46SS45x7YujXpakREgo0bN7LbbrvRsWNHVq1axZw5c3L22gqIFBMnhj6JP/856UpEJJ9k6pfMRX/lkCFDGDBgAPvuuy/nnnsuRxxxRPwvGjF3z9mLxam0tNQbesOgykro3RuGDIHZsxupMBHJS0uWLGG//fZLuoycSvc9m9k8dy9Nt71aEClatYLx4+HJJ6GiIulqRESSpYCoZsIE2L4d7r036UpERJKlgKimXz84/ni4664QFCLSfDWXQ+y1UZ/vVQGRxsSJsGIFPPdc0pWISFzatWvHhg0bCiIkqu4H0a5duzrtp+sg0jjlFOjaFX7zG/jKV5KuRkTi0KtXLyoqKli3bl3SpeRE1R3l6kIBkUa7dnDOOXDrrbB+PXTvnnRFItLYWrduXae7qxUiHWLKYOLEcMHcjBlJVyIikgwFRAb77w/DhoXDTAVwiFJEZCcKiBpMnAiLF8OLLyZdiYhI7ikgajB6NHToEFoRIiKFRgFRgw4dYMwYePhh2Lgx6WpERHJLAZHFpEnw8cfw0ENJVyIiklsKiCwOOQQOOECHmUSk8CggsjALndVz58JrryVdjYhI7iggauHss6FtW7UiRKSwKCBqoWtXOPVUeOAB+OSTpKsREcmNWAPCzIab2VIzKzezKWnWH21mr5pZpZmNqraut5k9bWZLzGyxmfWJs9ZsJk2CDz6Axx5LsgoRkdyJLSDMrCVwCzACGACcYWYDqm32NjAOeDDNU9wPXOfu+wFDgbVx1VobxxwDe+2lw0wiUjjibEEMBcrdfbm7bwEeAkambuDuK9x9AfC5Oy9EQdLK3Z+Jttvk7h/HWGtWLVqEmwn99a+wbFmSlYiI5EacAdETeCflcUW0rDa+CHxgZo+Z2b/M7LqoRfI5Zna+mZWZWVkuhuwdOxZatgw3ExIRae7ytZO6FXAUMBk4BOhHOBT1Oe5+h7uXuntpUVFR7EXtvjucdFK4HenWrbG/nIhIouIMiHeBPVIe94qW1UYFMD86PFUJ/AEY0sj11cukSbBmDfz5z0lXIiISrzgDYi7Q38z6mlkbYAwwqw77djazqmbBccDiGGqss+HDQ0tCndUi0tzFFhDRf/4XAXOAJcDv3H2RmU0zs5MBzOwQM6sATgNuN7NF0b7bCIeXnjOz1wED7oyr1rpo1QrGj4cnn4SKiqSrERGJjzWXG3aXlpZ6WVlZTl5r+fJwyuu0afDjH+fkJUVEYmFm89y9NN26fO2kzmv9+sHxx4ezmbZvz769iEhTpICop0mTYOVKeO65pCsREYmHAqKeTjkljNGkzmoRaa4UEPXUti2cey48/jisX590NSIijU8B0QAzZoQL5oqKwn0jqqaSkqQrExFpOAVEA2zYkH75mjW5rUNEJA4KCBERSUsBISIiaSkgREQkLQWEiIikpYBogOLiui0XEWlKWiVdQFO2evWO+W3bYPBg2LwZFufFuLMiIg2jFkQjadkSpk+H8nJdXS0izYMCohF99atw9NHwk5/Apk1JVyMi0jAKiEZkBtdcEy6Uu+GGpKsREWkYBUQjGzYMTj0Vrr0W1q1LuhoRkfpTQMTg6qvhk0/gpz9NuhIRkfpTQMRgn31gwgS47bZw9zkRkaZIARGTyy8P96/WLUlFpKlSQMRk993hu9+FBx+Ef/0r6WpEROpOARGj730v3HVuypSkKxERqTsFRIw6dYIf/QiefhqefTbpakRE6kYBEbMLL4Q99wytiO3bk65GRKT2FBAxa9sWrrwS5s2DRx5JuhoRkdpTQOTAmWfCgQfC1KmwZUvS1YiI1I4CIgeqBvJ76y24886kqxERqR0FRI4MHw7HHgvTpsFHHyVdjYhIdgqIHKkayG/tWvjFL5KuRkQkOwVEDg0dCqNGwc9/HkZ8FRHJZwqIHLvqKg3kJyJNQ6wBYWbDzWypmZWb2U7XE5vZ0Wb2qplVmtmoNOs7mlmFmf0qzjpz6YtfhEmT4Ne/Dp3WIiL5KraAMLOWwC3ACGAAcIaZDai22dvAOODBDE9zJfBCXDUm5bLLoE2bcJW1iEi+irMFMRQod/fl7r4FeAgYmbqBu69w9wXATtcYm9nBQDHwdIw1JqJHD7j4YnjoISgrS7oaEZH04gyInsA7KY8romVZmVkL4HpgcpbtzjezMjMrW9fEbt926aXQvTt8//vgnnQ1IiI7y9dO6guBJ9y9oqaN3P0Ody9199KioqIcldY4OnaETz+Fv/wFWrQIp8FWTSUlSVcnIgKtYnzud4E9Uh73ipbVxmHAUWZ2IdABaGNmm9y9WQ2cnemCOZ0CKyL5IM6AmAv0N7O+hGAYA5xZmx3d/ayqeTMbB5Q2t3AQEcl3sR1icvdK4CJgDrAE+J27LzKzaWZ2MoCZHWJmFcBpwO1mtiiuekREpG7Mm0kPaWlpqZc1sVOCzDKvayY/FhHJc2Y2z91L063L105qERFJmAIiQcXF6Zd36ZLbOkRE0lFAJGj16nAoqWratAn22Qd22QXWr0+6OhEpdAqIPNK+PcycGcJhwgT1Q4hIshQQeeagg8Ld52bNgttuS7oaESlkCog89O1vhzvQXXIJLFyYdDUiUqgUEHmoRQu4917o1AnGjAn3jxARyTUFRJ4qLob77oNFi2ByjUMWiojEQwGRx048MQwLfuut8Mc/Jl2NiBQaBUSeu/rq0HF93nnwbm2HOhQRaQQKiDzXtm049XXzZjjnHNi2LemKRKRQKCCagH32gZtvhuefh2uvTboaESkUCogmYvx4OP10+PGP4eWXk65GRAqBAqKJMIPbb4deveCMM2DjxqQrEpHmTgHRhHTuDL/9LaxcCRdemHQ1ItLcKSCamCOOgMsvD0ExY0bS1YhIc6aAaIKmToWjjgqtiPLypKsRkeZKAdEEtWwJDzwA//u/0L9/6J9InUpKkq5QRJqDGgPCzM5OmT+i2rqL4ipKsuvdO/Nw4GvW5LYWEWmesrUgLk6Zv7nauvMauRYREckj2QLCMsyneywiIs1ItoDwDPPpHouISDPSKsv6fc1sAaG1sFc0T/S4X6yViYhIorIFxH45qULqpbg4fYd0ixZQURGuuhYRqa8aDzG5+8rUCdgEDAG6R48lQatXhzOZUqe5c6F9ezjhBFi/PukKRaQpy3aa62wz2z+a7wEsJJy9NMPMvpOD+qSOSkvhT3+Cf/873NdaYzaJSH1l66Tu6+4Lo/nxwDPu/v+AQ9FprnnrmGPgkUfgtdfg5JN1T2sRqZ9sAbE1Zf544AkAd/8I2B5XUdJwX/tauKf1Cy+EYcK3bs2+j4hIqmwB8Y6Z/ZeZfZ3Q9/AUgJntArSOuzhpmDPPhFtugdmzYdw42K5IF5E6yHYW0wRgGvBlYLS7fxAtHwbcE2dh0jguuAA++AB++EPo1CkEhukSRxGphWxnMa1192+6+0h3fzpl+fPu/vNsT25mw81sqZmVm9mUNOuPNrNXzazSzEalLB9sZi+a2SIzW2Bmo+v6jckOU6bApZfCbbfBj36UdDUi0lTU2IIws1k1rXf3k2vYtyVwC/AVoAKYa2az3H1xymZvA+OAydV2/xg4192XmdnuwDwzm5PSgpE6MINrrgktiauvDjceuvTSpKsSkXyX7RDTYcA7wEzgZeo2/tJQoNzdlwOY2UPASOCzgHD3FdG6zx0dd/c3U+b/Y2ZrgSJAAVFPZqEFsXEjfO97ISQmTUq6KhHJZ9kCooTQAjgDOBP4MzDT3RfV4rl7EsKlSgXh9Ng6MbOhQBvgrbruK5/XsiXcf38IiW98Azp2hNE6eCciGWTrg9jm7k+5+1hCx3Q58Ndc3QsiujhvBjDe3Xc6B8fMzjezMjMrW7duXS5KavLatIFHHw23Lh0zZuebDemGQyJSJesd5cysrZmdCjwAfAu4CXi8Fs/9LrBHyuNe0bJaMbOOhBbLVHd/Kd027n6Hu5e6e2lRUVFtn7rg7bprOPU1E91wSEQgeyf1/cD+hAvkfpJyVXVtzAX6m1lfQjCMIRymysrM2hBC6H53f7QOrym11KlT0hWISL7L1oI4G+gPfBv4p5ltjKaPzKzGUX7cvRK4CJgDLAF+5+6LzGyamZ0MYGaHmFkFcBpwu5lV9W2cDhwNjDOz+dE0uN7fpYiI1Jl5phsbNzGlpaVeVlaWdBlNSk0XzDWTj4WIZGFm89y9NN26rH0QUpi2bEm6AhFJmgKigBUXZ143fDi8/37uahGR/KOAKGDpbjjkHq6V+Mc/YNgwKC9PukoRSYoCQnZyzjnw7LOwYQMcemgYMlxECo8CQtI66ih4+WUoKoIvfzncW0JECosCQjLaay948cUQFuPGwdSpuqeESCFRQEiNunSBp56CiRPDSLCjR8PHHyddlYjkggJCsmrdGu64A667Dn7/ezj22NDBLSLNmwJCasUMJk+Gxx6DRYtC5/WCBUlXJSJxUkBInZxyCvz971BZCYMGaTRYkeZMASF1NmQIvPJK5vUaDVakeVBASL307Jl0BSISNwWEiIikpYCQWOhUWJGmTwEhscjWTyEi+U8BIfWWaTTYLl1CC+Lww+Gyy2Dr1tzWJSKNQwEh9ZZpNNj33oPXX4ezz4Yrrwyjwi5alP35RCS/KCAkFp06wb33hgvr3nkHDj4Yrr8etm1LujIRqS0FhMTq61+HhQvDDYgmT4bjjoN//zvpqkSkNhQQErsvfAEefzy0KObPhwMPhLvu0n2vRfKdAkJywgzGjg19E0OHhtFh27XTUB0i+UwBITnVuzc88wz88pewZUv6bTRUh0h+UEBIzrVoAf/930lXISLZKCBERCQtBYTkpbvu0imxIklTQEhemjgRDjkEXngh6UpECpcCQhKTaaiO4mKYORPWr4djjoHTToMVK3JamoiggJAEZRqqY/VqGDMG3ngDfvITeOIJ2HdfmDoVNm1KumqRwqGAkLy1665hsL+lS0Mr4uqroX//cMHd9u1JVyfS/CkgJO/16gUzZsCLL8Kee8L48XDoodCtmy60E4mTAkKajGHD4J//hAcegFWrwqix6ehCO5HGEWtAmNlwM1tqZuVmNiXN+qPN7FUzqzSzUdXWjTWzZdE0Ns46pelo0QLOOiscdhKReMUWEGbWErgFGAEMAM4wswHVNnsbGAc8WG3frsDlwKHAUOByM+sSV63S9LRvn3QFIs1fnC2IoUC5uy939y3AQ8DI1A3cfYW7LwCqdzmeCDzj7u+5+/vAM8DwGGuVZubCC2HlyqSrEGna4gyInsA7KY8romWNtq+ZnW9mZWZWtm7dunoXKs3Pb34De+8NEyZAeXnS1Yg0TU26k9rd73D3UncvLSoqSrocybGaLrRbvhwuuAAefBD22QfOOQeWLMltfSJNXZwB8S6wR8rjXtGyuPeVAlHThXa9esFNN4W71118cbj16cCBcPrpsGBB0pWLNA1xBsRcoL+Z9TWzNsAYYFYt950DnGBmXaLO6ROiZSJ1UlIC110X+iN+8AN46ikYNAhOOUXXUYhkE1tAuHslcBHhD/sS4HfuvsjMppnZyQBmdoiZVQCnAbeb2aJo3/eAKwkhMxeYFi0TqZfu3eGqq0JQXHEF/O1vuo5CJBvzZnJj4NLSUi8rK0u6DGkiNm6ETp0yr28mvxYiWZnZPHcvTbeuSXdSi9RXx441r1+1Kjd1iOQzBYRIGr17wxlnhKE91JqQQqWAEEnjoovgySfhiCPg4IPh7rvhk0+SrkoktxQQUrBquo7ihhugogJ+/WvYujVccNerF3z/+7p5kRQOBYQUrJquowDo0AG+8Y1w3cTzz8OXvgTXXw/9+sHIkdC1q06TleZNASGShRkceyw8+mi48O6HPwz3pnj//fTb6zRZaS4UECJ1sMce8NOfwjvvZN9WpKlTQIjUQ9u2Na+fOhXefDM3tYjERQEhEoPp08MggUccAXfcAR9+mHRFInWngBCJQUUFXHstfPBB6OguKYEzz4Snn4Zt25KuTqR2FBAi9VTTabI9esCll8LChTB3bjhN9qmn4MQTYc89Q0d3UZHOgpL8prGYRHLk00/hT3+Ce+8NYVFTS6KZ/FpKE6CxmETyQNu2MGoUzJ6ts6CkaVBAiCSgR4+a148YEYb3yDQkuUguKCBE8tAbb4R+i+JihYUkRwEhkoeWLw+d2xdfDEuX7giL4cPhrrtgw4bQma1ObomTOqlFElJSkn5YjuLiHeNBQeiwfvVVeOSRMC1fDq1aQWVl5uduJr/WkgPqpBbJQ9kGC6xiFoYcnz4dysth3jy45JJkapbCooAQaULMYMiQEBY1ueSSMALt1q25qUuaJwWESDP0q1/BccdB9+5w+ulw//2wbl3SVUlTo4AQaYY2bIA//CGEw//8D4wdG/o2Dj8crr463ONCndySjTqpRZqo2nZyb98O8+eHC/Rmzw5nR2XTTP4sSC2ok1qkGaptJ3eLFqHf4rLL4JVXYNWqcKpsTTSgoIACQqTglJTAeefVvE337nDqqXDbbbBsmVoUhUoBISI7GTUqXHtx4YXwxS9Cnz7hYr2ZM2Ht2rCN+jCav1ZJFyAi+efOO0Or4a234Nlnw/T442HID4ADD8x8723dk7v5UECIFKji4syd3BBaA3vvHaZvfjP0S/zrXyEsnnkmt7VKMnQWk4jUi1nmdaWlcPTRYTrySOjWLXd1Sd3oLCYRyaldd4VbboFTTgkd3gccAN/6Fjz8MPznP2Eb9WHkPx1iEpFG97e/webN4ZqLF14I0333wa23hvV7760+jKYg1haEmQ03s6VmVm5mU9Ksb2tmD0frXzazPtHy1mZ2n5m9bmZLzOwHcdYpInVX0z25Adq1g6OOgqlTYc4c+OCDcB3Gz38OAwbU/NyffNK4tUr9xNaCMLOWwC3AV4AKYK6ZzXL3xSmbTQDed/e9zWwMcA0wGjgNaOvuB5jZrsBiM5vp7iviqldE6qb6BXnZtGoFhxwSpksuqbkPo2NHOOggGDYMDjssTHvu+fl9ansludRfnIeYhgLl7r4cwMweAkYCqQExErgimn8U+JWZGeBAezNrBewCbAE2xliriOSRyZPhxRfDFd833xyWFReHoKgKDR2iil+cAdETSL01ewVwaKZt3L3SzD4EuhHCYiSwCtgV+K6773TDRTM7HzgfoHfv3o1dv4gk5Gc/C18rK+H11+Gll0JgvPRSGIRQciNfz2IaCmwDdgf6ApeYWb/qG7n7He5e6u6lRUVFua5RRBogWx8GhMNSBx0EF1wQhix/880wbPns2TU/9yWXhKu+ly0LgxVK/cTZgngX2CPlca9oWbptKqLDSZ2ADcCZwFPuvhVYa2b/AEqB5THWKyI5VN9+gu7d4aSTat7m1lvDWVQAnTqFO/KVlu6Y+vSBHj3Uh5FNnAExF+hvZn0JQTCG8Ic/1SxgLPAiMAr4i7u7mb0NHAfMMLP2wDDgxhhrFZFm5KOPYPFiKCvbMd14I2zZEtZ37Qrv7XTQOlAfxg6xBUTUp3ARMAdoCdzt7ovMbBpQ5u6zgLsIIVAOvEcIEQhnP91jZosAA+5x9wVx1SoiTU9NQ4W0ahXGizrwwB0j127ZAgsX7giMO+/M/Nz33QeDBoXTcdu0iaf+pkBDbYhIQarpNNsqrVuHkBg8eMc0aBB06dJ8TrOtaagNXUktIlLNG2+Eu/BVTXPmhFZFlT33LIzTbBUQIiLV7LNPmEaP3rFs9Wp47bUdobFyZeb9b7ghjD+1//6hRZGutdIUWiA6xCQiBamhf6Brc4gKwki2VWGx//5hfuBA6Nw58z65/LOsQ0wiItXE+V/6mjWhQ3zhwnCh38KFcO+9sGlTfK8ZBwWEiEgj+8IX4LjjwlRl+3Z4++0dwfGDGoYgHTEC9tsvdJDvt1+Yunb9/Da5OESlgBARqYdsd+SrrkWLcIFenz7wta/VHBBr1oQh01NHtS0u3hEYAwbkppNcASEiUg9xHqJ69dXQ4li5Mlzwt2TJjq8PPAAbczR0qQJCRCQB2VogLVpA375hSh1axB1WrYKePeOvUQEhIpKA+rZAzGD33Ru3lkzydTRXERFJmAJCRKQJqs1w6Q2lQ0wiIk1QLq62VgtCRETSUkCIiEhaCggREUlLASEiImkpIEREJK1mM9y3ma0DahihPavuwPpGKicOqq9hVF/DqL6Gyef69nT3onQrmk1ANJSZlWUaEz0fqL6GUX0No/oaJt/ry0SHmEREJC0FhIiIpKWA2OGOpAvIQvU1jOprGNXXMPleX1rqgxARkbTUghARkbQUECIiklZBBYSZDTezpWZWbmZT0qxva2YPR+tfNrM+OaxtDzN73swWm9kiM/t2mm2ONbMPzWx+NF2Wq/pSalhhZq9Hr1+WZr2Z2U3Re7jAzIbksLZ9Ut6b+Wa20cy+U22bnL6HZna3ma01s4Upy7qa2TNmtiz62iXDvmOjbZaZ2dgc1nedmb0R/fweN7POGfat8bMQY31XmNm7KT/Dr2bYt8bf9xjrezilthVmNj/DvrG/fw3m7gUxAS2Bt4B+QBvgNWBAtW0uBH4dzY8BHs5hfT2AIdH8bsCbaeo7Fpid8Pu4Auhew/qvAk8CBgwDXk7w572acBFQYu8hcDQwBFiYsuxaYEo0PwW4Js1+XYHl0dcu0XyXHNV3AtAqmr8mXX21+SzEWN8VwORa/Pxr/H2Pq75q668HLkvq/WvoVEgtiKFAubsvd/ctwEPAyGrbjATui+YfBY43M8tFce6+yt1fjeY/ApYAObjrbKMbCdzvwUtAZzPrkUAdxwNvuXtDrq5vMHd/AXiv2uLUz9l9wClpdj0ReMbd33P394FngOG5qM/dn3b3yujhS0Cvxn7d2srw/tVGbX7fG6ym+qK/HacDMxv7dXOlkAKiJ/BOyuMKdv4D/Nk20S/Ih0C3nFSXIjq0dRDwcprVh5nZa2b2pJkNzGlhgQNPm9k8Mzs/zfravM+5MIbMv5hJv4fF7r4qml8NpLsHWL68j+cRWoTpZPssxOmi6BDY3RkO0eXD+3cUsMbdl2VYn+T7VyuFFBBNgpl1AH4PfMfdN1Zb/SrhkMkg4GbgD7muDzjS3YcAI4BvmdnRCdRQIzNrA5wMPJJmdT68h5/xcKwhL881N7OpQCXw2wybJPVZuA3YCxgMrCIcxslHZ1Bz6yHvf5cKKSDeBfZIedwrWpZ2GzNrBXQCNuSkuvCarQnh8Ft3f6z6enff6O6bovkngNZm1j1X9UWv+270dS3wOKEpn6o273PcRgCvuvua6ivy4T0E1lQddou+rk2zTaLvo5mNA74GnBWF2E5q8VmIhbuvcfdt7r4duDPD6yb9/rUCTgUezrRNUu9fXRRSQMwF+ptZ3+g/zDHArGrbzAKqzhYZBfwl0y9HY4uOV94FLHH3X2TYpqSqT8TMhhJ+frkMsPZmtlvVPKEzc2G1zWYB50ZnMw0DPkw5nJIrGf9zS/o9jKR+zsYCf0yzzRzgBDPrEh1COSFaFjszGw58DzjZ3T/OsE1tPgtx1Zfap/X1DK9bm9/3OH0ZeMPdK9KtTPL9q5Oke8lzORHOsHmTcHbD1GjZNMIvAkA7wmGJcuAVoF8OazuScKhhATA/mr4KfBP4ZrTNRcAiwhkZLwGH5/j96xe99mtRHVXvYWqNBtwSvcevA6U5rrE94Q9+p5Rlib2HhKBaBWwlHAefQOjXeg5YBjwLdI22LQV+k7LvedFnsRwYn8P6ygnH76s+h1Vn9u0OPFHTZyFH9c2IPlsLCH/0e1SvL3q80+97LuqLlt9b9ZlL2Tbn719DJw21ISIiaRXSISYREakDBYSIiKSlgBARkbQUECIikpYCQkRE0lJAiNSBmW2rNmJso40SamZ9UkcFFUlaq6QLEGliPnH3wUkXIZILakGINIJobP9ro/H9XzGzvaPlfczsL9HAcs+ZWe9oeXF0r4XXounw6IErX7sAAAE8SURBVKlamtmdFu4J8rSZ7ZLYNyUFTwEhUje7VDvENDpl3YfufgDwK+DGaNnNwH3ufiBh0LubouU3AX/zMGjgEMLVtAD9gVvcfSDwAfD/Y/5+RDLSldQidWBmm9y9Q5rlK4Dj3H15NOjianfvZmbrCUNBbI2Wr3L37ma2Dujl7p+mPEcfwj0g+kePvw+0dvefxv+diexMLQiRxuMZ5uvi05T5baifUBKkgBBpPKNTvr4Yzf+TMJIowFnA36P554ALAMyspZl1ylWRIrWl/05E6maXajehf8rdq0517WJmCwitgDOiZf8F3GNmlwLrgPHR8m8Dd5jZBEJL4QLCqKAieUN9ECKNIOqDKHX39UnXItJYdIhJRETSUgtCRETSUgtCRETSUkCIiEhaCggREUlLASEiImkpIEREJK3/A0VmmBucXG+cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(0, epochs)]\n",
    "plt.plot(x, losses, color='blue', marker='s', label='Train')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
