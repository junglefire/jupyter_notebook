{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from mxnet import autograd, np, npx \n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 定义绘图相关函数\n",
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\" \n",
    "    # display.set_matplotlib_formats('svg')\n",
    "    \n",
    "def set_figsize(figsize=(7, 5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, \n",
    "         xscale='linear', yscale='linear',fmts=('-', 'm--', 'g-.', 'r:'), figsize=(7, 5), axes=None\n",
    "    ):\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else plt.gca()\n",
    "\n",
    "    # Return True if X (ndarray or list) has 1 axis\n",
    "    def has_one_axis(X):\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  02. Preliminaries\n",
    "All machine learning is concerned with extracting information from data. So we will begin by learning the practical skills for storing, manipulating, and preprocessing data.\n",
    "\n",
    "Moreover, machine learning typically requires working with large datasets, which we can think of as tables, where the rows correspond to examples and the columns correspond to attributes. Linear algebra gives us a powerful set of techniques for working with tabular data. We will not go too far into the weeds but rather focus on the basic of matrix operations and their implementation.\n",
    "\n",
    "Additionally, deep learning is all about optimization. We have a model with some parameters and we want to find those that fit our data the best. Determining which way to move each parameter at each step of an algorithm requires a little bit of calculus, which will be briefly introduced. Fortunately, the autograd package automatically computes differentiation for us, and we will cover it next.\n",
    "\n",
    "Next, machine learning is concerned with making predictions: what is the likely value of some unknown attribute, given the information that we observe? To reason rigorously under uncertainty we will need to invoke the language of probability.\n",
    "\n",
    "In the end, the official documentation provides plenty of descriptions and examples that are beyond this book. To conclude the chapter, we will show you how to look up documentation for the needed information.\n",
    "\n",
    "This book has kept the mathematical content to the minimum necessary to get a proper understanding of deep learning. However, it does not mean that this book is mathematics free. Thus, this chapter provides a rapid introduction to basic and frequently-used mathematics to allow anyone to understand at least most of the mathematical content of the book. If you wish to understand all of the mathematical content, further reviewing `Chapter 18` should be sufficient.\n",
    "\n",
    "\n",
    "## 2.1 Data Manipulation\n",
    "In order to get anything done, we need some way to store and manipulate data. We introduce the $n$-dimensional array(ndarray), MXNet's primary tool for storing and transforming data. In MXNet, `ndarray` is a class and we call any instance \"an ndarray\".\n",
    "\n",
    "If you have worked with `NumPy`, then you will find this section familiar. That's by design. We designed MXNet's `ndarray` to be an extension to NumPy's `ndarray` with a few killer features:\n",
    "+ First, MXNet's `ndarray` supports asynchronous computation on CPU, GPU, and distributed cloud architectures, whereas NumPy only supports CPU computation\n",
    "+ Second, MXNet's `ndarray` supports automatic differentiation\n",
    "\n",
    "These properties make MXNet's `ndarray` suitable for deep learning. Throughout the book, when we say `ndarray`, we are referring to MXNet's `ndarray` unless otherwise stated.\n",
    "\n",
    "### 2.1.1 Getting Started\n",
    "In this section, we aim to get you up and running, equipping you with the basic math and numerical computing tools that you will build on as you progress through the book. Do not worry if you struggle to grok some of the mathematical concepts or library functions. The following sections will revisit this material in the context of practical examples and it will sink. On the other hand, if you already have some background and want to go deeper into the mathematical content, just skip this section.\n",
    "\n",
    "To start, we import the `np` (numpy) and `npx` (numpy_extension) modules from MXNet:\n",
    "+ `np` module includes functions supported by `NumPy`\n",
    "+ `npx` module contains a set of extensions developed to empower deep learning within a NumPy-like environment\n",
    "\n",
    "> **[NOTE]**\n",
    ">\n",
    "> When using `ndarray`, we almost always invoke the `set_np` function: this is for compatibility of `ndarray` processing by other components of MXNet.\n",
    "\n",
    "```python\n",
    "from mxnet import np, npx \n",
    "npx.set_np()\n",
    "```\n",
    "\n",
    "An `ndarray` represents a (possibly multi-dimensional) array of numerical values:\n",
    "+ With one axis, an `ndarray` corresponds (in math) to a **vector**\n",
    "+ With two axes, an `ndarray` corresponds to a **matrix**\n",
    "+ Arrays with more than two axes do not have special mathematical names---we simply call them **tensors**\n",
    "\n",
    "We can use arange to create a row vector $x$ containing the first $12$ integers starting with $0$. Unless otherwise specified, a new `ndarray` will be stored in main memory and designated for CPU-based computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(12) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12,), 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the shape of an `ndarray` without altering either the number of elements or their values, we can invoke the `reshape` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(3, 4) \n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new `ndarray` contains the exact same values, but views them as a matrix organized as $3$ rows and $4$ columns. \n",
    "\n",
    "Reshaping by manually specifying every dimension is unnecessary. If our target shape is a matrix with shape (height, width), then after we know the width, the height is given implicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `empty` method grabs a chunk of memory and hands us back a matrix without bothering to change the value of any of its entries. This is remarkably efficient but we must be careful because the entries might take arbitrary values, including very big ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.717361e-37, 0.000000e+00, 0.000000e+00, 0.000000e+00],\n",
       "       [0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00],\n",
       "       [0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we will want our matrices initialized either with `zeros`, `ones`, some other constants, or numbers randomly sampled from a specific distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we want to randomly sample the values for each element in an `ndarray` from some probability distribution. The following snippet creates an ndarray with shape ($3$, $4$). Each of its elements is randomly sampled from a standard Gaussian (normal) distribution with a mean of $0$ and a standard deviation of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.2122064 ,  1.1630787 ,  0.7740038 ,  0.4838046 ],\n",
       "       [ 1.0434403 ,  0.29956347,  1.1839255 ,  0.15302546],\n",
       "       [ 1.8917114 , -1.1688148 , -1.2347414 ,  1.5580711 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0, 1, size=(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the exact values for each element in the desired `ndarray` by supplying a Python list (or list of lists) containing the numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 4., 3.],\n",
       "       [1., 2., 3., 4.],\n",
       "       [4., 3., 2., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Operations\n",
    "Our interests are not limited to simply reading and writing data from/to arrays. We want to perform mathematical operations on those arrays. Some of the simplest and most useful operations are the `elementwise` operations. These apply a standard scalar operation to each element of an array. For functions that take two arrays as inputs, elementwise operations apply some standard binary operator on each pair of corresponding elements from the two arrays. We can create an elementwise function from any function that maps from a scalar to a scalar.\n",
    "\n",
    "In mathematical notation, we would denote such a unary scalar operator (taking one input) by the signature \n",
    "+ $f: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "This just means that the function is mapping from any real number ($\\mathbb{R}$) onto another. Likewise, we denote a binary scalar operator (taking two real inputs, and yielding one output) by the signature \n",
    "+ $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ of the same shape, and a binary operator $f$, we can produce a vector $\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$ by setting $c_i \\gets f(u_i, v_i)$ for all $i$, where $c_i, u_i$, and $v_i$ are the $i^\\mathrm{th}$ elements of vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$. \n",
    "\n",
    "Here, we produced the vector-valued \n",
    "+ $F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ \n",
    "\n",
    "by lifting the scalar function to an elementwise vector operation.\n",
    "\n",
    "In MXNet, the common standard arithmetic operators $(+, -, *, /, and **)$ have all been lifted to elementwise operations for any identically-shaped tensors of arbitrary shape. We can call elementwise operations on any two tensors of the same shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  4.,  6., 10.]),\n",
       " array([-1.,  0.,  2.,  6.]),\n",
       " array([ 2.,  4.,  8., 16.]),\n",
       " array([0.5, 1. , 2. , 4. ]),\n",
       " array([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 4, 8]) \n",
    "y = np.array([2, 2, 2, 2]) \n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more operations can be applied elementwise, including unary operators like exponentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.7182817e+00, 7.3890562e+00, 5.4598148e+01, 2.9809580e+03])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to elementwise computations, we can also perform linear algebra operations, including vector dot products and matrix multiplication. We will explain the crucial bits of linear algebra in `Section 2.3`.\n",
    "\n",
    "We can also concatenate multiple `ndarray` together, stacking them end-to-end to form a larger `ndarray`. We just need to provide a list of `ndarray` and tell the system along which axis to concatenate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]]),\n",
       " array([[2., 1., 4., 3.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [4., 3., 2., 1.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(12).reshape(3, 4)\n",
    "y = np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]]),\n",
       " array([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([x, y], axis=0), np.concatenate([x, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to construct a binary `ndarray` via logical statements. Take $x == y$ as an example. For each position, if $x$ and $y$ are equal at that position, the corresponding entry in the new `ndarray` takes a value of $1$, meaning that the logical statement $x == y$ is true at that position; otherwise that position takes $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False,  True],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing all the elements in the `ndarray` yields an `ndarray` with only one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(66.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Broadcasting Mechanism\n",
    "In the above section, we saw how to perform elementwise operations on two `ndarray` of the same shape. Under certain conditions, even when shapes differ, we can still perform elementwise operations by invoking the `broadcasting` mechanism. This mechanism works in the following way: \n",
    "+ First, expand one or both arrays by copying elements appropriately so that after this transformation, the two `ndarray` have the same shape\n",
    "+ Second, carry out the elementwise operations on the resulting arrays\n",
    "\n",
    "In most cases, we broadcast along an axis where an array initially only has length $1$, such as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.],\n",
       "        [1.],\n",
       "        [2.]]),\n",
       " array([[0., 1.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(3).reshape(3, 1)\n",
    "b = np.arange(2).reshape(1, 2)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `a` and `b` are $3\\times1$ and $1\\times2$ matrices respectively, their shapes do not match up if we want to add them. We broadcast the entries of both matrices into `a` larger $3\\times2$ matrix as follows: for matrix `a` it replicates the columns and for matrix `b` it replicates the rows before adding up both elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 2.],\n",
       "       [2., 3.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Indexing and Slicing\n",
    "Elements in an ndarray can be accessed by index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]]),\n",
       " array([ 8.,  9., 10., 11.]),\n",
       " array([[ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x[-1], x[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond reading, we can also write elements of a matrix by specifying indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  9.,  7.],\n",
       "       [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1, 2] = 9\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to assign multiple elements the same value, we simply index all of them and then assign them the value. For instance, $[0:2, :]$ accesses the first and second rows, where : takes all the elements along axis $1$ (column). While we discussed indexing for matrices, this obviously also works for vectors and for tensors of more than $2$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12., 12., 12., 12.],\n",
       "       [12., 12., 12., 12.],\n",
       "       [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2, :] = 12\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Saving Memory\n",
    "Running operations can cause new memory to be allocated to host results. For example, if we write $y = x + y$, we will dereference the `ndarray` that $y$ used to point to and instead point $y$ at the newly allocated memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(y)\n",
    "y = y + x \n",
    "id(y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be undesirable for two reasons:\n",
    "+ First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates in place\n",
    "+ Second, we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters\n",
    "\n",
    "Fortunately, performing in-place operations in MXNet is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., $y[:] = <\\text{expression}>$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(z): 140705666253808\n",
      "id(z): 140705666253808\n"
     ]
    }
   ],
   "source": [
    "z = np.zeros_like(y)\n",
    "print('id(z):', id(z))\n",
    "z[:] = x + y\n",
    "print('id(z):', id(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value of $x$ is not reused in subsequent computations, we can also use $x[:] = x + y$ or $x += y$ to reduce the memory overhead of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(x)\n",
    "x += y\n",
    "id(x) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Conversion to Other Python Objects\n",
    "Converting an MXNet `ndarray` to a NumPy `ndarray`, or vice versa, is easy. ***The converted result does not share memory***. This minor inconvenience is actually quite important: when you perform operations on the CPU or on GPUs, you do not want MXNet to halt computation, waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory. The `array` and `asnumpy` functions do the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, mxnet.numpy.ndarray)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = x.asnumpy() \n",
    "b = np.array(a) \n",
    "type(a), type(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert a size-one `ndarray` to a Python scalar, we can invoke the `item` function or Python's built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.5]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ MXNet's `ndarray` is an extension to NumPy's `ndarray` with a few killer advantages that make it suitable for deep learning.\n",
    "+ MXNet's `ndarray` provides a variety of functionalities including basic mathematics operations, broadcasting, indexing, slicing, memory saving, and conversion to other Python objects.\n",
    "\n",
    "##### Exercises\n",
    "+ Run the code in this section. Change the conditional statement $x == y4 in this section to $x < y$ or $x > y$, and then see what kind of `ndarray` you can get.\n",
    "\n",
    "+ Replace the two `ndarray` that operate by element in the broadcasting mechanism with other shapes, e.g., three dimensional tensors. Is the result the same as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Preprocessing\n",
    "So far we have introduced a variety of techniques for manipulating data that are already stored in `ndarray`. To apply deep learning to solving real-world problems, we often begin with preprocessing raw data, rather than those nicely prepared data in the `ndarray` format. Among popular data analytic tools in Python, the `pandas` package is commonly used. Like many other extension packages in the vast ecosystem of Python, `pandas` can work together with `ndarray`. \n",
    "\n",
    "### 2.2.1 Reading the Dataset\n",
    "As an example, we begin by creating an artificial dataset that is stored in a csv file `../data/house_tiny.csv`. Data stored in other formats may be processed in similar ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved in the d2l package for later use\n",
    "def mkdir_if_not_exist(path):\n",
    "    if not isinstance(path, str):\n",
    "        path = os.path.join(*path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "data_file = './data/house_tiny.csv'\n",
    "mkdir_if_not_exist('./data')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
    "    f.write('NA,Pave,127500\\n')  # Each row is a data point\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the raw dataset from the created csv file, we import the `pandas` package and invoke the `read_csv` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms</th>\n",
       "      <th>Alley</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Pave</td>\n",
       "      <td>127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumRooms Alley   Price\n",
       "0       NaN  Pave  127500\n",
       "1       2.0   NaN  106000\n",
       "2       4.0   NaN  178100\n",
       "3       NaN   NaN  140000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Handling Missing Data\n",
    "Note that `NaN` entries are missing values. To handle missing data, typical methods include imputation and deletion, where imputation replaces missing values with substituted ones, while deletion ignores missing values. Here we will consider imputation.\n",
    "\n",
    "By integer-location based indexing (`iloc`), we split data into inputs and outputs, where the former takes the first 2 columns while the latter only keeps the last column. For numerical values in inputs that are missing, we replace the `NaN` entries with the mean value of the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms</th>\n",
       "      <th>Alley</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Pave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumRooms Alley\n",
       "0       3.0  Pave\n",
       "1       2.0   NaN\n",
       "2       4.0   NaN\n",
       "3       3.0   NaN"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2] \n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical or discrete values in inputs, we consider `NaN` as a category. Since the `Alley` column only takes 2 types of categorical values `Pave` and `NaN`, pandas can automatically convert this column to 2 columns `Alley_Pave` and `Alley_nan`. A row whose alley type is `Pave` will set values of `Alley_Pave` and `Alley_nan` to 1 and 0. A row with a missing alley type will set their values to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms</th>\n",
       "      <th>Alley_Pave</th>\n",
       "      <th>Alley_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumRooms  Alley_Pave  Alley_nan\n",
       "0       3.0           1          0\n",
       "1       2.0           0          1\n",
       "2       4.0           0          1\n",
       "3       3.0           0          1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pd.get_dummies(inputs, dummy_na=True) \n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Conversion to the ndarray Format\n",
    "Now that all the entries in inputs and outputs are numerical, they can be converted to the ndarray format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3., 1., 0.],\n",
       "        [2., 0., 1.],\n",
       "        [4., 0., 1.],\n",
       "        [3., 0., 1.]], dtype=float64),\n",
       " array([127500, 106000, 178100, 140000], dtype=int64))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = np.array(inputs.values), np.array(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ Like many other extension packages in the vast ecosystem of Python, pandas can work together with ndarray.\n",
    "+ Imputation and deletion can be used to handle missing data.\n",
    "\n",
    "##### Exercises\n",
    "+ Create a raw dataset with more rows and columns.\n",
    "+ Delete the column with the most missing values.\n",
    "+ Convert the preprocessed dataset to the ndarray format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Linear Algebra\n",
    "Let us briefly review the subset of basic linear algebra that you will need to understand and implement most of models covered in this book. \n",
    "\n",
    "### 2.3.1 Scalars\n",
    "Formally, we call values consisting of just one numerical quantity scalars. \n",
    "\n",
    "In this book, we adopt the mathematical notation where scalar variables are denoted by ordinary lower-cased letters (e.g., $x$, $y$, and $z$). We denote the space of all (continuous) real-valued scalars by $\\mathbb{R}$. For expedience, we will punt on rigorous definitions of what precisely space is, but just remember for now that the expression $x \\in \\mathbb{R}$ is a formal way to say that $x$ is a real-valued scalar. The symbol $\\in$ can be pronounced \"in\" and simply denotes membership in a set. Analogously, we could write $x, y \\in {0, 1}$ to state that $x$ and $y$ are numbers whose value can only be $0$ or $1$.\n",
    "\n",
    "In MXNet code, a scalar is represented by an `ndarray` with just one element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(5.), array(6.), array(1.5), array(9.))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npx.set_np()\n",
    "x = np.array(3.0) \n",
    "y = np.array(2.0)\n",
    "x + y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Vectors\n",
    "You can think of a vector as simply a list of scalar values. \n",
    "\n",
    "In math notation, we will usually denote vectors as bold-faced, lower-cased letters (e.g., $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z})$.\n",
    "\n",
    "In MXNet, we work with vectors via $1$-dimensional `ndarray`. In general `ndarray` can have arbitrary lengths, subject to the memory limits of your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to any element of a vector by using a subscript. For example, we can refer to the $i^\\mathrm{th}$ element of $\\mathbf{x}$ by $x_i$. Note that the element $x_i$ is a scalar, so we do not bold-face the font when referring to it. \n",
    "\n",
    "##### Length, Dimensionality, and Shape\n",
    "A vector is just an array of numbers. And just as every array has a length, so does every vector. In math notation, if we want to say that a vector $\\mathbf{x}$ consists of $n$ real-valued scalars, we can express this as $\\mathbf{x} \\in \\mathbb{R}^n$. The length of a vector is commonly called the dimension of the vector.\n",
    "\n",
    "As with an ordinary Python array, we can access the length of an ndarray by calling Python's built-in `len()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an `ndarray` represents a vector (with precisely one axis), we can also access its length via the `.shape` attribute. The shape is a tuple that lists the length (dimensionality) along each axis of the `ndarray`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the word `dimension` tends to get overloaded in these contexts and this tends to confuse people. To clarify, we use the dimensionality of a vector or an axis to refer to its length, i.e., the number of elements of a vector or an axis. However, we use the dimensionality of an `ndarray` to refer to the number of axes that an `ndarray` has. In this sense, the dimensionality of some axis of an `ndarray` will be the length of that axis.\n",
    "\n",
    "### 2.3.3 Matrices\n",
    "Just as vectors generalize scalars from order $0$ to order $1$, matrices generalize vectors from order $1$ to order $2$. Matrices, which we will typically denote with bold-faced, capital letters (e.g., $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$), are represented in code as ndarrays with $2$ axes.\n",
    "\n",
    "In math notation, we use $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ to express that the matrix $\\mathbf{A}$ consists of $m$ rows and $n$ columns of real-valued scalars. Visually, we can illustrate any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ as a table, where each element $a_{ij}$ belongs to the $i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column:\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\ \\end{bmatrix}$$\n",
    "\n",
    "For any $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the shape of $\\mathbf{A}$ is ($m$, $n$) or $m \\times n$. Specifically, when a matrix has the same number of rows and columns, its shape becomes a square; thus, it is called a square matrix.\n",
    "\n",
    "We can create an $m \\times n$ matrix in MXNet by specifying a shape with two components $m$ and $n$ when calling any of our favorite functions for instantiating an `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.],\n",
       "       [12., 13., 14., 15.],\n",
       "       [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the scalar element $a_{ij}$ of a matrix $\\mathbf{A}$ in by specifying the indices for the row ($i$) and column ($j$), such as $[\\mathbf{A}]_{ij}$. When the scalar elements of a matrix $\\mathbf{A}$ are not given, we may simply use the lower-case letter of the matrix $\\mathbf{A}$ with the index subscript, $a_{ij}$, to refer to $[\\mathbf{A}]_{ij}$. To keep notation simple, commas are inserted to separate indices only when necessary, such as $a_{2, 3j}$ and $[\\mathbf{A}]_{2i-1, 3}$.\n",
    "\n",
    "Sometimes, we want to flip the axes. When we exchange a matrix's rows and columns, the result is called the transpose of the matrix. Formally, we signify a matrix $\\mathbf{A}$'s transpose by $\\mathbf{A}^\\top$ and if $\\mathbf{B} = \\mathbf{A}^\\top$, then $b_{ij} = a_{ji}$ for any $i$ and $j$. Thus, the transpose of $\\mathbf{A}$ is a $n \\times m$ matrix:\n",
    "$$ \\mathbf{A}^\\top = \\begin{bmatrix} a_{11} & a_{21} & \\dots & a_{m1} \\\\ a_{12} & a_{22} & \\dots & a_{m2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{1n} & a_{2n} & \\dots & a_{mn} \\end{bmatrix}$$\n",
    "\n",
    "In code, we access a matrix's transpose via the `T` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  4.,  8., 12., 16.],\n",
       "       [ 1.,  5.,  9., 13., 17.],\n",
       "       [ 2.,  6., 10., 14., 18.],\n",
       "       [ 3.,  7., 11., 15., 19.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a special type of the square matrix, a symmetric matrix $\\mathbf{A}$ is equal to its transpose: $\\mathbf{A} = \\mathbf{A}^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 2., 3.],\n",
       "        [2., 0., 4.],\n",
       "        [3., 4., 5.]]),\n",
       " array([[1., 2., 3.],\n",
       "        [2., 0., 4.],\n",
       "        [3., 4., 5.]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B, B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are useful data structures: they allow us to organize data that have different modalities of variation. For example, rows in our matrix might correspond to different houses (data points), while columns might correspond to different attributes. Thus, although the default orientation of a single vector is a column vector, in a matrix that represents a tabular dataset, it is more conventional to treat each data point as a row vector in the matrix. And, as we will see in later chapters, this convention will enable common deep learning practices. For example, along the outermost axis of an `ndarray`, we can access or enumerate minibatches of data points, or just data points if no minibatch exists.\n",
    "\n",
    "### 2.3.4 Tensors\n",
    "Just as vectors generalize scalars, and matrices generalize vectors, we can build data structures with even more axes. Tensors give us a generic way of describing `ndarray` with an arbitrary number of axes. Vectors, for example, are first-order tensors, and matrices are second-order tensors. Tensors are denoted with capital letters of a special font face (e.g., $\\mathsf{X}$, $\\mathsf{Y}$, and $\\mathsf{Z}$) and their indexing mechanism (e.g., $x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$) is similar to that of matrices.\n",
    "\n",
    "Tensors will become more important when we start working with images, which arrive as ndarrays with 3 axes corresponding to the height, width, and a channel axis for stacking the color channels (red, green, and blue). For now, we will skip over higher order tensors and focus on the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]],\n",
       "\n",
       "       [[12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.],\n",
       "        [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Basic Properties of Tensor Arithmetic\n",
    "Scalars, vectors, matrices, and tensors of an arbitrary number of axes have some nice properties that often come in handy. For example, you might have noticed from the definition of an elementwise operation that any elementwise unary operation does not change the shape of its operand. Similarly, given any two tensors with the same shape, the result of any binary elementwise operation will be a tensor of that same shape. For example, adding two matrices of the same shape performs elementwise addition over these two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]]),\n",
       " array([[ 0.,  2.,  4.,  6.],\n",
       "        [ 8., 10., 12., 14.],\n",
       "        [16., 18., 20., 22.],\n",
       "        [24., 26., 28., 30.],\n",
       "        [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.arange(20).reshape(5, 4)\n",
    "B = A.copy()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, elementwise multiplication of two matrices is called their `Hadamard product` (math notation $\\odot$). Consider matrix $\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ whose element of row $i$ and column $j$ is $b_{ij}$. The Hadamard product of matrices $\\mathbf{A}$ and $\\mathbf{B}$:\n",
    "$$ \\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} a_{11} b_{11} & a_{12} b_{12} & \\dots & a_{1n} b_{1n} \\\\ a_{21} b_{21} & a_{22} b_{22} & \\dots & a_{2n} b_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} b_{m1} & a_{m2} b_{m2} & \\dots & a_{mn} b_{mn} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   1.,   4.,   9.],\n",
       "       [ 16.,  25.,  36.,  49.],\n",
       "       [ 64.,  81., 100., 121.],\n",
       "       [144., 169., 196., 225.],\n",
       "       [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying or adding a tensor by a scalar also does not change the shape of the tensor, where each element of the operand tensor will be added or multiplied by the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13.]],\n",
       " \n",
       "        [[14., 15., 16., 17.],\n",
       "         [18., 19., 20., 21.],\n",
       "         [22., 23., 24., 25.]]]),\n",
       " (2, 3, 4))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = np.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 Reduction\n",
    "One useful operation that we can perform with arbitrary tensors is to calculate the sum of their elements. In mathematical notation, we express sums using the $\\sum$ symbol. To express the sum of the elements in a vector $\\mathbf{x}$ of length $d$, we write $\\displaystyle\\sum_{i=1}^d x_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3.]), array(6.))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express sums over the elements of tensors of arbitrary shape. For example, the sum of the elements of an $m \\times n$ matrix $\\mathbf{A}$ could be written $\\displaystyle\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 4), array(190.))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, invoking the `sum` function reduces a tensor along all its axes to a scalar. We can also specify the axes along which the tensor is reduced via summation. Take matrices as an example. To reduce the row dimension (axis $0$) by summing up elements of all the rows, we specify $axis=0$ when invoking `sum`. Since the input matrix reduces along axis $0$ to generate the output vector, the dimension of axis $0$ of the input is lost in the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]]),\n",
       " array([40., 45., 50., 55.]),\n",
       " (4,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A, A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing a matrix along both rows and columns via summation is equivalent to summing up all the elements of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(190.)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1])  # Same as A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related quantity is the `mean`, which is also called the `average`. We calculate the mean by dividing the sum by the total number of elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(9.5), array(9.5))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `sum`, `mean` can also reduce a tensor along the specified axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8.,  9., 10., 11.]), array([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-Reduction Sum\n",
    "However, sometimes it can be useful to keep the number of axes unchanged when invoking `sum` or `mean` by setting $keepdims=True$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.],\n",
       "       [22.],\n",
       "       [38.],\n",
       "       [54.],\n",
       "       [70.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, since `sum_A` still keeps its $2$ axes after summing each row, we can divide `A` by `sum_A` with broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.16666667, 0.33333334, 0.5       ],\n",
       "       [0.18181819, 0.22727273, 0.27272728, 0.3181818 ],\n",
       "       [0.21052632, 0.23684211, 0.2631579 , 0.28947368],\n",
       "       [0.22222222, 0.24074075, 0.25925925, 0.2777778 ],\n",
       "       [0.22857143, 0.24285714, 0.25714287, 0.27142859]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to calculate the cumulative `sum` of elements of `A` along some axis, say `axis=0` (row by row), we can call the `cumsum` function. This function will not reduce the input tensor along any axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  6.,  8., 10.],\n",
       "       [12., 15., 18., 21.],\n",
       "       [24., 28., 32., 36.],\n",
       "       [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7 Dot Products\n",
    "Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their dot product $\\mathbf{x}^\\top \\mathbf{y}$ (or $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$) is a sum over the products of the elements at the same position: $\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3.]), array([1., 1., 1., 1.]), array(6.))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "y = np.ones(4)\n",
    "x, y, np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can express the dot product of two vectors equivalently by performing an elementwise multiplication and then a sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(6.)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector $\\mathbf{x} \\in \\mathbb{R}^d$ and a set of weights denoted by $\\mathbf{w} \\in \\mathbb{R}^d$, the weighted sum of the values in $\\mathbf{x}$ according to the weights $\\mathbf{w}$ could be expressed as the dot product $\\mathbf{x}^\\top \\mathbf{w}$. When the weights are non-negative and sum to one (i.e., $\\left(\\sum_{i=1}^{d} {w_i} = 1\\right)$), the dot product expresses a weighted average. After normalizing two vectors to have the unit length, the dot products express the cosine of the angle between them. We will formally introduce this notion of length later in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8 Matrix-Vector Products\n",
    "Now that we know how to calculate dot products, we can begin to understand matrix-vector products. Recall the matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and the vector $\\mathbf{x} \\in \\mathbb{R}^n$ defined and visualized in `(2.3.2)` and `(2.3.1)` respectively. Let us start off by visualizing the matrix $\\mathbf{A}$ in terms of its row vectors\n",
    "\n",
    "$$\\mathbf{A}= \\begin{bmatrix} \\mathbf{a}^\\top_{1} \\\\ \\mathbf{a}^\\top_{2} \\\\ \\vdots \\\\ \\mathbf{a}^\\top_m \\ \\ \\end{bmatrix}$$\n",
    "\n",
    "where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ is a row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$. The matrix-vector product $\\mathbf{A}\\mathbf{x}$ is simply a column vector of length $m$, whose $i^\\mathrm{th}$ element is the dot product $\\mathbf{a}^\\top_i \\mathbf{x}$:\n",
    "\n",
    "$$ \\mathbf{A}\\mathbf{x} = \\begin{bmatrix} \\mathbf{a}^\\top_{1} \\\\ \\mathbf{a}^\\top_{2} \\\\ \\vdots \\\\ \\mathbf{a}^\\top_m \\\\ \\end{bmatrix}\\mathbf{x} = \\begin{bmatrix} \\mathbf{a}^\\top_{1} \\mathbf{x} \\\\ \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\ \\vdots\\\\ \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\ \\end{bmatrix}$$\n",
    "\n",
    "We can think of multiplication by a matrix $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$ as a transformation that projects vectors from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$. These transformations turn out to be remarkably useful. For example, we can represent rotations as multiplications by a square matrix. As we will see in subsequent chapters, we can also use matrix-vector products to describe the most intensive calculations required when computing each layer in a neural network given the values of the previous layer.\n",
    "\n",
    "Expressing matrix-vector products in code with ndarrays, we use the same dot function as for dot products. When we call `np.dot(A, x)` with a matrix `A` and a vector `x`, the matrix-vector product is performed. Note that the column dimension of `A` (its length along axis $1$) must be the same as the dimension of `x` (its length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]]),\n",
       " array([0., 1., 2., 3.]),\n",
       " (5, 4),\n",
       " (4,),\n",
       " array([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, x, A.shape, x.shape, np.dot(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.9 Matrix-Matrix Multiplication\n",
    "If you have gotten the hang of dot products and matrix-vector products, then matrix-matrix multiplication should be straightforward.\n",
    "\n",
    "Say that we have two matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1k} \\\\ a_{21} & a_{22} & \\cdots & a_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\ \\end{bmatrix},\\quad \\mathbf{B}=\\begin{bmatrix} b_{11} & b_{12} & \\cdots & b_{1m} \\\\ b_{21} & b_{22} & \\cdots & b_{2m} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{k1} & b_{k2} & \\cdots & b_{km} \\\\ \\end{bmatrix}$$\n",
    "\n",
    "Denote by $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$ the row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$, and let $\\mathbf{b}_{j} \\in \\mathbb{R}^k$ be the column vector from the $j^\\mathrm{th}$ column of the matrix $\\mathbf{B}$. To produce the matrix product $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$, it is easiest to think of $\\mathbf{A}$ in terms of its row vectors and $\\mathbf{B}$ in terms of its column vectors:\n",
    "\n",
    "$$\\mathbf{A}= \\begin{bmatrix} \\mathbf{a}^\\top_{1} \\\\ \\mathbf{a}^\\top_{2} \\\\ \\vdots \\\\ \\mathbf{a}^\\top_n \\\\ \\end{bmatrix}, \\quad \\mathbf{B}=\\begin{bmatrix} \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\ \\end{bmatrix}$$\n",
    "\n",
    "Then the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$ is produced as we simply compute each element $c_{ij}$ as the dot product $\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix} \\mathbf{a}^\\top_{1} \\\\ \\mathbf{a}^\\top_{2} \\\\ \\vdots \\\\ \\mathbf{a}^\\top_n \\\\ \\end{bmatrix} \\begin{bmatrix} \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\ \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\ \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\ \\vdots & \\vdots & \\ddots &\\vdots\\\\ \\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m \\end{bmatrix}$$\n",
    "\n",
    "We can think of the matrix-matrix multiplication $\\mathbf{AB}$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix. Just as with ordinary dot products and matrix-vector products, we can compute matrix-matrix multiplication by using the dot function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  6.,  6.],\n",
       "       [22., 22., 22.],\n",
       "       [38., 38., 38.],\n",
       "       [54., 54., 54.],\n",
       "       [70., 70., 70.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.ones(shape=(4, 3))\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-matrix multiplication can be simply called matrix multiplication, and should not be confused with the Hadamard product.\n",
    "\n",
    "### 2.3.10 Norms\n",
    "Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big a vector is. The notion of size under consideration here concerns not dimensionality but rather the magnitude of the components.\n",
    "\n",
    "In linear algebra, a vector norm is a function $f$ that maps a vector to a scalar, satisfying a handful of properties. Given any vector $\\mathbf{x}$, the first property says that if we scale all the elements of a vector by a constant factor $\\alpha$, its norm also scales by the absolute value of the same constant factor:\n",
    "\n",
    "$$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$$\n",
    "\n",
    "The second property is the familiar triangle inequality:\n",
    "\n",
    "$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$$\n",
    "\n",
    "The third property simply says that the norm must be non-negative:\n",
    "\n",
    "$$f(\\mathbf{x}) \\geq 0.$$\n",
    "\n",
    "That makes sense, as in most contexts the smallest size for anything is 0. The final property requires that the smallest norm is achieved and only achieved by a vector consisting of all zeros.\n",
    "\n",
    "$$\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$$\n",
    "\n",
    "You might notice that norms sound a lot like measures of distance. In fact, the Euclidean distance is a norm: specifically it is the $\\ell_2$ norm. Suppose that the elements in the $n$-dimensional vector $\\mathbf{x}$ are $x_1, \\ldots, x_n$. The $\\ell_2$ norm of $\\mathbf{x}$ is the square root of the sum of the squares of the vector elements:\n",
    "\n",
    "$$\\Vert\\mathbf{x}\\Vert_2 = \\displaystyle\\sqrt{\\sum_{i=1}^n x_i^2},$$\n",
    "\n",
    "where the subscript $2$ is often omitted in $\\ell_2$ norms, i.e., $\\Vert\\mathbf{x}\\Vert$ is equivalent to $\\Vert\\mathbf{x}\\Vert_2$. In code, we can calculate the $\\ell_2$ norm of a vector by calling `linalg.norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.array([3, -4]) \n",
    "np.linalg.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, we work more often with the squared $\\ell_2$ norm. You will also frequently encounter the $\\ell_1$ norm, which is expressed as the sum of the absolute values of the vector elements:\n",
    "\n",
    "$$\\Vert\\mathbf{x}\\Vert_1 = \\displaystyle\\sum_{i=1}^n \\vert x_i \\vert.$$\n",
    "\n",
    "As compared with the $\\ell_2$ norm, it is less influenced by outliers. To calculate the $\\ell_1$ norm, we compose the absolute value function with a sum over the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(7.)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the $\\ell_2$ norm and the $\\ell_1$ norm are special cases of the more general $\\ell_p$ norm:\n",
    "\n",
    "$$\\Vert\\mathbf{x}\\Vert_p = \\displaystyle\\bigg(\\sum_{i=1}^n \\vert x_i \\vert ^p \\bigg)^{\\frac{1}{p}}.$$\n",
    "\n",
    "Analogous to $\\ell_2$ norms of vectors, the `Frobenius norm` of a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ is the square root of the sum of the squares of the matrix elements:\n",
    "\n",
    "$$\\Vert\\mathbf{X}\\Vert_F = \\displaystyle\\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$\n",
    "\n",
    "The Frobenius norm satisfies all the properties of vector norms. It behaves as if it were an $\\ell_2$ norm of a matrix-shaped vector. Invoking `linalg.norm` will calculate the Frobenius norm of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(6.)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Norms and Objectives\n",
    "While we do not want to get too far ahead of ourselves, we can plant some intuition already about why these concepts are useful. In deep learning, we are often trying to solve optimization problems: maximize the probability assigned to observed data; minimize the distance between predictions and the ground-truth observations. Assign vector representations to items (like words, products, or news articles) such that the distance between similar items is minimized, and the distance between dissimilar items is maximized. Oftentimes, the objectives, perhaps the most important components of deep learning algorithms (besides the data), are expressed as norms.\n",
    "\n",
    "### 2.3.11 More on Linear Algebra\n",
    "In just this section, we have taught you all the linear algebra that you will need to understand a remarkable chunk of modern deep learning. There is a lot more to linear algebra and a lot of that mathematics is useful for machine learning. For example, matrices can be decomposed into factors, and these decompositions can reveal low-dimensional structure in real-world datasets. There are entire subfields of machine learning that focus on using matrix decompositions and their generalizations to high-order tensors to discover structure in datasets and solve prediction problems. But this book focuses on deep learning. And we believe you will be much more inclined to learn more mathematics once you have gotten your hands dirty deploying useful machine learning models on real datasets. So while we reserve the right to introduce more mathematics much later on, we will wrap up this section here.\n",
    "\n",
    "##### Summary\n",
    "+ Scalars, vectors, matrices, and tensors are basic mathematical objects in linear algebra.\n",
    "+ Vectors generalize scalars, and matrices generalize vectors.\n",
    "+ In the ndarray representation, scalars, vectors, matrices, and tensors have 0, 1, 2, and an arbitrary number of axes, respectively.\n",
    "+ A tensor can be reduced along the specified axes by sum and mean.\n",
    "+ Elementwise multiplication of two matrices is called their Hadamard product. It is different from matrix multiplication.\n",
    "+ In deep learning, we often work with norms such as the $\\ell_1$ norm, the $\\ell_2$ norm, and the Frobenius norm.\n",
    "+ We can perform a variety of operations over scalars, vectors, matrices, and tensors with ndarray functions.\n",
    "\n",
    "##### Exercises\n",
    "+ Prove that the transpose of a matrix $\\mathbf{A}$'s transpose is $\\mathbf{A}$: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$.\n",
    "+ Given two matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that the sum of transposes is equal to the transpose of a sum: $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$.\n",
    "+ Given any square matrix $\\mathbf{A}$, is $\\mathbf{A} + \\mathbf{A}^\\top$ always symmetric? Why?\n",
    "+ We defined the tensor X of shape ($2$, $3$, $4$) in this section. What is the output of len(X)?\n",
    "+ For a tensor X of arbitrary shape, does len(X) always correspond to the length of a certain axis of X? What is that axis?\n",
    "+ Run A / A.sum(axis=1) and see what happens. Can you analyze the reason?\n",
    "+ When traveling between two points in Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?\n",
    "+ Consider a tensor with shape ($2$, $3$, $4$). What are the shapes of the summation outputs along axis $0$, $1$, and $2$?\n",
    "+ Feed a tensor with 3 or more axes to the linalg.norm function and observe its output. What does this function compute for ndarrays of arbitrary shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Calculus\n",
    "Finding the area of a polygon had remained mysterious until at least $2,500$ years ago, when ancient Greeks divided a polygon into triangles and summed their areas. To find the area of curved shapes, such as a circle, ancient Greeks inscribed polygons in such shapes. As shown in `Fig 2.4.1`, an inscribed polygon with more sides of equal length better approximates the circle. This process is also known as the method of exhaustion.\n",
    "\n",
    "<img src=\"images/02_01.png\" style=\"width:500px;\"/>\n",
    "\n",
    "In fact, the method of exhaustion is where integral calculus originates from. More than $2,000$ years later, the other branch of calculus, differential calculus, was invented. Among the most critical applications of differential calculus, optimization problems consider how to do something the best. As discussed in `Section 2.3.10`, such problems are ubiquitous in deep learning.\n",
    "\n",
    "In deep learning, we train models, updating them successively so that they get better and better as they see more and more data. Usually, getting better means minimizing a loss function, a score that answers the question \"how bad is our model?\" This question is more subtle than it appears. Ultimately, what we really care about is producing a model that performs well on data that we have never seen before. But we can only fit the model to data that we can actually see. Thus we can decompose the task of fitting models into two key concerns:\n",
    "1. optimization: the process of fitting our models to observed data\n",
    "2. generalization: the mathematical principles and practitioners' wisdom that guide as to how to produce models whose validity extends beyond the exact set of data points used to train them\n",
    "\n",
    "To help you understand optimization problems and methods in later chapters, here we give a very brief primer on differential calculus that is commonly used in deep learning.\n",
    "\n",
    "### 2.4.1 Derivatives and Differentiation\n",
    "We begin by addressing the calculation of derivatives, a crucial step in nearly all deep learning optimization algorithms. In deep learning, we typically choose loss functions that are differentiable with respect to our model's parameters. Put simply, this means that for each parameter, we can determine how rapidly the loss would increase or decrease, were we to increase or decrease that parameter by an infinitesimally small amount.\n",
    "\n",
    "Suppose that we have a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, whose input and output are both scalars. The derivative of $f$ is defined as\n",
    "\n",
    "$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h},$$ \n",
    "\n",
    "if this limit exists. If $f'(a)$ exists, $f$ is said to be differentiable at $a$. If $f$ is differentiable at every number of an interval, then this function is differentiable on this interval. We can interpret the derivative $f'(x)$ as the instantaneous rate of change of $f(x)$ with respect to $x$. The so-called instantaneous rate of change is based on the variation $h$ in $x$, which approaches $0$.\n",
    "\n",
    "To illustrate derivatives, let us experiment with an example. Define $u = f(x) = 3x^2-4x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting $x=1$ and letting $h$ approach $0$, the numerical result of $\\frac{f(x+h) - f(x)}{h}$ approaches $2$. Though this experiment is not a mathematical proof, we will see later that the derivative $u'$ is $2$ when $x=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "def numerical_lim(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "h = 0.1 \n",
    "for i in range(5):\n",
    "    print('h=%.5f, numerical limit=%.5f' % (h, numerical_lim(f, 1, h))) \n",
    "    h *= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us familiarize ourselves with a few equivalent notations for derivatives. Given $y = f(x)$, where $x$ and $y$ are the independent variable and the dependent variable of the function $f$, respectively. The following expressions are equivalent:\n",
    "\n",
    "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
    "\n",
    "where symbols $\\frac{d}{dx}$ and $D$ are differentiation operators that indicate operation of differentiation. \n",
    "\n",
    "We can use the following rules to differentiate common functions:\n",
    "+ $DC = 0$ ($C$ is a constant)\n",
    "+ $Dx^n = nx^{n-1}$ (the power rule, $n$ is any real number)\n",
    "+ $De^x = e^x$\n",
    "+ $D\\ln(x) = 1/x$\n",
    "\n",
    "To differentiate a function that is formed from a few simpler functions such as the above common functions, the following rules can be handy for us. Suppose that functions $f$ and $g$ are both differentiable and $C$ is a constant, we have the constant multiple rule\n",
    "\n",
    "$$\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x),$$\n",
    "\n",
    "the sum rule\n",
    "\n",
    "$$\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x),$$\n",
    "\n",
    "the product rule\n",
    "\n",
    "$$\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} [g(x)] + g(x) \\frac{d}{dx} [f(x)],$$\n",
    "\n",
    "and the quotient rule\n",
    "\n",
    "$$\\frac{d}{dx} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x) \\frac{d}{dx} [f(x)] - f(x) \\frac{d}{dx} [g(x)]}{[g(x)]^2}.$$\n",
    "\n",
    "Now we can apply a few of the above rules to find $u' = f'(x) = 3 \\frac{d}{dx} x^2-4\\frac{d}{dx}x = 6x-4$. Thus, by setting $x = 1$, we have $u' = 2$. This derivative is also the slope of the tangent line to the curve $u = f(x)$ when $x = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Partial Derivatives\n",
    "So far we have dealt with the differentiation of functions of just one variable. In deep learning, functions often depend on many variables. Thus, we need to extend the ideas of differentiation to these multivariate functions.\n",
    "\n",
    "Let $y = f(x_1, x_2, \\ldots, x_n)$ be a function with $n$ variables. The partial derivative of $y$ with respect to its $i^\\mathrm{th}$ parameter $x_i$ is\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
    "\n",
    "To calculate $\\frac{\\partial y}{\\partial x_i}$, we can simply treat $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ as constants and calculate the derivative of $y$ with respect to $x_i$. For notation of partial derivatives, the following are equivalent:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$\n",
    "\n",
    "### 2.4.3 Gradients\n",
    "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain the gradient vector of the function. Suppose that the input of function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is an $n$-dimensional vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ and the output is a scalar. The gradient of the function $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ is a vector of $n$ partial derivatives:\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n",
    "\n",
    "where $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ is often replaced by $\\nabla f(\\mathbf{x})$ when there is no ambiguity.\n",
    "\n",
    "Let $\\mathbf{x}$ be an $n$-dimensional vector, the following rules are often used when differentiating multivariate functions:\n",
    "+ For all $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$\n",
    "+ For all $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$, $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} = \\mathbf{A}$\n",
    "+ For all $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$\n",
    "+ $\\nabla_{\\mathbf{x}} |\\mathbf{x} |^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$\n",
    "\n",
    "Similarly, for any matrix $\\mathbf{X}$, we have $\\nabla_{\\mathbf{X}} \\Vert\\mathbf{X} \\Vert_F^2 = 2\\mathbf{X}$. As we will see later, gradients are useful for designing optimization algorithms in deep learning.\n",
    "\n",
    "### 2.4.4 Chain Rule\n",
    "However, such gradients can be hard to find. This is because multivariate functions in deep learning are often composite, so we may not apply any of the aforementioned rules to differentiate these functions. Fortunately, the chain rule enables us to differentiate composite functions.\n",
    "\n",
    "Let us first consider functions of a single variable. Suppose that functions $y=f(u)$ and $u=g(x)$ are both differentiable, then the chain rule states that\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n",
    "\n",
    "Now let us turn our attention to a more general scenario where functions have an arbitrary number of variables. Suppose that the differentiable function $y$ has variables $u_1, u_2, \\ldots, u_m$, where each differentiable function $u_i$ has variables $x_1, x_2, \\ldots, x_n$. Note that $y$ is a function of $x_1, x_2, \\ldots, x_n$. Then the chain rule gives\n",
    "\n",
    "$$\\frac{dy}{dx_i} = \\frac{dy}{du_1} \\frac{du_1}{dx_i} + \\frac{dy}{du_2} \\frac{du_2}{dx_i} + \\cdots + \\frac{dy}{du_m} \\frac{du_m}{dx_i}$$\n",
    "\n",
    "for any $i = 1, 2, \\ldots, n$.\n",
    "\n",
    "##### Summary\n",
    "+ Differential calculus and integral calculus are two branches of calculus, where the former can be applied to the ubiquitous optimization problems in deep learning.\n",
    "+ A derivative can be interpreted as the instantaneous rate of change of a function with respect to its variable. It is also the slope of the tangent line to the curve of the function.\n",
    "+ A gradient is a vector whose components are the partial derivatives of a multivariate function with respect to all its variables.\n",
    "+ The chain rule enables us to differentiate composite functions.\n",
    "\n",
    "##### Exercises\n",
    "+ Plot the function $y = f(x) = x^3 - \\frac{1}{x}$ and its tangent line when $x = 1$.\n",
    "+ Find the gradient of the function $f(\\mathbf{x}) = 3x_1^2 + 5e^{x_2}$.\n",
    "+ What is the gradient of the function $f(\\mathbf{x}) = |\\mathbf{x}|_2$?\n",
    "+ Can you write out the chain rule for the case where $u = f(x, y, z)$ and $x = x(a, b)$, $y = y(a, b)$, and $z = z(a, b)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.5 Automatic Differentiation\n",
    "As we have explained in `Section 2.4`, differentiation is a crucial step in nearly all deep learning optimization algorithms. While the calculations for taking these derivatives are straightforward, requiring only some basic calculus, for complex models, working out the updates by hand can be a pain (and often error-prone).\n",
    "\n",
    "The autograd package expedites this work by automatically calculating derivatives, i.e., automatic differentiation. And while many other libraries require that we compile a symbolic graph to take automatic derivatives, autograd allows us to take derivatives while writing ordinary imperative code. Every time we pass data through our model, autograd builds a graph on the fly, tracking which data combined through which operations to produce the output. This graph enables autograd to subsequently backpropagate gradients on command. Here, backpropagate simply means to trace through the computational graph, filling in the partial derivatives with respect to each parameter.\n",
    "\n",
    "### 2.5.1 A Simple Example\n",
    "As a toy example, say that we are interested in differentiating the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to the column vector $\\mathbf{x}$. To start, let us create the variable x and assign it an initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before we even calculate the gradient of $y$ with respect to $\\mathbf{x}$, we will need a place to store it. It is important that we do not allocate new memory every time we take a derivative with respect to a parameter because we will often update the same parameters thousands or millions of times and could quickly run out of memory.\n",
    "\n",
    "Note also that a gradient of a scalar-valued function with respect to a vector $\\mathbf{x}$ is itself vector-valued and has the same shape as $\\mathbf{x}$. Thus it is intuitive that in code, we will access a gradient taken with respect to $x$ as an attribute of the `ndarray` $x$ itself. We allocate memory for an `ndarray`'s gradient by invoking its `attach_grad` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we calculate a gradient taken with respect to $x$, we will be able to access it via the `grad` attribute. As a safe default, `x.grad` is initialized as an array containing all zeros. That is sensible because our most common use case for taking gradient in deep learning is to subsequently update parameters by adding (or subtracting) the gradient to maximize (or minimize) the differentiated function. By initializing the gradient to an array of zeros, we ensure that any update accidentally executed before a gradient has actually been calculated will not alter the parameters' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us calculate $y$. Because we wish to subsequently calculate gradients, we want MXNet to generate a computational graph on the fly. We could imagine that MXNet would be turning on a recording device to capture the exact path by which each variable is generated.\n",
    "\n",
    "Note that building the computational graph requires a nontrivial amount of computation. So MXNet will only build the graph when explicitly told to do so. We can invoke this behavior by placing our code inside an `autograd.record` scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(28.)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * np.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can automatically calculate the gradient of $y$ with respect to each component of $x$ by calling $y$'s backward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we recheck the value of `x.grad`, we will find its contents overwritten by the newly calculated gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$. Let us quickly verify that our desired gradient was calculated correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we subsequently compute the gradient of another variable whose value was calculated as a function of $x$, the contents of `x.grad` will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Backward for Non-Scalar Variables\n",
    "Technically, when $y$ is not a scalar, the most natural interpretation of the differentiation of a vector $y$ with respect to a vector $x$ is a matrix. For higher-order and higher-dimensional $y$ and $x$, the differentiation result could be a gnarly high-order tensor.\n",
    "\n",
    "However, while these more exotic objects do show up in advanced machine learning (including in deep learning), more often when we are calling backward on a vector, we are trying to calculate the derivatives of the loss functions for each constituent of a batch of training examples. Here, our intent is not to calculate the differentiation matrix but rather the sum of the partial derivatives computed individually for each example in the batch.\n",
    "\n",
    "Thus when we invoke backward on a vector-valued variable $y$, which is a function of $x$, MXNet assumes that we want the sum of the gradients. In short, MXNet will create a new scalar variable by summing the elements in $y$, and compute the gradient of that scalar variable with respect to $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * x  # y is a vector\n",
    "y.backward()\n",
    "\n",
    "u = x.copy()\n",
    "u.attach_grad()\n",
    "with autograd.record():\n",
    "    v = (u * u).sum()  # v is a scalar\n",
    "v.backward()\n",
    "\n",
    "x.grad == u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 Detaching Computation\n",
    "Sometimes, we wish to move some calculations outside of the recorded computational graph. For example, say that $y$ was calculated as a function of $x$, and that subsequently $z$ was calculated as a function of both $y$ and $x$. Now, imagine that we wanted to calculate the gradient of $z$ with respect to $x$, but wanted for some reason to treat $y$ as a constant, and only take into account the role that $x$ played after $y$ was calculated.\n",
    "\n",
    "Here, we can call `u = y.detach()` to return a new variable $u$ that has the same value as $y$ but discards any information about how $y$ was computed in the computational graph. In other words, the gradient will not flow backwards through $u$ to $x$. This will provide the same functionality as if we had calculated $u$ as a function of $x$ outside of the `autograd.record` scope, yielding a $u$ that will be treated as a constant in any backward call. Thus, the following backward function computes the partial derivative of $z = u * x$ with respect to $x$ while treating $u$ as a constant, instead of the partial derivative of $z = x * x * x$ with respect to $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * x\n",
    "    u = y.detach()\n",
    "    z = u * x\n",
    "z.backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the computation of $y$ was recorded, we can subsequently call `y.backward()` to get the derivative of $y = x * x$ with respect to $x$, which is $2 * x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that attaching gradients to a variable $x$ implicitly calls `x = x.detach()`. If $x$ is computed based on other variables, this part of computation will not be used in the backward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3.]),\n",
       " array([2., 2., 2., 2.]),\n",
       " array([-1., -1., -1., -1.]),\n",
       " array([5., 5., 5., 5.]),\n",
       " array([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.ones(4) * 2\n",
    "y.attach_grad()\n",
    "with autograd.record():\n",
    "    u = x * y\n",
    "    u.attach_grad()  # Implicitly run u = u.detach()\n",
    "    z = 5 * u - x\n",
    "z.backward()\n",
    "x, y, x.grad, u.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4 Computing the Gradient of Python Control Flow\n",
    "One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable. In the following snippet, note that the number of iterations of the while loop and the evaluation of the if statement both depend on the value of the input $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while np.linalg.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again to compute gradients, we just need to record the calculation and then call the backward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal()\n",
    "a.attach_grad()\n",
    "with autograd.record():\n",
    "    d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now analyze the $f$ function defined above. Note that it is piecewise linear in its input $a$. In other words, for any $a$ there exists some constant scalar $k$ such that $f(a) = k * a$, where the value of $k$ depends on the input $a$. Consequently $\\frac{d}{a}$ allows us to verify that the gradient is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.5 Training Mode and Prediction Mode\n",
    "As we have seen, after we call `autograd.record`, MXNet logs the operations in the following block. There is one more subtle detail to be aware of. Additionally, `autograd.record` will change the running mode from prediction mode to training mode. We can verify this behavior by calling the `is_training` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(autograd.is_training())\n",
    "with autograd.record():\n",
    "    print(autograd.is_training())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get to complicated deep learning models, we will encounter some algorithms where the model behaves differently during training and when we subsequently use it to make predictions. We will cover these differences in detail in later chapters.\n",
    "\n",
    "##### Summary\n",
    "+ MXNet provides the `autograd` package to automate the calculation of derivatives. To use it, we first attach gradients to those variables with respect to which we desire partial derivatives. We then record the computation of our target value, execute its `backward` function, and access the resulting gradient via our variable's `grad` attribute.\n",
    "+ We can detach gradients to control the part of the computation that will be used in the `backward` function.\n",
    "+ The running modes of MXNet include training mode and prediction mode. We can determine the running mode by calling the `is_training` function.\n",
    "\n",
    "##### Exercises\n",
    "+ Why is the second derivative much more expensive to compute than the first derivative?\n",
    "+ After running `y.backward()`, immediately run it again and see what happens.\n",
    "+ In the control flow example where we calculate the derivative of $d$ with respect to $a$, what would happen if we changed the variable $a$ to a random vector or matrix. At this point, the result of the calculation $f(a)$ is no longer a scalar. What happens to the result? How do we analyze this?\n",
    "+ Redesign an example of finding the gradient of the control flow. Run and analyze the result.\n",
    "+ Let $f(x) = \\sin(x)$. Plot $f(x)$ and $\\frac{df(x)}{dx}$, where the latter is computed without exploiting that $f'(x) = \\cos(x)$.\n",
    "+ In a second-price auction (such as in eBay or in computational advertising), the winning bidder pays the second-highest price. Compute the gradient of the final price with respect to the winning bidder's bid using autograd. What does the result tell you about the mechanism? If you are curious to learn more about second-price auctions, check out the paper by Edelman et al((Edelman et al., 2007).)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Probability\n",
    "In some form or another, machine learning is all about making predictions. We might want to predict the probability of a patient suffering a heart attack in the next year, given their clinical history. In anomaly detection, we might want to assess how likely a set of readings from an airplane's jet engine would be, were it operating normally. In reinforcement learning, we want an agent to act intelligently in an environment. This means we need to think about the probability of getting a high reward under each of the available actions. And when we build recommender systems we also need to think about probability. For example, say hypothetically that we worked for a large online bookseller. We might want to estimate the probability that a particular user would buy a particular book. For this we need to use the language of probability. Entire courses, majors, theses, careers, and even departments, are devoted to probability. So naturally, our goal in this section is not to teach the whole subject. Instead we hope to get you off the ground, to teach you just enough that you can start building your first deep learning models, and to give you enough of a flavor for the subject that you can begin to explore it on your own if you wish.\n",
    "\n",
    "We have already invoked probabilities in previous sections without articulating what precisely they are or giving a concrete example. Let us get more serious now by considering the first case: distinguishing cats and dogs based on photographs. This might sound simple but it is actually a formidable challenge. To start with, the difficulty of the problem may depend on the resolution of the image.\n",
    "\n",
    "<img src=\"images/02_02.png\" style=\"width:600px;\"/>\n",
    "\n",
    "As shown in `Fig 2.6.1`, while it is easy for humans to recognize cats and dogs at the resolution of $160 \\times 160$ pixels, it becomes challenging at $40 \\times 40$ pixels and next to impossible at $10 \\times 10$ pixels. In other words, our ability to tell cats and dogs apart at a large distance (and thus low resolution) might approach uninformed guessing. Probability gives us a formal way of reasoning about our level of certainty:\n",
    "+ If we are completely sure that the image depicts a cat, we say that the probability that the corresponding label $y$ is \"cat\", denoted $P(y=$ \"cat\"$)$ equals $1$\n",
    "+ If we had no evidence to suggest that $y =$ \"cat\" or that $y =$ \"dog\", then we might say that the two possibilities were equally likely expressing this as $P(y=$ \"cat\"$) = P(y=$ \"dog\"$) = 0.5$\n",
    "+ If we were reasonably confident, but not sure that the image depicted a cat, we might assign a probability $0.5 < P(y=$ \"cat\"$) < 1$\n",
    "\n",
    "Now consider the second case: given some weather monitoring data, we want to predict the probability that it will rain in Taipei tomorrow. If it is summertime, the rain might come with probability $0.5$.\n",
    "\n",
    "In both cases, we have some value of interest. And in both cases we are uncertain about the outcome. But there is a key difference between the two cases. In this first case, the image is in fact either a dog or a cat, and we just do not know which. In the second case, the outcome may actually be a random event, if you believe in such things (and most physicists do). So probability is a flexible language for reasoning about our level of certainty, and it can be applied effectively in a broad set of contexts.\n",
    "\n",
    "### 2.6.1 Basic Probability Theory\n",
    "Say that we cast a die and want to know what the chance is of seeing a $1$ rather than another digit. If the die is fair, all the $6$ outcomes ${1, \\ldots, 6}$ are equally likely to occur, and thus we would see a $1$ in one out of six cases. Formally we state that $1$ occurs with probability $\\frac{1}{6}$.\n",
    "\n",
    "For a real die that we receive from a factory, we might not know those proportions and we would need to check whether it is tainted. The only way to investigate the die is by casting it many times and recording the outcomes. For each cast of the die, we will observe a value in ${1, \\ldots, 6}$. Given these outcomes, we want to investigate the probability of observing each outcome.\n",
    "\n",
    "One natural approach for each value is to take the individual count for that value and to divide it by the total number of tosses. This gives us an estimate of the probability of a given event. `The law of large numbers` tell us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability. Before going into the details of what is going here, let us try it out.\n",
    "\n",
    "In statistics we call this process of drawing examples from probability `distributions sampling`. The distribution that assigns probabilities to a number of discrete choices is called the `multinomial distribution`. We will give a more formal definition of distribution later, but at a high level, think of it as just an assignment of probabilities to events. In MXNet, we can sample from the multinomial distribution via the aptly named `np.random.multinomial` function. The function can be called in many ways, but we will focus on the simplest. To draw a single sample, we simply pass in a vector of probabilities. The output of the `np.random.multinomial` function is another vector of the same length: its value at index $i$ is the number of times the sampling outcome corresponds to $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fair_probs = [1.0 / 6] * 6\n",
    "np.random.multinomial(1, fair_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the sampler a bunch of times, you will find that you get out random values each time. As with estimating the fairness of a die, we often want to generate many samples from the same distribution. It would be unbearably slow to do this with a Python for loop, so `random.multinomial` supports drawing multiple samples at once, returning an array of independent samples in any shape we might desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multinomial(10, fair_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also conduct, say, $3$ groups of experiments, where each group draws $10$ samples, all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 2, 1, 1, 2],\n",
       "       [2, 1, 0, 2, 0, 5],\n",
       "       [5, 2, 1, 0, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = np.random.multinomial(10, fair_probs, size=3)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to sample rolls of a die, we can simulate 1000 rolls. We can then go through and count, after each of the 1000 rolls, how many times each number was rolled. Specifically, we calculate the relative frequency as the estimate of the true probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17 , 0.175, 0.177, 0.163, 0.166, 0.149])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the results as 32-bit floats for division\n",
    "counts = np.random.multinomial(1000, fair_probs).astype(np.float32)\n",
    "counts / 1000  # Relative frequency as the estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we generated the data from a fair die, we know that each outcome has true probability $\\frac{1}{6}$, roughly $0.167$, so the above output estimates look good.\n",
    "\n",
    "Let us conduct $500$ groups of experiments where each group draws $10$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = np.random.multinomial(10, fair_probs, size=500)\n",
    "cum_counts = counts.astype(np.float32).cumsum(axis=0)\n",
    "estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.plot(estimates[:, i].asnumpy(), label=(\"P(die=\" + str(i + 1) + \")\"))\n",
    "plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
    "plt.gca().set_xlabel('Groups of experiments')\n",
    "plt.gca().set_ylabel('Estimated probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each solid curve corresponds to one of the six values of the die and gives our estimated probability that the die turns up that value as assessed after each group of experiments. The dashed black line gives the true underlying probability. As we get more data by conducting more experiments, the $6$ solid curves converge towards the true probability.\n",
    "\n",
    "##### Axioms of Probability Theory\n",
    "When dealing with the rolls of a die, we call the set $\\mathcal{S} = {1, 2, 3, 4, 5, 6}$ the sample space or outcome space, where each element is an outcome. An event is a set of outcomes from a given sample space. For instance, \"seeing a $5$\" (${5}$) and \"seeing an odd number\" (${1, 3, 5}$) are both valid events of rolling a die. Note that if the outcome of a random experiment is in event $\\mathcal{A}$, then event $\\mathcal{A}$ has occurred. That is to say, if $3$ dots faced up after rolling a die, since $3 \\in {1, 3, 5}$, we can say that the event \"seeing an odd number\" has occurred.\n",
    "\n",
    "Formally, probability can be thought of a function that maps a set to a real value. The probability of an event $\\mathcal{A}$ in the given sample space $\\mathcal{S}$, denoted as $P(\\mathcal{A})$, satisfies the following properties:\n",
    "\n",
    "For any event $\\mathcal{A}$, its probability is never negative, i.e., $P(\\mathcal{A}) \\geq 0$;\n",
    "Probability of the entire sample space is $1$, i.e., $P(\\mathcal{S}) = 1$;\n",
    "For any countable sequence of events $\\mathcal{A}_1, \\mathcal{A}_2, \\ldots$ that are mutually exclusive ($\\mathcal{A}_i \\cap \\mathcal{A}j = \\emptyset$ for all $i \\neq j$), the probability that any happens is equal to the sum of their individual probabilities, i.e., $P(\\bigcup{i=1}^{\\infty} \\mathcal{A}i) = \\sum{i=1}^{\\infty} P(\\mathcal{A}_i)$.\n",
    "These are also the axioms of probability theory, proposed by Kolmogorov in 1933. Thanks to this axiom system, we can avoid any philosophical dispute on randomness; instead, we can reason rigorously with a mathematical language. For instance, by letting event $\\mathcal{A}_1$ be the entire sample space and $\\mathcal{A}_i = \\emptyset$ for all $i > 1$, we can prove that $P(\\emptyset) = 0$, i.e., the probability of an impossible event is $0$.\n",
    "\n",
    "##### Random Variables\n",
    "In our random experiment of casting a die, we introduced the notion of a random variable. A random variable can be pretty much any quantity and is not deterministic. It could take one value among a set of possibilities in a random experiment. Consider a random variable $X$ whose value is in the sample space $\\mathcal{S} = {1, 2, 3, 4, 5, 6}$ of rolling a die. We can denote the event \"seeing a $5$\" as ${X = 5}$ or $X = 5$, and its probability as $P({X = 5})$ or $P(X = 5)$. By $P(X = a)$, we make a distinction between the random variable $X$ and the values (e.g., $a$) that $X$ can take. However, such pedantry results in a cumbersome notation. For a compact notation, on one hand, we can just denote $P(X)$ as the distribution over the random variable $X$: the distribution tells us the probability that $X$ takes any value. On the other hand, we can simply write $P(a)$ to denote the probability that a random variable takes the value $a$. Since an event in probability theory is a set of outcomes from the sample space, we can specify a range of values for a random variable to take. For example, $P(1 \\leq X \\leq 3)$ denotes the probability of the event ${1 \\leq X \\leq 3}$, which means ${X = 1, 2, \\text{or}, 3}$. Equivalently, $P(1 \\leq X \\leq 3)$ represents the probability that the random variable $X$ can take a value from ${1, 2, 3}$.\n",
    "\n",
    "Note that there is a subtle difference between discrete random variables, like the sides of a die, and continuous ones, like the weight and the height of a person. There is little point in asking whether two people have exactly the same height. If we take precise enough measurements you will find that no two people on the planet have the exact same height. In fact, if we take a fine enough measurement, you will not have the same height when you wake up and when you go to sleep. So there is no purpose in asking about the probability that someone is $1.80139278291028719210196740527486202$ meters tall. Given the world population of humans the probability is virtually $0$. It makes more sense in this case to ask whether someone's height falls into a given interval, say between $1.79$ and $1.81$ meters. In these cases we quantify the likelihood that we see a value as a density. The height of exactly $1.80$ meters has no probability, but nonzero density. In the interval between any two different heights we have nonzero probability. In the rest of this section, we consider probability in discrete space. For probability over continuous random variables, you may refer to :numref:sec_random_variables.\n",
    "\n",
    "### 2.6.2 Dealing with Multiple Random Variables\n",
    "Very often, we will want to consider more than one random variable at a time. For instance, we may want to model the relationship between diseases and symptoms. Given a disease and a symptom, say \"flu\" and \"cough\", either may or may not occur in a patient with some probability. While we hope that the probability of both would be close to zero, we may want to estimate these probabilities and their relationships to each other so that we may apply our inferences to effect better medical care.\n",
    "\n",
    "As a more complicated example, images contain millions of pixels, thus millions of random variables. And in many cases images will come with a label, identifying objects in the image. We can also think of the label as a random variable. We can even think of all the metadata as random variables such as location, time, aperture, focal length, ISO, focus distance, and camera type. All of these are random variables that occur jointly. When we deal with multiple random variables, there are several quantities of interest.\n",
    "\n",
    "##### Joint Probability\n",
    "The first is called the joint probability $P(A = a, B=b)$. Given any values $a$ and $b$, the joint probability lets us answer, what is the probability that $A=a$ and $B=b$ simultaneously? Note that for any values $a$ and $b$, $P(A=a, B=b) \\leq P(A=a)$. This has to be the case, since for $A=a$ and $B=b$ to happen, $A=a$ has to happen and $B=b$ also has to happen (and vice versa). Thus, $A=a$ and $B=b$ cannot be more likely than $A=a$ or $B=b$ individually.\n",
    "\n",
    "##### Conditional Probability\n",
    "This brings us to an interesting ratio: $0 \\leq \\frac{P(A=a, B=b)}{P(A=a)} \\leq 1$. We call this ratio a conditional probability and denote it by $P(B=b \\mid A=a)$: it is the probability of $B=b$, provided that $A=a$ has occurred.\n",
    "\n",
    "##### Bayes theorem\n",
    "Using the definition of conditional probabilities, we can derive one of the most useful and celebrated equations in statistics: Bayes' theorem. It goes as follows. By construction, we have the multiplication rule that $P(A, B) = P(B \\mid A) P(A)$. By symmetry, this also holds for $P(A, B) = P(A \\mid B) P(B)$. Assume that $P(B) > 0$. Solving for one of the conditional variables we get\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}.$$\n",
    "\n",
    "Note that here we use the more compact notation where $P(A, B)$ is a joint distribution and $P(A \\mid B)$ is a conditional distribution. Such distributions can be evaluated for particular values $A = a, B=b$.\n",
    "\n",
    "##### Marginalization\n",
    "Bayes' theorem is very useful if we want to infer one thing from the other, say cause and effect, but we only know the properties in the reverse direction, as we will see later in this section. One important operation that we need, to make this work, is marginalization. It is the operation of determining $P(B)$ from $P(A, B)$. We can see that the probability of $B$ amounts to accounting for all possible choices of $A$ and aggregating the joint probabilities over all of them:\n",
    "\n",
    "$$P(B) = \\sum_{A} P(A, B),$$\n",
    "\n",
    "which is also known as the sum rule. The probability or distribution as a result of marginalization is called a marginal probability or a marginal distribution.\n",
    "\n",
    "##### Independence\n",
    "Another useful property to check for is dependence vs. independence. Two random variables $A$ and $B$ being independent means that the occurrence of one event of $A$ does not reveal any information about the occurrence of an event of $B$. In this case $P(B \\mid A) = P(B)$. Statisticians typically express this as $A \\perp B$. From Bayes' theorem, it follows immediately that also $P(A \\mid B) = P(A)$. In all the other cases we call $A$ and $B$ dependent. For instance, two successive rolls of a die are independent. In contrast, the position of a light switch and the brightness in the room are not (they are not perfectly deterministic, though, since we could always have a broken light bulb, power failure, or a broken switch).\n",
    "\n",
    "Since $P(A \\mid B) = \\frac{P(A, B)}{P(B)} = P(A)$ is equivalent to $P(A, B) = P(A)P(B)$, two random variables are independent if and only if their joint distribution is the product of their individual distributions. Likewise, two random variables $A$ and $B$ are conditionally independent given another random variable $C$ if and only if $P(A, B \\mid C) = P(A \\mid C)P(B \\mid C)$. This is expressed as $A \\perp B \\mid C$.\n",
    "\n",
    "##### Application\n",
    "Let us put our skills to the test. Assume that a doctor administers an AIDS test to a patient. This test is fairly accurate and it fails only with $1\\%$ probability if the patient is healthy but reporting him as diseased. Moreover,\n",
    "it never fails to detect HIV if the patient actually has it. We use $D_1$ to indicate the diagnosis ($1$ if positive and $0$ if negative) and $H$ to denote the HIV status ($1$ if positive and $0$ if negative).\n",
    ":numref:`conditional_prob_D1` lists such conditional probabilities.\n",
    "\n",
    ":Conditional probability of $P(D_1 \\mid H)$.\n",
    "\n",
    "| Conditional probability | $H=1$ | $H=0$ |\n",
    "|---|---|---|\n",
    "|$P(D_1 = 1 \\mid H)$|            1 |         0.01 |\n",
    "|$P(D_1 = 0 \\mid H)$|            0 |         0.99 |\n",
    ":label:`conditional_prob_D1`\n",
    "\n",
    "Note that the column sums are all $1$ (but the row sums are not), since the conditional probability needs to sum up to $1$, just like the probability. Let us work out the probability of the patient having AIDS if the test comes back positive, i.e., $P(H = 1 \\mid D_1 = 1)$. Obviously this is going to depend on how common the disease is, since it affects the number of false alarms. Assume that the population is quite healthy, e.g., $P(H=1) = 0.0015$. To apply Bayes' theorem, we need to apply marginalization and the multiplication rule to determine\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(D_1 = 1) \\\\\n",
    "=& P(D_1=1, H=0) + P(D_1=1, H=1)  \\\\\n",
    "=& P(D_1=1 \\mid H=0) P(H=0) + P(D_1=1 \\mid H=1) P(H=1) \\\\\n",
    "=& 0.011485.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, we get\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(H = 1 \\mid D_1 = 1)\\\\ =& \\frac{P(D_1=1 \\mid H=1) P(H=1)}{P(D_1=1)} \\\\ =& 0.1306 \\end{aligned}.$$\n",
    "\n",
    "In other words, there is only a 13.06% chance that the patient\n",
    "actually has AIDS, despite using a very accurate test. \n",
    "As we can see, probability can be counterintuitive.\n",
    "\n",
    "What should a patient do upon receiving such terrifying news? Likely, the patient\n",
    "would ask the physician to administer another test to get clarity. The second\n",
    "test has different characteristics and it is not as good as the first one, as shown in :numref:`conditional_prob_D2`.\n",
    "\n",
    "\n",
    ":Conditional probability of $P(D_2 \\mid H)$.\n",
    "\n",
    "| Conditional probability | $H=1$ | $H=0$ |\n",
    "|---|---|---|\n",
    "|$P(D_2 = 1 \\mid H)$|            0.98 |         0.03 |\n",
    "|$P(D_2 = 0 \\mid H)$|            0.02 |         0.97 |\n",
    ":label:`conditional_prob_D2`\n",
    "\n",
    "Unfortunately, the second test comes back positive, too. \n",
    "Let us work out the requisite probabilities to invoke Bayes' theorem \n",
    "by assuming the conditional independence:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(D_1 = 1, D_2 = 1 \\mid H = 0) \\\\\n",
    "=& P(D_1 = 1 \\mid H = 0) P(D_2 = 1 \\mid H = 0)  \\\\\n",
    "=& 0.0003,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(D_1 = 1, D_2 = 1 \\mid H = 1) \\\\\n",
    "=& P(D_1 = 1 \\mid H = 1) P(D_2 = 1 \\mid H = 1)  \\\\\n",
    "=& 0.98.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we can apply marginalization and the multiplication rule:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(D_1 = 1, D_2 = 1) \\\\\n",
    "=& P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\\\\n",
    "=& P(D_1 = 1, D_2 = 1 \\mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \\mid H = 1)P(H=1)\\\\\n",
    "=& 0.00176955.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In the end, the probability of the patient having AIDS given both positive tests is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(H = 1 \\mid D_1 = 1, D_2 = 1)\\\\\n",
    "=& \\frac{P(D_1 = 1, D_2 = 1 \\mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} \\\\\n",
    "=& 0.8307.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "That is, the second test allowed us to gain much higher confidence that not all is well. Despite the second test being considerably less accurate than the first one, it still significantly improved our estimate.\n",
    "\n",
    "\n",
    "\n",
    "### 2.6.3Expectation and Variance\n",
    "To summarize key characteristics of probability distributions,\n",
    "we need some measures.\n",
    "The *expectation* (or average) of the random variable $X$ is denoted as\n",
    "\n",
    "$$E[X] = \\sum_{x} x P(X = x).$$\n",
    "\n",
    "When the input of a function $f(x)$ is a random variable drawn from the distribution $P$ with different values $x$,\n",
    "the expectation of $f(x)$ is computed as\n",
    "\n",
    "$$E_{x \\sim P}[f(x)] = \\sum_x f(x) P(x).$$\n",
    "\n",
    "\n",
    "In many cases we want to measure by how much the random variable $X$ deviates from its expectation. This can be quantified by the variance\n",
    "\n",
    "$$\\mathrm{Var}[X] = E\\left[(X - E[X])^2\\right] =\n",
    "E[X^2] - E[X]^2.$$\n",
    "\n",
    "Its square root is called the *standard deviation*.\n",
    "The variance of a function of a random variable measures\n",
    "by how much the function deviates from the expectation of the function,\n",
    "as different values $x$ of the random variable are sampled from its distribution:\n",
    "\n",
    "$$\\mathrm{Var}[f(x)] = E\\left[\\left(f(x) - E[f(x)]\\right)^2\\right].$$\n",
    "\n",
    "\n",
    "##### Summary\n",
    "+ We can use MXNet to sample from probability distributions.\n",
    "+ We can analyze multiple random variables using joint distribution, conditional distribution, Bayes' theorem, marginalization, and independence assumptions.\n",
    "+ Expectation and variance offer useful measures to summarize key characteristics of probability distributions.\n",
    "\n",
    "\n",
    "##### Exercises\n",
    "+ We conducted $m=500$ groups of experiments where each group draws $n=10$ samples. Vary $m$ and $n$. Observe and analyze the experimental results.\n",
    "+ Given two events with probability $P(\\mathcal{A})$ and $P(\\mathcal{B})$, compute upper and lower bounds on $P(\\mathcal{A} \\cup \\mathcal{B})$ and $P(\\mathcal{A} \\cap \\mathcal{B})$. (Hint: display the situation using a [Venn Diagram](https://en.wikipedia.org/wiki/Venn_diagram).)\n",
    "+ Assume that we have a sequence of random variables, say $A$, $B$, and $C$, where $B$ only depends on $A$, and $C$ only depends on $B$, can you simplify the joint probability $P(A, B, C)$? (Hint: this is a [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain).)\n",
    "+ In :numref:`subsec_probability_hiv_app`, the first test is more accurate. Why not just run the first test a second time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
