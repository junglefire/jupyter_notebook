{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from numpy import ndarray\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Callable, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(array: np.ndarray, array_grad: np.ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "        '''\n",
    "        Two ndarrays should have the same shape;\n",
    "        instead, first ndarray's shape is {0}\n",
    "        and second ndarray's shape is {1}.\n",
    "        '''.format(tuple(array_grad.shape), tuple(array.shape))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Deep Learning from Scratch\n",
    "You may not realize it, but you now have all the mathematical and conceptual foundations to answer the key questions about deep learning models that I posed at the beginning of the book: you understand how neural networks work—the computations involved with the matrix multiplications, the loss, and the partial derivatives with respect to that loss—as well as why those computations work (namely, the chain rule from calculus). We achieved this understanding by building neural networks from first principles, representing them as a series of “building blocks” where each building block was a single mathematical function. In this chapter, you’ll learn to represent these building blocks themselves as abstract Python classes and then use these classes to build deep learning models; by the end of this chapter, you will indeed have done “deep learning from scratch”!\n",
    "\n",
    "We’ll also map the descriptions of neural networks in terms of these building blocks to more conventional descriptions of deep learning models that you may have heard before. For example, by the end of this chapter, you’ll know what it means for a deep learning model to have “multiple hidden layers.” This is really the essence of understanding a concept: being able to translate between high-level descriptions and low-level details of what is actually going on. Let’s begin building toward this translation. So far, we’ve described models just in terms of the operations that happen at a low level. In the first part of this chapter, we’ll map this description of models to common higher-level concepts such as “layers” that will ultimately allow us to more easily describe more complex models.\n",
    "\n",
    "## 3.1 Deep Learning Definition: A First Pass\n",
    "What is a “deep learning” model? In the previous chapter, we defined a model as a mathematical function represented by a computational graph. The purpose of such a model was to try to map inputs, each drawn from some dataset with common characteristics (such as separate inputs representing different features of houses) to outputs drawn from a related distribution (such as the prices of those houses). We found that if we defined the model as a function that included parameters as inputs to some of its operations, we could “fit” it to optimally describe the data using the following procedure:\n",
    "1. Repeatedly feed observations through the model, keeping track of the quantities computed along the way during this “forward pass.”\n",
    "\n",
    "2. Calculate a loss representing how far off our model’s predictions were from the desired outputs or target.\n",
    "\n",
    "3. Using the quantities computed on the forward pass and the chain rule math worked out in `Chapter 1`, compute how much each of the input parameters ultimately affects this loss.\n",
    "\n",
    "4. Update the values of the parameters so that the loss will hopefully be reduced when the next set of observations is passed through the model.\n",
    "\n",
    "We started out with a model containing just a series of linear operations transforming our features into the target (which turned out to be equivalent to a traditional linear regression model). This had the expected limitation that, even when fit “optimally”, the model could nevertheless represent only linear relationships between our features and our target.\n",
    "\n",
    "We then defined a function structure that applied these linear operations first, then a nonlinear operation (the sigmoid function), and then a final set of linear operations. We showed that with this modification, our model could learn something closer to the true, nonlinear relationship between input and output, while having the additional benefit that it could learn relationships between combinations of our input features and the target.\n",
    "\n",
    "What is the connection between models like these and deep learning models? We’ll start with a somewhat clumsy attempt at a definition: ***deep learning models are represented by series of operations that have at least two, nonconsecutive nonlinear functions involved***.\n",
    "\n",
    "I’ll show where this definition comes from shortly, but first note that since deep learning models are just a series of operations, the process of training them is in fact identical to the process we’ve been using for the simpler models we’ve already seen. After all, what allows this training process to work is the differentiability of the model with respect to its inputs; and as mentioned in `Chapter 1`, the composition of differentiable functions is differentiable, so as long as the individual operations making up the function are differentiable, the whole function will be differentiable, and we’ll be able to train it using the same four-step training procedure just described.\n",
    "\n",
    "However, so far our approach to actually training these models has been to compute these derivatives by manually coding the forward and backward passes and then multiplying the appropriate quantities together to get the derivatives. For the simple neural network model in `Chapter 2`, this required 17 steps. Because we’re describing the model at such a low level, it isn’t immediately clear how we could add more complexity to this model (or what exactly what that would mean) or even make a simple change such as swapping out a different nonlinear function for the sigmoid function. To transition to being able to build arbitrarily “deep” and otherwise “complex” deep learning models, we’ll have to think about where in these 17 steps we can create reusable components, at a higher level than individual operations, that we can swap in and out to build different models. To guide us in the right direction as far as which abstractions to create, we’ll try to map the operations we’ve been using to traditional descriptions of neural networks as being made up of “layers,” “neurons,” and so on.\n",
    "\n",
    "As our first step, we’ll have to create an abstraction to represent the individual operations we’ve been working with so far, instead of continuing to code the same matrix multiplication and bias addition over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 The Building Blocks of Neural Networks: Operations\n",
    "The `Operation` class will represent one of the constituent functions in our neural networks. We know that at a high level, based on the way we’ve used such functions in our models, it should have forward and backward methods, each of which receives an `ndarray` as an input and outputs an `ndarray`. Some operations, such as matrix multiplication, seem to have another special kind of input, also an `ndarray`: the parameters. In our `Operation` class—or perhaps in another class that inherits from it—we should allow for `params` as another instance variable.\n",
    "\n",
    "Another insight is that there seem to be two types of `Operations`: \n",
    "+ some, such as the matrix multiplication, return an `ndarray` as output that is a different shape than the `ndarray` they received as input; \n",
    "+ by contrast, some `Operations`, such as the sigmoid function, simply apply some function to each element of the input `ndarray`. \n",
    "\n",
    "What, then, is the “general rule” about the shapes of the `ndarrays` that get passed between our operations? Let’s consider the `ndarrays` passed through our `Operations`: each `Operation` will send outputs forward on the forward pass and will receive an “output gradient” on the backward pass, which will represent the partial derivative of the loss with respect to every element of the `Operation`’s output (computed by the other `Operations` that make up the network). Also on the backward pass, each `Operation` will send an “input gradient” backward, representing the partial derivative of the loss with respect to each element of the input.\n",
    "\n",
    "These facts place a few important restrictions on the workings of our `Operations` that will help us ensure we’re computing the gradients correctly:\n",
    "+ The shape of the output gradient `ndarray` must match the shape of the output.\n",
    "\n",
    "+ The shape of the input gradient that the `Operation` sends backward during the backward pass must match the shape of the `Operation`’s input.\n",
    "\n",
    "This will all be clearer once you see it in a diagram; let’s look at that next.\n",
    "\n",
    "<img src=\"images/03_01_02.png\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for an \"operation\" in a neural network.\n",
    "class Operation(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Stores input in the self._input instance variable \n",
    "    # Calls the self._output() function.\n",
    "    def forward(self, input_: ndarray):\n",
    "        self.input_ = input_\n",
    "        self.output = self._output()\n",
    "        return self.output\n",
    "\n",
    "    # Calls the self._input_grad() function.\n",
    "    # Checks that the appropriate shapes match.\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    # The _output method must be defined for each Operation\n",
    "    def _output(self) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # The _input_grad method must be defined for each Operation\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any individual `Operation` that we define, we’ll have to implement the `_output` and `_input_grad` functions, so named because of the quantities they compute.\n",
    "\n",
    "> We’re defining base classes like this primarily for pedagogical reasons: it is important to have the mental model that all `Operations` you’ll encounter throughout deep learning fit this blueprint of sending inputs forward and gradients backward, with the shapes of what they receive on the forward pass matching the shapes of what they send backward on the backward pass, and vice versa.\n",
    "\n",
    "We’ll define the specific `Operations` we’ve used thus far—matrix multiplication and so on—later in this chapter. First we’ll define another class that inherits from `Operation` that we’ll use specifically for `Operations` that involve parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Operation with parameters.\n",
    "class ParamOperation(Operation):\n",
    "    # The ParamOperation method\n",
    "    def __init__(self, param: ndarray) -> ndarray:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    # Calls self._input_grad and self._param_grad.\n",
    "    # Checks appropriate shapes.\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "        return self.input_grad\n",
    "    \n",
    "    # Every subclass of ParamOperation must implement _param_grad.\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the base `Operation`, an individual `ParamOperation` would have to define the `_param_grad` function in addition to the `_output` and `_input_grad` functions.\n",
    "\n",
    "We have now formalized the neural network building blocks we’ve been using in our models so far. We could skip ahead and define neural networks directly in terms of these `Operations`, but there is an intermediate class we’ve been dancing around for a chapter and a half that we’ll define first: the `Layer`.\n",
    "\n",
    "## 3.3 The Building Blocks of Neural Networks: Layers\n",
    "In terms of `Operations`, layers are a series of linear operations followed by a nonlinear operation. For example, our neural network from the last chapter could be said to have had five total operations: two linear operations—a weight multiplication and the addition of a bias term—followed the sigmoid function and then two more linear operations. In this case, we would say that the first three operations, up to and including the nonlinear one, would constitute the first layer, and the last two operations would constitute the second layer. In addition, we say that the input itself represents a special kind of layer called the **input layer** (in terms of numbering the layers, this layer doesn’t count, so that we can think of it as the “zeroth” layer). The last layer, similarly, is called the **output layer**. The middle layer—the “first one,” according to our numbering—also has an important name: it is called a **hidden layer**, since it is the only layer whose values we don’t typically see explicitly during the course of training.\n",
    "\n",
    "The output layer is an important exception to this definition of layers, in that it does not have to have a nonlinear operation applied to it; this is simply because we often want the values that come out of this layer to have values between negative infinity and infinity (or at least between 0 and infinity), whereas nonlinear functions typically “squash down” their input to some subset of that range relevant to the particular problem we’re trying to solve (for example, the sigmoid function squashes down its input to between 0 and 1).\n",
    "\n",
    "To make the connection explicit, `Figure 3-3` shows the diagram of the neural network from the prior chapter with the individual operations grouped into layers.\n",
    "\n",
    "<img src=\"images/03_03.png\" style=\"width:600px;\"/>\n",
    "\n",
    "You can see that the input represents an “input” layer, the next three operations (ending with the sigmoid function) represent the next layer, and the last two operations represent the last layer.\n",
    "\n",
    "This is, of course, rather cumbersome. And that’s the point: representing neural networks as a series of individual operations, while showing clearly how neural networks work and how to train them, is too “low level” for anything more complicated than a two-layer neural network. That’s why the more common way to represent neural networks is in terms of layers, as shown in `Figure 3-4`.\n",
    "\n",
    "<img src=\"images/03_04.png\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "## 3.4 Building Blocks on Building Blocks\n",
    "What specific `Operations` do we need to implement for the models in the prior chapter to work? Based on our experience of implementing that neural network step by step, we know there are three kinds:\n",
    "+ The matrix multiplication of the input with the matrix of parameters\n",
    "+ The addition of a bias term\n",
    "+ The sigmoid activation function\n",
    "\n",
    "Let’s start with the `WeightMultiply Operation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight multiplication operation for a neural network.\n",
    "class WeightMultiply(ParamOperation):\n",
    "    # Initialize Operation with self.param = W.\n",
    "    def __init__(self, W: ndarray):\n",
    "        super().__init__(W)\n",
    "\n",
    "    # Compute output.\n",
    "    def _output(self) -> ndarray:\n",
    "        return np.dot(self.input_, self.param)\n",
    "\n",
    "    # Compute input gradient.\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
    "\n",
    "    # Compute parameter gradient.\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:      \n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply code up the matrix multiplication on the forward pass, as well as the rules for “sending gradients backward” to both the inputs and the parameters on the backward pass (using the rules for doing so that we reasoned through at the end of `Chapter 1`). As you’ll see shortly, we can now use this as a building block that we can simply plug into our `Layers`.\n",
    "\n",
    "Next up is the addition operation, which we’ll call `BiasAdd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bias addition.\n",
    "class BiasAdd(ParamOperation):\n",
    "    # Initialize Operation with self.param = B.\n",
    "    # Check appropriate shape.\n",
    "    def __init__(self, B: ndarray):\n",
    "        assert B.shape[0] == 1\n",
    "        super().__init__(B)\n",
    "\n",
    "    # Compute output.\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    # Compute input gradient.\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    # Compute parameter gradient.\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s do sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function.\n",
    "class Sigmoid(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    # Compute output.\n",
    "    def _output(self) -> ndarray:\n",
    "        return 1.0/(1.0+np.exp(-1.0 * self.input_))\n",
    "\n",
    "    # Compute input gradient.\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Identity\" activation function\n",
    "class Linear(Operation):\n",
    "    def __init__(self) -> None:       \n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve defined these Operations precisely, we can use them as building blocks to define a Layer.\n",
    "\n",
    "### The Layer Blueprint\n",
    "Because of the way we’ve written the `Operations`, writing the `Layer` class is easy:\n",
    "+ The forward and backward methods simply involve sending the input successively forward through a series of `Operations`—exactly as we’ve been doing in the diagrams all along! This is the most important fact about the working of Layers; the rest of the code is a wrapper around this and mostly involves bookkeeping:\n",
    "    - Defining the correct series of `Operations` in the `_setup_layer` function and initializing and storing the parameters in these `Operations` (which will also take place in the `_setup_layer` function)\n",
    "    - Storing the correct values in `self.input_` and `self.output` on the forward method\n",
    "    - Performing the correct assertion checking in the backward method\n",
    "+ Finally, the `_params` and `_param_grads` functions simply extract the parameters and their gradients (with respect to the loss) from the `ParamOperations` within the layer.\n",
    "\n",
    "Here’s what all that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"layer\" of neurons in a neural network.\n",
    "class Layer(object):\n",
    "    # The number of \"neurons\" roughly corresponds to the \"breadth\" of the layer\n",
    "    def __init__(self, neurons: int):\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    # The _setup_layer function must be implemented for each layer\n",
    "    def _setup_layer(self, num_in: int) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Passes input forward through a series of operations\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "        self.input_ = input_\n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "        self.output = input_\n",
    "        return self.output\n",
    "\n",
    "    # Passes output_grad backward through a series of operations\n",
    "    # Checks appropriate shapes\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "        input_grad = output_grad\n",
    "        self._param_grads()\n",
    "        return input_grad\n",
    "\n",
    "    # Extracts the _param_grads from a layer's operations\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    # Extracts the _params from a layer's operations\n",
    "    def _params(self) -> ndarray:\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we moved from an abstract definition of an `Operation` to the implementation of specific `Operations` needed for the neural network from `Chapter 2`, let’s now implement the Layer from that network as well.\n",
    "\n",
    "### The Dense Layer\n",
    "We called the `Operations` we’ve been dealing with `WeightMultiply`, `BiasAdd`, and so on. What should we call the layer we’ve been using so far? A `LinearNonLinear` layer?\n",
    "\n",
    "A defining characteristic of this layer is that each output neuron is a function of all of the input neurons. Thus these layers are often called `fully connected layers`; recently, in the popular `Keras` library, they are also often called `Dense layers`, a more concise term that gets across the same idea.\n",
    "\n",
    "Now that we know what to call it and why, let’s define the `Dense layer` in terms of the operations we’ve already defined—as you’ll see, because of how we defined our `Layer` base class, all we need to do is to put the `Operations` defined in the previous section in as a list in the `_setup_layer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fully connected layer which inherits from \"Layer\"\n",
    "class Dense(Layer):\n",
    "    # Requires an activation function upon initialization\n",
    "    def __init__(self, neurons: int, activation: Operation = Sigmoid()):\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    # Defines the operations of a fully connected layer.\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "        self.params = []\n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "        # bias\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What building blocks should we now add on top of `Operation` and `Layer`? To train our model, we know we’ll need a `NeuralNetwork` class to wrap around Layers, just as `Layers` wrapped around `Operations`. It isn’t obvious what other classes will be needed, so we’ll just dive in and build `NeuralNetwork` and figure out the other classes we’ll need as we go.\n",
    "\n",
    "\n",
    "## 3.5 The NeuralNetwork Class, and Maybe Others\n",
    "What should our `NeuralNetwork` class be able to do? At a high level, it should be able to learn from data: more precisely, it should be able to take in batches of data representing “observations” ($X$) and “correct answers” ($y$) and learn the relationship between $X$ and $y$, which means learning a function that can transform $X$ into predictions $p$ that are very close to $y$.\n",
    "\n",
    "How exactly will this learning take place, given the `Layer` and `Operation` classes just defined? Recalling how the model from the last chapter worked, we’ll implement the following:\n",
    "1. The neural network should take $X$ and pass it successively forward through each `Layer` (which is really a convenient wrapper around feeding it through many `Operations`), at which point the result will represent the prediction.\n",
    "\n",
    "2. Next, prediction should be compared with the value $y$ to calculate the loss and generate the “loss gradient,” which is the partial derivative of the loss with respect to each element in the last layer in the network (namely, the one that generated the prediction).\n",
    "\n",
    "3. Finally, we’ll send this loss gradient successively backward through each layer, along the way computing the “parameter gradients”—the partial derivative of the loss with respect to each of the parameters—and storing them in the corresponding Operations.\n",
    "\n",
    "<img src=\"images/03_05.png\" style=\"width:600px;\"/>\n",
    "\n",
    "### Loss Class\n",
    "How should we implement this? First, we’ll want our neural network to ultimately deal with `Layers` the same way our `Layers` dealt with `Operations`. For example, we want the forward method to receive $X$ as input and simply do something like:\n",
    "\n",
    "```python\n",
    "for layer in self.layers:\n",
    "    X = layer.forward(X)\n",
    "return X\n",
    "```\n",
    "\n",
    "Similarly, we’ll want our backward method to take in an argument—let’s initially call it grad—and do something like:\n",
    "\n",
    "```python\n",
    "for layer in reversed(self.layers): \n",
    "    grad = layer.backward(grad)\n",
    "```\n",
    "\n",
    "Where will grad come from? It has to come from the loss, a special function that takes in the prediction along with $y$ and:\n",
    "+ Computes a single number representing the “penalty” for the network making that prediction.\n",
    "\n",
    "+ Sends backward a gradient for every element of the prediction with respect to the loss. This gradient is what the last Layer in the network will receive as the input to its backward function.\n",
    "\n",
    "In the example from the prior chapter, the loss function was the squared difference between the prediction and the target, and the gradient of the prediction with respect to the loss was computed accordingly.\n",
    "\n",
    "How should we implement this? It seems like this concept is important enough to deserve its own class. Furthermore, this class can be implemented similarly to the `Layer` class, except the forward method will produce an actual number (a float) as the loss, instead of an ndarray to be sent forward to the next `Layer`. Let’s formalize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"loss\" of a neural network\n",
    "class Loss(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Computes the actual loss value\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        assert_same_shape(prediction, target)\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        loss_value = self._output()\n",
    "        return loss_value\n",
    "\n",
    "    # Computes gradient of the loss value with respect to the input to the loss function\n",
    "    def backward(self) -> ndarray:\n",
    "        self.input_grad = self._input_grad()\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "    # Every subclass of \"Loss\" must implement the _output function.\n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Every subclass of \"Loss\" must implement the _input_grad function.\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the `Operation` class, we check that the gradient that the loss sends backward is the same shape as the prediction received as input from the last layer of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    # Computes the per-observation squared error loss\n",
    "    def _output(self) -> float:\n",
    "        loss = (np.sum(np.power(self.prediction - self.target, 2))/self.prediction.shape[0])\n",
    "        return loss\n",
    "    \n",
    "    # Computes the loss gradient with respect to the input for MSE loss\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we simply code the forward and backward rules of the mean squared error loss formula.\n",
    "\n",
    "This is the last key building block we need to build deep learning from scratch. Let’s review how these pieces fit together and then proceed with building a model!\n",
    "\n",
    "\n",
    "## 3.6 Deep Learning from Scratch\n",
    "We ultimately want to build a `NeuralNetwork` class, using `Figure 3-5` as a guide, that we can use to define and train deep learning models. Before we dive in and start coding, let’s describe precisely what such a class would be and how it would interact with the `Operation`, `Layer`, and `Loss` classes we just defined:\n",
    "\n",
    "1. A `NeuralNetwork` will have a list of `Layers` as an attribute. The `Layers` would be as defined previously, with forward and backward methods. These methods take in `ndarray` objects and return `ndarray` objects.\n",
    "\n",
    "2. Each `Layer` will have a list of `Operations` saved in the operations attribute of the layer during the `_setup_layer` function.\n",
    "\n",
    "3. These `Operations`, just like the Layer itself, have forward and backward methods that take in `ndarray` objects as arguments and return `ndarray` objects as outputs.\n",
    "\n",
    "4. In each operation, the shape of the `output_grad` received in the backward method must be the same as the shape of the `output` attribute of the `Layer`. The same is true for the shapes of the `input_grad` passed backward during the backward method and the `input_` attribute.\n",
    "\n",
    "5. Some operations have parameters (stored in the `param` attribute); these operations inherit from the `ParamOperation` class. The same constraints on input and output shapes apply to `Layers` and their forward and backward methods as well — they take in `ndarray` objects and output `ndarray` objects, and the shapes of the input and output attributes and their corresponding gradients must match.\n",
    "\n",
    "6. A `NeuralNetwork` will also have a `Loss`. This class will take the output of the last operation from the `NeuralNetwork` and the target, check that their shapes are the same, and calculate both a loss value (a number) and an `ndarray` `loss_grad` that will be fed into the output layer, starting **backpropagation**.\n",
    "\n",
    "### Implementing Batch Training\n",
    "We’ve covered several times the high-level steps for training a model one batch at a time. They are important and worth repeating:\n",
    "1. Feed input through the model function (the “forward pass”) to get a prediction.\n",
    "\n",
    "2. Calculate the number representing the loss.\n",
    "\n",
    "3. Calculate the gradient of the loss with respect to the parameters, using the chain rule and the quantities computed during the forward pass.\n",
    "\n",
    "4. Update the parameters using these gradients.\n",
    "\n",
    "We would then feed a new batch of data through and repeat these steps.\n",
    "\n",
    "Translating these steps into the `NeuralNetwork` framework just described is straightforward:\n",
    "1. Receive $X$ and $y$ as inputs, both `ndarrays`.\n",
    "\n",
    "2. Feed $X$ successively forward through each `Layer`.\n",
    "\n",
    "3. Use the `Loss` to produce loss value and the loss gradient to be sent backward.\n",
    "\n",
    "4. Use the loss gradient as input to the backward method for the network, which will calculate the `param_grads` for each layer in the network.\n",
    "\n",
    "5. Call the `update_params` function on each layer, which will use the overall learning rate for the `NeuralNetwork` as well as the newly calculated `param_grads`.\n",
    "\n",
    "We finally have our full definition of a neural network that can accommodate batch training. Now let’s code it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class for a neural network.\n",
    "class NeuralNetwork(object):\n",
    "    # Neural networks need layers, and a loss.\n",
    "    def __init__(self, layers: List[Layer], loss: Loss, seed: int = 1) -> None:\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)        \n",
    "\n",
    "    # Passes data forward through a series of layers.\n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    # Passes data backward through a series of layers.\n",
    "    def backward(self, loss_grad: ndarray) -> None:\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return None\n",
    "    \n",
    "    # Passes data forward through the layers.\n",
    "    # Computes the loss.\n",
    "    # Passes data backward through the layers.\n",
    "    def train_batch(self, x_batch: ndarray, y_batch: ndarray) -> float:\n",
    "        predictions = self.forward(x_batch)\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "        self.backward(self.loss.backward())\n",
    "        return loss\n",
    "    \n",
    "    # Gets the parameters for the network.\n",
    "    def params(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "\n",
    "    # Gets the gradient of the loss with respect to the parameters for the network.\n",
    "    def param_grads(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this `NeuralNetwork` class, we can implement the models from the prior chapter in a more modular, flexible way and define other models to represent complex nonlinear relationships between input and output. For example, here’s how to easily instantiate the two models we covered in the last chapter—the linear regression and the neural network:\n",
    "\n",
    "```python\n",
    "linear_regression = NeuralNetwork(\n",
    "    layers=[Dense(neurons = 1)], \n",
    "    loss = MeanSquaredError(), \n",
    "    learning_rate = 0.01)\n",
    "\n",
    "neural_network = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13, activation=Sigmoid()), \n",
    "            Dense(neurons=1, activation=Linear())], \n",
    "    loss = MeanSquaredError(), \n",
    "    learning_rate = 0.01 )\n",
    "```\n",
    "\n",
    "We’re basically done; now we just feed data repeatedly through the network in order for it to learn. To make this process cleaner and easier to extend to the more complicated deep learning scenarios we’ll see in the following chapter, however, it will help us to define another class that carries out the training, as well as an additional class that carries out the “learning”, or the actual updating of the `NeuralNetwork` parameters given the gradients computed on the backward pass. Let’s quickly define these two classes.\n",
    "\n",
    "\n",
    "## 3.7 Trainer and Optimizer\n",
    "First, let’s note the similarities between these classes and the code we used to train the network in `Chapter 2`. There, we used the following code to implement the four steps described earlier for training the model:\n",
    "\n",
    "```python\n",
    "# pass X_batch forward and compute the loss \n",
    "forward_info, loss = forward_loss(X_batch, y_batch, weights)\n",
    "\n",
    "# compute the gradient of the loss with respect to each of the weights \n",
    "loss_grads = loss_gradients(forward_info, weights)\n",
    "\n",
    "# update the weights \n",
    "for key in weights.keys():\n",
    "    weights[key] -= learning_rate * loss_grads[key]\n",
    "```\n",
    "\n",
    "This code was within a for loop that repeatedly fed data through the function defining and updated our network.\n",
    "\n",
    "With the classes we have now, we’ll ultimately do this inside a fit function within the `Trainer` class that will mostly be a wrapper around the train function used in the prior chapter. The main difference is that inside this new function, the first two lines from the preceding code block will be replaced with this line:\n",
    "\n",
    "```python\n",
    "neural_network.train_batch(X_batch, y_batch)\n",
    "```\n",
    "\n",
    "Updating the parameters, which happens in the following two lines, will take place in a separate `Optimizer` class. And finally, the for loop that previously wrapped around all of this will take place in the `Trainer` class that wraps around the `NeuralNetwork` and the `Optimizer`.\n",
    "\n",
    "Next, let’s discuss why we need an `Optimizer` class and what it should look like.\n",
    "\n",
    "### Optimizer\n",
    "In the model we described in the last chapter, each `Layer` contains a simple rule for updating the weights based on the parameters and their gradients. As we’ll touch on in the next chapter, there are many other update rules we can use, such as ones involving the history of gradient updates rather than just the gradient updates from the specific batch that was fed in at that iteration. Creating a separate `Optimizer` class will give us the flexibility to swap in one update rule for another, something that we’ll explore in more detail in the next chapter.\n",
    "\n",
    "The base `Optimizer` class will take in a `NeuralNetwork` and, every time the step function is called, will update the parameters of the network based on their current values, their gradients, and any other information stored in the `Optimizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for a neural network optimizer.\n",
    "class Optimizer(object):\n",
    "    # Every optimizer must have an initial learning rate.\n",
    "    def __init__(self, lr: float = 0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    # Every optimizer must implement the \"step\" function.\n",
    "    def step(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here’s how this looks with the straightforward update rule we’ve seen so far, known as *stochastic gradient descent*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochasitc gradient descent optimizer.\n",
    "class SGD(Optimizer):   \n",
    "    def __init__(self, lr: float = 0.01) -> None:\n",
    "        super().__init__(lr)\n",
    "\n",
    "    # For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment \n",
    "    # based on the learning rate.\n",
    "    def step(self):\n",
    "        for (param, param_grad) in zip(self.net.params(), self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that while our `NeuralNetwork` class does not have an `_update_params` method, we do rely on the `params()` and `param_grads()` methods to extract the correct `ndarrays` for optimization.\n",
    "\n",
    "That’s the basic `Optimizer` class; let’s cover the `Trainer` class next.\n",
    "\n",
    "### Trainer\n",
    "In addition to training the model as described previously, the `Trainer` class also links together the `NeuralNetwork` with the `Optimizer`, ensuring the latter trains the former properly. You may have noticed in the previous section that we didn’t pass in a `NeuralNetwork` when initializing our `Optimizer`; instead, we’ll assign the `NeuralNetwork` to be an attribute of the `Optimizer` when we initialize the `Trainer` class shortly, with this line:\n",
    "\n",
    "```python\n",
    "setattr(self.optim, 'net', self.net)\n",
    "```\n",
    "\n",
    "In the following subsection, I show a simplified but working version of the `Trainer` class that for now contains just the fit method. This method trains our model for a number of epochs and prints out the loss value after each set number of epochs. In each epoch, we:\n",
    "1. Shuffle the data at the beginning of the epoch\n",
    "\n",
    "2. Feed the data through the network in batches, updating the parameters after each batch has been fed through\n",
    "\n",
    "The epoch ends when we have fed the entire training set through the `Trainer`.\n",
    "\n",
    "In the following is the code for a simple version of the `Trainer` class, we hide two self-explanatory helper methods used during the fit function: \n",
    "+ **generate_batches**: generates batches of data from `X_train` and `y_train` for training\n",
    "+ **permute_data**: shuffles `X_train` and `y_train` at the beginning of each epoch\n",
    "\n",
    "We also include a restart argument in the train function: if `True` (default), it will reinitialize the model’s parameters to random values upon calling the train function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a neural network\n",
    "class Trainer(object):\n",
    "    # Requires a neural network and an optimizer in order for training to occur. \n",
    "    # Assign the neural network as an instance variable to the optimizer.\n",
    "    def __init__(self, net: NeuralNetwork, optim: Optimizer) -> None:\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "    \n",
    "    # Generates batches for training \n",
    "    def generate_batches(self, X: ndarray, y: ndarray, size: int = 32) -> Tuple[ndarray]:\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "        '''\n",
    "        features and target must have the same number of rows, instead\n",
    "        features has {0} and target has {1}\n",
    "        '''.format(X.shape[0], y.shape[0])\n",
    "        # gen batch\n",
    "        N = X.shape[0]\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "    # Fits the neural network on the training data for a certain number of epochs.\n",
    "    # Every \"eval_every\" epochs, it evaluated the neural network on the testing data.\n",
    "    def fit(self, X_train: ndarray, y_train: ndarray, X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 1,\n",
    "            restart: bool = True)-> None:\n",
    "        np.random.seed(seed)\n",
    "        # restart\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "            self.best_loss = 1e9\n",
    "        # epoch loop\n",
    "        for e in range(epochs):\n",
    "            if (e+1) % eval_every == 0:\n",
    "                # for early stopping\n",
    "                last_model = deepcopy(self.net)\n",
    "            # shuffles X_train and y_train at the beginning of each epoch\n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "            batch_generator = self.generate_batches(X_train, y_train, batch_size)\n",
    "            # batch loop\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "                self.optim.step()\n",
    "            # test\n",
    "            if (e+1) % eval_every == 0:\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, \n",
    "                        using the model from epoch {e+1-eval_every}\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Putting Everything Together\n",
    "Here is the full code to train our network using all the `Trainer` and `Optimizer` classes. We’ll set the learning rate to $0.01$ and the maximum number of epochs to $50$ and evaluate our models every $10$ epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean absolute error for a neural network.\n",
    "def mae(y_true: ndarray, y_pred: ndarray): \n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Compute root mean squared error for a neural network.\n",
    "def rmse(y_true: ndarray, y_pred: ndarray):\n",
    "    return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n",
    "\n",
    "# Compute mae and rmse for a neural network.\n",
    "def eval_regression_model(model: NeuralNetwork, X_test: ndarray, y_test: ndarray):\n",
    "    preds = model.forward(X_test)\n",
    "    preds = preds.reshape(-1, 1)\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mae(preds, y_test)))\n",
    "    print(\"Root mean squared error {:.2f}\".format(rmse(preds, y_test)))\n",
    "    \n",
    "# Turns a 1D Tensor into 2D\n",
    "def to_2d_np(a: ndarray, type: str=\"col\") -> ndarray:\n",
    "    assert a.ndim == 1, \"Input tensors must be 1 dimensional\"\n",
    "    if type == \"col\":        \n",
    "        return a.reshape(-1, 1)\n",
    "    elif type == \"row\":\n",
    "        return a.reshape(1, -1)\n",
    "\n",
    "# shuffles dataset\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load boston housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "\n",
    "# make target 2d array\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 30.293\n",
      "Validation loss after 20 epochs is 28.469\n",
      "Validation loss after 30 epochs is 26.293\n",
      "Validation loss after 40 epochs is 25.541\n",
      "Validation loss after 50 epochs is 25.087\n",
      "\n",
      "Mean absolute error: 3.52\n",
      "Root mean squared error 5.01\n"
     ]
    }
   ],
   "source": [
    "lr = NeuralNetwork(\n",
    "    layers=[Dense(neurons=1, activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "trainer = Trainer(lr, SGD(lr=0.01))\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "eval_regression_model(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 27.435\n",
      "Validation loss after 20 epochs is 21.839\n",
      "Validation loss after 30 epochs is 18.918\n",
      "Validation loss after 40 epochs is 17.195\n",
      "Validation loss after 50 epochs is 16.215\n",
      "\n",
      "Mean absolute error: 2.60\n",
      "Root mean squared error 4.03\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13, activation=Sigmoid()),\n",
    "            Dense(neurons=1, activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "trainer = Trainer(nn, SGD(lr=0.01))\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "eval_regression_model(nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 44.143\n",
      "Validation loss after 20 epochs is 25.278\n",
      "Validation loss after 30 epochs is 22.339\n",
      "Validation loss after 40 epochs is 16.500\n",
      "Validation loss after 50 epochs is 14.655\n",
      "\n",
      "Mean absolute error: 2.45\n",
      "Root mean squared error 3.83\n"
     ]
    }
   ],
   "source": [
    "dl = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13, activation=Sigmoid()),\n",
    "            Dense(neurons=13, activation=Sigmoid()),\n",
    "            Dense(neurons=1, activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "trainer = Trainer(dl, SGD(lr=0.01))\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "eval_regression_model(dl, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
