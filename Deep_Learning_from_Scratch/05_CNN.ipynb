{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from scipy.special import logsumexp\n",
    "from numpy import ndarray\n",
    "\n",
    "from typing import Callable, List\n",
    "\n",
    "# 导入smartflow\n",
    "from smartflow.operation import *\n",
    "from smartflow.operation import *\n",
    "from smartflow.trainer import *\n",
    "from smartflow.layer import *\n",
    "from smartflow.loss import *\n",
    "from smartflow.util import *\n",
    "from smartflow.eval import *\n",
    "from smartflow.nn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置logging输出到jupyter notebook\n",
    "\n",
    "import importlib as implib\n",
    "import logging as log\n",
    "import sys\n",
    "\n",
    "implib.reload(log)\n",
    "log.basicConfig(format=\"[smartflow] %(message)s\", stream=sys.stdout, level=log.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. CNN\n",
    "In this chapter, we’ll cover `CNN`. `CNN` are the standard neural network architecture used for prediction when the input observations are images. \n",
    "\n",
    "In the book, we’ve focused exclusively on fully connected neural networks, which we implemented as a series of `Dense` layers. Thus, we’ll start this chapter by reviewing some key elements of these networks and use this to motivate why we might want to use a different architecture for images. We’ll then cover `CNN` in a manner similar to that in which we introduced other concepts in this book: we’ll first discuss how they work at a high level, then move to discussing them at a lower level, and finally show in detail how they work by coding up the convolution operation from scratch. By the end of this chapter, you’ll have a thorough enough understanding of how `CNN` work to be able to use them both to solve problems and to learn about advanced `CNN` variants, such as `ResNet`, `DenseNet`, and `Octave Convolution` on your own.\n",
    "\n",
    "## 5.1 Neural Networks and Representation Learning\n",
    "Neural networks initially receive data on observations, with each observation represented by some number $n$ features. So far we’ve seen two examples of this in two very different domains: the first was the house prices dataset, where each observation was made up of $13$ features, each of which represented a numeric characteristic about that house. The second was the MNIST dataset of handwritten digits; since the images were represented with $784$ pixels, each observation was represented by $784$ values indicating the lightness or darkness of each pixel.\n",
    "\n",
    "In each case, after appropriately scaling the data, we were able to build a model that predicted the appropriate outcome for that dataset with high accuracy. Also in each case, a simple neural network model with one hidden layer performed better than a model without that hidden layer. Why is that? One reason, as I showed in the case of the house prices data, is that the neural network could learn nonlinear relationships between input and output. However, a more general reason is that in machine learning, we often need linear combinations of our original features in order to effectively predict our target. \n",
    "\n",
    "Let’s say that the pixel values for an MNIST digit are $x_1$ through $x_{784}$. It could be the case, for example, that a combination of $x_1$ being higher than average, $x_{139}$ being lower than average, and $x_{237}$ also being lower than average strongly predicts that an image will be of digit $9$. There may be many other such combinations, all of which contribute positively or negatively to the probability that an image is of a particular digit. Neural networks can automatically discover combinations of the original features that are important through their training process. That process starts by creating initially random combinations of the original features via multiplication by a random weight matrix; through training, the neural network learns to refine combinations that are helpful and discard those that aren’t. This process of learning which combinations of features are important is known as `representation learning`, and it’s the main reason why neural networks are successful across different domains. This is summarized in `Figure 5-1`.\n",
    "\n",
    "<img src=\"images/05_01.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Is there any reason to modify this process for image data? The fundamental insight that suggests the answer is “yes” is that in images, the interesting \"combinations of features\" (pixels) tend to come from pixels that are close together in the image. In an image, it is simply much less likely that an interesting feature will result from a combination of $9$ randomly selected pixels throughout the image than from a $3\\times 3$ patch of adjacent pixels. We want to exploit this fundamental fact about image data: that the order of the features matters since it tells us which pixels are near each other spatially, whereas in the house prices data the order of the features doesn’t matter. But how do we do it?\n",
    "\n",
    "### A Different Architecture for Image Data\n",
    "The solution, at a high level, will be to create combinations of features, as before, but an order of magnitude more of them, and have each one be only a combination of the pixels from a small rectangular patch in the input image. `Figure 5-2` describes this.\n",
    "\n",
    "<img src=\"images/05_02.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Having our neural network learn combinations of all of the input features, that is, combinations of all of the pixels in the input image, turns out to be very inefficient, since it ignores the insight described in the prior section: that most of the interesting combinations of features in images occur in these small patches. Nevertheless, previously it was at least extremely easy to compute new features that were combinations of all the input features: if we had $f$ input features and wanted to compute $n$ new features, we could simply multiply the `ndarray` containing our input features by an $f\\times n$ matrix. What operation can we use to compute many combinations of the pixels from local patches of the input image? The answer is the *convolution operation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Convolution Operation\n",
    "Before we describe the convolution operation, let’s make clear what we mean by \"a feature that is a combination of pixels from a local patch of an image\". Let’s say we have a $5 \\times 5$ input image `I`:\n",
    "\n",
    "$$I = \\begin{bmatrix} i_{11} & i_{12} & i_{13} & i_{14} & i_{15} \\\\ i_{21} & i_{22} & i_{23} & i_{24} & i_{25} \\\\ i_{31} & i_{32} & i_{33} & i_{34} & i_{35} \\\\ i_{41} & i_{42} & i_{43} & i_{44} & i_{45} \\\\ i_{51} & i_{52} & i_{53} & i_{54} & i_{55} \\end{bmatrix}$$\n",
    "\n",
    "And let’s say we want to calculate a new feature that is a function of the $3 \\times 3$ patch of pixels in the middle. Well, just as we’ve defined new features as linear combinations of old features in the neural networks we’ve seen so far, we’ll define a new feature that is a function of this $3 \\times 3$ patch, which we’ll do by defining a $3 \\times 3$ set of weights, $W$:\n",
    "\n",
    "$$W = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33}  \\end{bmatrix}$$\n",
    "\n",
    "Then we’ll simply take the dot product of $W$ with the relevant patch from $I$ to get the value of the feature in the output, which, since the section of the input image involved was centered at $(3,3)$, we’ll denote as $o_{33}$ (the $o$ stands for \"output\"):\n",
    "\n",
    "$$o_{33} = w_{11}i_{22} + w_{12}i_{23} + w_{13}i_{24} + w_{21}i_{32} + w_{22}i_{33} + w_{23}i_{34} + w_{31}i_{42} + w_{32}i_{43} + w_{33}i_{44}$$\n",
    "\n",
    "This value will then be treated like the other computed features we’ve seen in neural networks: it may have a bias added to it and then will probably be fed through an activation function, and then it will represent a \"neuron\" or \"learned feature\" that will get passed along to subsequent layers of the network. Thus we can define features that are functions of small patches of an input image.\n",
    "\n",
    "How should we interpret such features? It turns out that features computed in this way have a special interpretation: they represent whether a visual pattern defined by the weights is present at that location of the image. The fact that $3 \\times 3$ or $5 \\times 5$ arrays of numbers can represent \"pattern detectors\" when their dot product is taken with the pixel values at each location of an image has been well known in the field of computer vision for a long time. For example, taking the dot product of the following $3 \\times 3$ array of numbers:\n",
    "\n",
    "$$\\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}$$\n",
    "\n",
    "with a given section of an input image detects whether there is an edge at that location of the image. There are similar matrices known to be able to detect whether corners exist, whether vertical or horizontal lines exist, and so on.\n",
    "\n",
    "Now suppose that we used the same set of weights $W$ to detect whether the visual pattern defined by $W$ existed at each location in the input image. We could imagine \"sliding $W$ over the input image\", taking the dot product of $W$ with the pixels at each location of the image, and ending up with a new image $O$ of almost identical size to the original image (it may be slightly different, depending on how we handle the edges). This image $O$ would be a kind of \"feature map\" showing the locations in the input image where the pattern defined by $W$ was present. This operation is in fact what happens in convolutional neural networks; it is called a *convolution*, and its output is indeed called a *feature map*.\n",
    "\n",
    "This operation is at the core of how `CNN` work. Before we can incorporate it into a full-fledged `Operation`, of the kind we’ve seen in the prior chapters, we have to add another dimension to it—literally.\n",
    "\n",
    "### The Multichannel Convolution Operation\n",
    "To review: `CNN` differ from regular neural networks in that they create an order of magnitude more features, and in that each feature is a function of just a small patch from the input image. Now we can get more specific: starting with $n$ input pixels, the convolution operation just described will create $n$ output features, one for each location in the input image. What actually happens in a convolutional `Layer` in a neural network goes one step further: there, we’ll create $f$ sets of $n$ features, each with a corresponding (initially random) set of weights defining a visual pattern whose detection at each location in the input image will be captured in the feature map. These $f$ feature maps will be created via $f$ convolution operations. This is captured in `Figure 5-3`.\n",
    "\n",
    "<img src=\"images/05_03.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Now that we’ve introduced a bunch of concepts, let’s define them for clarity. While each \"set of features\" detected by a particular set of weights is called a **feature map**, in the context of a **convolutional Layer**, the number of feature maps is referred to as the number of **channels** of the Layer, this is why the operation involved with the `Layer` is called the **multichannel convolution**. In addition, the $f$ sets of weights $W_i$ are called the **convolutional filters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Convolutional Layers\n",
    "Now that we understand the multichannel convolution operation, we can think about how to incorporate this operation into a neural network layer. Previously, our neural network layers were relatively straightforward: they received two-dimensional `ndarrays` as input and produced two-dimensional ndarrays as output. Based on the description in the prior section, however, convolutional layers will have a 3D `ndarray` as output for a single image, with dimensions $\\textit{number of channels}\\times\\textit{image height}\\times\\textit{image width}$.\n",
    "\n",
    "This raises a question: how can we feed this `ndarray` forward into another convolutional layer to create a \"deep convolutional\" neural network? We’ve seen how to perform the convolution operation on an image with a single channel and our filters; how can we perform the multichannel convolution on an input with multiple channels, as we’ll have to do when two convolutional layers are strung together? Understanding this is the key to understanding deep convolutional neural networks.\n",
    "\n",
    "Consider what happens in a neural network with fully connected layers: in the first hidden layer, we have, let’s say, $h_1$ features that are combinations of all of the original features from the input layer. In the layer that follows, the features are combinations of all of the features from the prior layer, so that we might have $h_2$ \"features of features\" of the original features. To create this next layer of $h_2$ features, we use $h_1 \\times h_2$  weights to represent that each of the $h_2$ features is a function of each of the $h_1$ features in the prior layer.\n",
    "\n",
    "As described in the prior section, an analogous process happens in the first layer of a convolutional neural network: we first transform the input image into $m_1$ feature maps, using $m_1$ convolutional filters. We should think of the output of this layer as representing whether each of the $m_1$ different visual patterns represented by the weights of the $m_1$ filters is present at each location in the input image. Just as different layers of a fully connected neural network can contain different numbers of neurons, the next layer of the convolutional neural network could contain $m_2$ filters. In order for the network to learn complex patterns, the interpretation of each of these should be whether each of the \"patterns of patterns\" or higher-order visual features represented by combinations of the $m_1$ visual patterns from the prior layer was present at that location of the image. This implies that if the output of the convolutional layer is a 3D `ndarray` of shape $m_2 \\textit{channels}\\times\\textit{image height}\\times\\textit{image width}$, then a given location in the image on one of the $m_2$ feature maps is a linear combination of convolving $m_1$ different filters over that same location in each of the corresponding $m_1$ feature maps from the prior layer. This will allow each location in each of the $m_2$ filter maps to represent a combination of the $m_1$ visual features already learned in the prior convolutional layer.\n",
    "\n",
    "### Implementation Implications\n",
    "This understanding of how two multichannel convolutional layers are connected tells us how to implement the operation: just as we need $h_1\\times h_2$ weights to connect a fully connected layer with $h_1$ neurons to one with $h_2$, we need $m_1\\times m_2$ convolutional filters to connect a convolutional layer with $m_1$ channels to one with $m_2$. With this last detail in place, we can now specify the dimensions of the `ndarrays` that will make up the input, output, and parameters of the full, multichannel convolution operation:\n",
    "\n",
    "1. The input will have shape:\n",
    "    + Batch size\n",
    "    + Input channels\n",
    "    + Image height\n",
    "    + Image width\n",
    "\n",
    "2. The output will have shape:\n",
    "    + Batch size\n",
    "    + Output channels\n",
    "    + Image height\n",
    "    + Image width\n",
    "\n",
    "3. The convolutional filters themselves will have shape:\n",
    "    + Input channels\n",
    "    + Output channels\n",
    "    + Filter height\n",
    "    + Filter width\n",
    "\n",
    "> The order of the dimensions may vary from library to library, but these four dimensions will always be present.\n",
    "\n",
    "We’ll keep all of this in mind when we implement this convolution operation later in the chapter.\n",
    "\n",
    "### The Differences Between Convolutional and Fully Connected Layers\n",
    "At the beginning of the chapter, we discussed the differences between convolutional and fully connected layers at a high level; `Figure 5-4` revisits that comparison, now that we’ve described convolutional layers in more detail.\n",
    "\n",
    "<img src=\"images/05_04.png\" style=\"width:600px;\"/>\n",
    "\n",
    "In addition, one last difference between the two kinds of layers is the way in which the individual neurons themselves are interpreted:\n",
    "+ The interpretation of each neuron of a fully connected layer is that it detects whether or not a particular combination of the features learned by the prior layer is present in the current observation.\n",
    "\n",
    "+ The interpretation of a neuron of a convolutional layer is that it detects whether or not a particular combination of visual patterns learned by the prior layer is present at the given location of the input image.\n",
    "\n",
    "There’s one more problem we need to solve before we can incorporate such a layer into a neural network: how to use the dimensional `ndarrays` we obtain as output to make predictions.\n",
    "\n",
    "### Making Predictions with Convolutional Layers: The Flatten Layer\n",
    "We’ve covered how convolutional layers learn features that represent whether visual patterns exist in images and store those features in layers of feature maps; how do we use these layers of feature maps to make predictions? When using fully connected neural networks to predict which of $10$ classes an image belonged to in the prior chapter, we just had to ensure that the last layer had dimension $10$; we could then feed these $10$ numbers into the softmax cross entropy loss function to ensure they were interpreted as probabilities. Now we need to figure out what we can do in the case of our convolutional layer, where we have a three-dimensional ndarray per observation of shape $m-\\textit{channels}\\times\\textit{image height}\\times\\textit{image width}$.\n",
    "\n",
    "To see the answer, recall that each neuron simply represents whether a particular combination of visual features (which, if this is a deep convolutional neural network, could be a feature of features or a feature of features of features) is present at a given location in the image. This is no different from the features that would be learned if we applied a fully connected neural network to this image: the first fully connected layer would represent features of the individual pixels, the second would represent features of these features, and so on. And in a fully connected architecture, we would simply treat each \"feature of features\" that the network had learned as a single neuron that would be used as input to a prediction of which class the image belonged to.\n",
    "\n",
    "It turns out that we can do the same thing with `CNN`. We treat the $m$ feature maps as $m\\times\\textit{image}_{height}\\times\\textit{image}_{width}$ neurons and use a `Flatten` operation to squash these three dimensions (the number of channels, the image height, and the image width) down into a one-dimensional vector, after which we can use a simple matrix multiplication to make our final predictions. The intuition for why this works is that each individual neuron fundamentally represents the same \"kind of thing\" as the neurons in a fully connected layer - specifically, whether a given visual feature (or combination of features) is present at a given location in an image - and thus we can treat them the same way in the final layer of the neural network.\n",
    "\n",
    "We’ll see how to implement the `Flatten` layer later in the chapter. But before we dive into the implementation, let’s discuss another kind of layer that is important in many `CNN` architectures, though we won’t cover it in great detail in this book.\n",
    "\n",
    "### Pooling Layers\n",
    "Pooling layers are another kind of layer commonly used in `CNN`. They simply downsample each of the feature maps created by a convolution operation; for the most typically used pooling size of $2$, this involves mapping each $2 \\times 2$ section of each feature map either to the maximum value of that section, in the case of `max-pooling`, or to the average value of that section, in the case of `averagen-pooling`. For an $n\\times n$ image, then, this would map the entire image to one of size $\\frac{n}{2}\\times\\frac{n}{2}$. `Figure 5-5` illustrates this.\n",
    "\n",
    "<img src=\"images/05_05.png\" style=\"width:600px;\"/>\n",
    "\n",
    "The main advantage of pooling is computational: by downsampling the image to contain one-fourth as many pixels as the prior layer, pooling decreases both the number of weights and the number of computations needed to train the network by a factor of 4; this can be further compounded if multiple pooling layers are used in the network, as they were in many architectures in the early days of `CNN`. The downside of pooling, of course, is that only one-fourth as much information can be extracted from the downsampled image. However, the fact that architectures showed very strong performance on benchmarks in image recognition despite the use of pooling suggested that, even though pooling was causing the networks to \"lose information\" about the images by decreasing the images’ resolution, the trade-offs in terms of increased computational speed were worth it. Nevertheless, pooling was considered by many to be a trick that just happened to work but should probably be done away with; as `Geoffrey Hinton` wrote on a Reddit AMA in 2014, \"The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster\". And indeed, most recent `CNN` architectures (such as `ResNet`) use pooling minimally or not at all. Thus, in this book, we’re not going to implement pooling layers, but given their importance for \"putting CNNs on the map\" via their use in famous architectures such as `AlexNet`, we mention them here for completeness.\n",
    "\n",
    "##### Applying CNN beyond images\n",
    "Everything we have described so far is extremely standard for dealing with images using neural networks: the images are typically represented as a set of $m_1$ channels of pixels, where $m_1 = 1$ for black-and-white images, and $m_1 = 3$ for color images—and then some number $m_2$ of convolution operations are applied to each channel (using the $m_1 \\times m_2$ filter maps as explained previously), with this pattern continuing on for several layers. This has all been covered in other treatments of convolutional neural networks; what is less commonly covered is that the idea of organizing data into \"channels\" and then processing that data using a CNN goes beyond just images. For example, this data representation was a key to `DeepMind`’s series of AlphaGo programs showing that neural networks could learn to play `Go`(`DeepMind (David Silver et al.), Mastering the Game of Go Without Human Knowledge, 2017`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Implementing the Multichannel Convolution Operation\n",
    "It turns out that implementing this daunting operation - which involves a four-dimensional input `ndarray` and a four-dimensional parameter `ndarray` - is much clearer if we first examine the one-dimensional case. Building up to the full operation from that starting point will turn out mostly to be a matter of adding a bunch of for loops. \n",
    "\n",
    "### The Forward Pass\n",
    "\n",
    "The convolution in one dimension is conceptually identical to the convolution in two dimensions: we take in a one-dimensional input and a one-dimensional convolutional filter as inputs and then create the output by sliding the filter along the input.\n",
    "\n",
    "Let’s suppose our input is of length 5:\n",
    "\n",
    "$$\\textit{Input} = [t_1, t_2, t_3, t_4, t_5]$$\n",
    "\n",
    "And let’s say the size of the \"patterns\" we want to detect is length 3:\n",
    "\n",
    "$$\\textit{Filter} = [w_1, w_2, w_3]$$\n",
    "\n",
    "The first element of the output would be created by convolving the first element of the input with the filter:\n",
    "\n",
    "$$\\textit{Output Feature 1}: O_1 = t_1w_1+t_2w_2+t_3w_3$$\n",
    "\n",
    "The second element of the output would be created by sliding the filter one unit to the right and convolving it with the next set values of the series:\n",
    "\n",
    "$$\\textit{Output Feature 2}: O_2 = t_2w_1+t_3w_2+t_4w_3$$\n",
    "\n",
    "Fair enough. However, when we compute the next output value, we realize that we have run out of room:\n",
    "\n",
    "$$\\textit{Output Feature 3}: O_3 = t_3w_1+t_4w_2+t_5w_3$$\n",
    "\n",
    "We have hit the end of our input, and the resulting output has just three elements, when we started with five! How can we address this?\n",
    "\n",
    "##### Padding\n",
    "To avoid the output shrinking as a result of the convolution operation, we’ll introduce a trick used throughout convolutional neural networks: we \"pad\" the input with zeros around the edges, enough so that the output remains the same size as the input. Otherwise, every time we convolve a filter over the input, we’ll end up with an output that is slightly smaller than the input, as seen previously.\n",
    "\n",
    "As you can reason from the preceding convolution example: for a filter of size 3, there should be one unit of padding around the edges to keep the output the same size as the input. More generally, since we almost always use odd-numbered filter sizes, we add padding equal to the filter size divided by 2 and rounded down to the nearest integer.\n",
    "\n",
    "Coding up this part turns out to be pretty straightforward. Before we do, let’s summarize the steps we just discussed:\n",
    "1. We ultimately want to produce an output that is the same size as the input.\n",
    "2. To do this without “shrinking” the output, we’ll first need to pad the input.\n",
    "3. Then we’ll have to write some sort of loop that goes through the input and convolves each position of it with the filter.\n",
    "\n",
    "We’ll start with our input and our filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1d = np.array([1,2,3,4,5])\n",
    "param_1d = np.array([1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a helper function that can pad our one-dimensional input on each end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _pad_1d(inp: np.ndarray, num: int) -> np.ndarray:\n",
    "    z = np.array([0])\n",
    "    z = np.repeat(z, num) \n",
    "    return np.concatenate([z, inp, z])\n",
    "\n",
    "_pad_1d(input_1d, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the convolution itself? Observe that for each element in the output that we want to produce, we have a corresponding element in the padded input where we \"start\" the convolution operation; once we figure out where to start, we simply loop through all the elements in the filter, doing a multiplication at each element and adding the result to the total.\n",
    "\n",
    "How do we find this \"corresponding element\"? Note that, simply, the value at the first element in the output gets its value starting at the first element of the padded input! This makes the for loop quite easy to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  6.,  9., 12.,  9.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_1d(inp: np.ndarray, param: np.ndarray) -> np.ndarray:\n",
    "    # assert correct dimensions assert_dim(inp, 1) assert_dim(param, 1)\n",
    "    # pad the input\n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2 \n",
    "    input_pad = _pad_1d(inp, param_mid)\n",
    "    # initialize the output \n",
    "    out = np.zeros(inp.shape)\n",
    "    # perform the 1d convolution \n",
    "    for o in range(out.shape[0]):\n",
    "        for p in range(param_len):\n",
    "            out[o] += param[p] * input_pad[o+p]\n",
    "    # ensure shapes didn't change \n",
    "    assert_same_shape(inp, out)\n",
    "    return out\n",
    "\n",
    "conv_1d(input_1d, param_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s simple enough. Before we move on to the backward pass of this operation, let’s briefly discuss a hyperparameter of convolutions that we’re glossing over: `stride`.\n",
    "\n",
    "##### A note on stride\n",
    "We noted earlier that pooling operations were one way to downsample images from feature maps. In many early convolutional architectures, these did indeed significantly reduce the amount of computation needed without any significant hit to accuracy; nevertheless, they’ve fallen out of favor because of their downside: they effectively downsample the image so that an image with just half the resolution is passed forward into the next layer.\n",
    "\n",
    "A much more widely accepted way to do this is to modify the stride of the convolution operation. The stride is the amount that the filter is incrementally slid over the image. In the previous case, we are using a stride of $1$, and as a result each filter is convolved with every element of the input, which is why the output ends up being the same size as the input. With a stride of $2$, the filter would be convolved with every other element of the input image, so that the output would be half the size of the input; with a stride of $3$, the filter would be convolved with every third element of the input image, and so on. This means that, for example, using a stride of $2$ would result in the same output size and thus much the same reduction in computation we would get from pooling with size $2$, but without as much loss of information: with pooling of size 2, only one-fourth of the elements in the input have any effect on the output, whereas with a stride of $2$, every element of the input has some effect on the output. The use of a stride of greater than $1$ is thus significantly more prevalent than pooling for downsampling even in the most advanced `CNN` architectures of today.\n",
    "\n",
    "Nevertheless, in this book I’ll just show examples with a stride of $1$, and modifying these operations to allow a stride of greater than $1$ is left as an exercise for the reader. Using a stride equal to $1$ also makes writing the backward pass easier.\n",
    "\n",
    "### Convolutions: The Backward Pass\n",
    "The backward pass is where convolutions get a bit trickier. Let’s recall what we’re trying to do: before, we produced the output of a convolution operation using the input and the parameters. We now want to compute:\n",
    "+ The partial derivative of the loss with respect to each element of the input to the convolution operation - `inp` previously\n",
    "+ The partial derivative of the loss with respect to each element of the *filter* - `param_1d` previously\n",
    "\n",
    "Think of how the `ParamOperations` we saw in `Chapter 4` work: in the backward method, they receive an output gradient representing how much each element of the output ultimately affects the loss and then use this output gradient to compute the gradients for the input and the parameters. So we need to write a function that takes in an `output_grad` with the same shape as the input and produces an `input_grad` and a `param_grad`.\n",
    "\n",
    "How can we test whether the computed gradients are correct? We’ll bring back an idea from the first chapter: we know that the partial derivative of a sum with respect to any one of its inputs is $1$ (if the sum $s = a+b+c$, then $\\frac{\\partial S}{\\partial a} = \\frac{\\partial S}{\\partial b} = \\frac{\\partial S}{\\partial c} = 1$). So we can compute the `input_grad` and `param_grad` quantities using our `_input_grad` and `_param_grad` functions (which we’ll reason through and write shortly) and an `output_grad` equal to all $1$. Then we’ll check whether these gradients are correct by changing elements of the input by some quantity $\\alpha$ and seeing whether the resulting sum changes by the gradient times $\\alpha$.\n",
    "\n",
    "##### What \"should\" the gradient be?\n",
    "Using the logic just described, let’s calculate what an element of the gradient vector for the input should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1d_sum(inp: np.ndarray, param: np.ndarray) -> np.ndarray: \n",
    "    out = conv_1d(inp, param) \n",
    "    return np.sum(out)\n",
    "\n",
    "# randomly choose to increase 5th element by 1\n",
    "input_1d = np.array([1,2,3,4,5])\n",
    "input_1d_2 = np.array([1,2,3,4,6])\n",
    "param_1d = np.array([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39.0, 41.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_sum(input_1d, param_1d), conv_1d_sum(input_1d_2, param_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the gradient of the fifth element of the input should be $41 – 39 = 2$.\n",
    "\n",
    "Now let’s try to reason through how we should compute such a gradient without simply computing the difference between these two sums. Here is where things get interesting.\n",
    "\n",
    "##### Computing the gradient of a 1D convolution\n",
    "We see that increasing this element of the input increased the output by $2$. Taking a close look at the output shows exactly how it does this:\n",
    "\n",
    "<img src=\"images/a_05_01.png\" style=\"width:200px;\"/>\n",
    "\n",
    "This particular element of the input is denoted $t_5$. It appears in the output in two places:\n",
    "+ As part of $o_4$, it is multiplied by $w_3$.\n",
    "\n",
    "+ As part of $o_5$, it is multiplied by $w_2$.\n",
    "\n",
    "To help see the general pattern of how inputs map to the sum of outputs, note that if there was an $o_6$ present, $t_5$ would also contribute to the output through being multiplied by $w_1$.\n",
    "\n",
    "Therefore, the amount that $t_5$ ultimately affects the loss, which we can denote as $\\displaystyle\\frac{\\partial L}{\\partial t_5}$ will be:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial t_5} = \\frac{\\partial L}{\\partial o_4}\\times w_3 + \\frac{\\partial L}{\\partial o_5}\\times w_2 + \\frac{\\partial L}{\\partial o_6}\\times w_1$$\n",
    "\n",
    "Of course, in this simple example, when the loss is just the sum, $\\displaystyle\\frac{\\partial L}{\\partial o_i}=1$ for all elements, in the output (except for the \"padding\" elements for which this quantity is 0). This sum is very easy to compute: it is simply $w_2 + w_3$, which is indeed $2$ since $w_2 = w_3 = 1$.\n",
    "\n",
    "##### What’s the general pattern?\n",
    "Now let’s look for the general pattern for a generic input element. This turns out to be an exercise in keeping track of indices. Since we’re translating math into code here, let’s use $o_{i}^{grad}$ to denote the $i$-th element of the output gradient (since we’ll ultimately be accessing it via `output_grad[i]`). Then:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial t_5} = o_{4}^{grad}\\times w_3 + o_{5}^{grad}\\times w_2 + o_{6}^{grad}\\times w_1$$\n",
    "\n",
    "Looking closely at this output, we can reason similarly that:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial t_3} = o_{2}^{grad}\\times w_3 + o_{3}^{grad}\\times w_2 + o_{4}^{grad}\\times w_1$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial t_4} = o_{3}^{grad}\\times w_3 + o_{4}^{grad}\\times w_2 + o_{5}^{grad}\\times w_1$$\n",
    "\n",
    "There’s clearly a pattern here, and translating it into code is a bit tricky, especially since the indices on the output increase at the same time the indices on the weights decrease. Nevertheless, the way to express this turns out to be via the following double for loop:\n",
    "\n",
    "```python\n",
    "# param: in our case an ndarray of shape (1,3) \n",
    "# param_len: the integer 3 \n",
    "# inp: in our case an ndarray of shape (1,5) \n",
    "# input_grad: always an ndarray the same shape as \"inp\" \n",
    "# output_pad: in our case an ndarray of shape (1,7) \n",
    "for o in range(inp.shape[0]):\n",
    "    for p in range(param.shape[0]):\n",
    "        input_grad[o] += output_pad[o+param_len-p-1] * param[p]\n",
    "```\n",
    "\n",
    "This does the appropriate incrementing of the indices of the weights, while decreasing the weights on the output at the same time.\n",
    "\n",
    "Though it may not be obvious now, reasoning through this and getting it is out to be the trickiest part of calculating the gradients for convolution operations. Adding more complexity to this, such as batch sizes, convolutions with two-dimensional inputs, or inputs with multiple channels, is simply a matter of adding more for loops to the preceding lines, as we’ll see in the next few sections.\n",
    "\n",
    "##### Computing the parameter gradient\n",
    "We can reason similarly about how increasing an element of the filter should increase the output. First, let’s increase (arbitrarily) the first element of the filter by one unit and observe the resulting impact on the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39.0, 49.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1d = np.array([1,2,3,4,5])\n",
    "# randomly choose to increase first element by 1\n",
    "param_1d_2 = np.array([2,1,1])\n",
    "\n",
    "conv_1d_sum(input_1d, param_1d), conv_1d_sum(input_1d, param_1d_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should find that $\\displaystyle\\frac{\\partial L}{\\partial w_1}=10$.\n",
    "\n",
    "Just as we did for the input, by closely examining the output and seeing which elements of the filter affect it, as well as padding the input to more clearly see the pattern, we see that:\n",
    "\n",
    "$$w_{1}^{grad} = t_0\\times o_{1}^{grad} + t_1\\times o_{2}^{grad} + t_2\\times o_{3}^{grad} + t_3\\times o_{4}^{grad} + t_4\\times o_{5}^{grad}$$\n",
    "\n",
    "And since, for the sum, all of the $o_{i}^{grad}$ elements are just $1$, and $t_0$ is $0$, we have:\n",
    "\n",
    "$$w_{1}^{grad} = t_1 + t_2 + t_3 + t_4 = 1 + 2 + 3 + 4 = 10$$\n",
    "\n",
    "This confirms the calculation from earlier.\n",
    "\n",
    "Coding this up turns out to be easier than writing the code for the input gradient, since this time \"the indices are moving in the same direction\". Within the same nested for loop, the code is:\n",
    "\n",
    "```python\n",
    "# param: in our case an ndarray of shape (1,3) \n",
    "# param_grad: an ndarray the same shape as param \n",
    "# inp: in our case an ndarray of shape (1,5) \n",
    "# input_pad: an ndarray the same shape as (1,7) \n",
    "# output_grad: in our case an ndarray of shape (1,5) \n",
    "for o in range(inp.shape[0]):\n",
    "    for p in range(param.shape[0]):\n",
    "        param_grad[p] += input_pad[o+p] * output_grad[o]\n",
    "```\n",
    "\n",
    "Finally, we can combine these two computations and write a function to compute both the input gradient and the filter gradient with the following steps:\n",
    "1. Take the input and filter as arguments.\n",
    "\n",
    "2. Compute the output.\n",
    "\n",
    "3. Pad the input and the output gradient (to get, say, `input_pad` and `output_pad`).\n",
    "\n",
    "4. As shown earlier, use the padded output gradient and the filter to compute the gradient.\n",
    "\n",
    "5. Similarly, use the output gradient (not padded) and the padded input to compute the filter gradient.\n",
    "\n",
    "That concludes our explanation of how to implement convolutions in 1D! As we’ll see in the next several sections, extending this reasoning to work on two-dimensional inputs, batches of two-dimensional inputs, or even multichannel batches of two-dimensional inputs is (perhaps surprisingly) straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _param_grad_1d(inp:np.ndarray, param:np.ndarray, output_grad:np.ndarray = None) -> np.ndarray:\n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    input_pad = _pad_1d(inp, param_mid)\n",
    "\n",
    "    if output_grad is None:\n",
    "        output_grad = np.ones_like(inp)\n",
    "    else:\n",
    "        assert_same_shape(inp, output_grad)\n",
    "    # Zero padded 1 dimensional convolution\n",
    "    param_grad = np.zeros_like(param)\n",
    "    input_grad = np.zeros_like(inp)\n",
    "    \n",
    "    for o in range(inp.shape[0]):\n",
    "        for p in range(param.shape[0]):\n",
    "            param_grad[p] += input_pad[o+p] * output_grad[o]\n",
    "    assert_same_shape(param_grad, param)\n",
    "    return param_grad\n",
    "\n",
    "def _input_grad_1d(inp:np.ndarray, param:np.ndarray, output_grad:np.ndarray = None) -> np.ndarray:\n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    inp_pad = _pad_1d(inp, param_mid)\n",
    "    \n",
    "    if output_grad is None:\n",
    "        output_grad = np.ones_like(inp)\n",
    "    else:\n",
    "        assert_same_shape(inp, output_grad)\n",
    "    \n",
    "    output_pad = _pad_1d(output_grad, param_mid)\n",
    "    # Zero padded 1 dimensional convolution\n",
    "    param_grad = np.zeros_like(param)\n",
    "    input_grad = np.zeros_like(inp)\n",
    "\n",
    "    for o in range(inp.shape[0]):\n",
    "        for f in range(param.shape[0]):\n",
    "            input_grad[o] += output_pad[o+param_len-f-1] * param[f]\n",
    "    assert_same_shape(param_grad, param)\n",
    "    return input_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches, 2D Convolutions, and Multiple Channels\n",
    "First, let’s add the capability for these convolution functions to work with batches of inputs - `2D inputs whose first dimension represents the batch size of the input and whose second dimension represents the length of the 1D sequence:\n",
    "\n",
    "```python\n",
    "input_1d_batch = np.array([[0,1,2,3,4,5,6], [1,2,3,4,5,6,7]])\n",
    "```\n",
    "\n",
    "We can follow the same general steps defined before: we’ll first pad the input, use this to compute the output, and then pad the output gradient to compute both the input and filter gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
