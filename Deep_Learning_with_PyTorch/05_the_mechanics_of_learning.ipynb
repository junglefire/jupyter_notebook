{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Pytorch env\n",
    "import os\n",
    "os.environ['TORCH_HOME']=\"/home/alex/data/pytorch\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision as torchv\n",
    "import torch as torch\n",
    "import PIL as pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. The mechanics of learning\n",
    "With the blooming of machine learning that has occurred over the last decade, the notion of machines that learn from experience has become a mainstream theme in both technical and journalistic circles. Now, how is it exactly that a machine learns? What are the mechanics of this process—or, in words, what is the algorithm behind it? From the point of view of an observer, a learning algorithm is presented with input data that is paired with desired outputs. Once learning has occurred, that algorithm will be capable of producing correct outputs when it is fed new data that is similar enough to the input data it was trained on. With deep learning, this process works even when the input data and the desired output are far from each other: when they come from different domains, like an image and a sentence describing it, as we saw in `chapter 2`.\n",
    "\n",
    "## 5.1 A timeless lesson in modeling\n",
    "Building models that allow us to explain input/output relationships dates back centuries at least. When Johannes Kepler, a German mathematical astronomer, figured out his three laws of planetary motion in the early 1600s, he based them on data collected by his mentor Tycho Brahe during naked-eye observations. Not having Newton’s law of gravitation at his disposal (actually, Newton used Kepler’s work to figure things out), Kepler extrapolated the simplest possible geometric model that could fit the data. And, by the way, it took him six years of staring at data that didn’t make sense to him, together with incremental realizations, to finally formulate these laws. We can see this process in `figure 5.1`.\n",
    "\n",
    "<img src=\"images/05_01.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Kepler’s first law reads: \"The orbit of every planet is an ellipse with the Sun at one of the two foci\". He didn’t know what caused orbits to be ellipses, but given a set of observations for a planet, he could estimate the shape and size of the ellipse. With those two parameters computed from the data, he could tell where the planet might be during its journey in the sky. Once he figured out the second law—\"A line joining a planet and the Sun sweeps out equal areas during equal intervals of time\"—he could also tell when a planet would be at a particular point in space, given observations in time.\n",
    "\n",
    "So, how did Kepler estimate the eccentricity and size of the ellipse without computers, pocket calculators, or even calculus, none of which had been invented yet? Essentially, Kepler had to try different shapes, using a certain number of observations to find the curve, then use the curve to find some more positions, for times when he had observations available, and then check whether these calculated positions agreed with the observed ones.\n",
    "\n",
    "So let’s sum things up. Over six years, Kepler\n",
    "1. Got lots of good data from his friend Brahe \n",
    "\n",
    "2. Tried to visualize the heck out of it, because he felt there was something fishy going on \n",
    "\n",
    "3. Chose the simplest possible model that had a chance to fit the data (an ellipse) \n",
    "\n",
    "4. Split the data so that he could work on part of it and keep an independent set for validation \n",
    "\n",
    "5. Started with a tentative eccentricity and size for the ellipse and iterated until the model fit the observations \n",
    "\n",
    "6. Validated his model on the independent observations \n",
    "\n",
    "7. Looked back in disbelief\n",
    "\n",
    "There’s a data science handbook for you, all the way from 1609. The history of science is literally constructed on these seven steps. And we have learned over the centuries that deviating from them is a recipe for disaster.\n",
    "\n",
    "This is exactly what we will set out to do in order to learn something from data. In fact, in this book there is virtually no difference between saying that we’ll fit the data or that we’ll make an algorithm learn from data. The process always involves a function with a number of unknown parameters whose values are estimated from data: in short, a model.\n",
    "\n",
    "In this book, we’re interested in models that are not engineered for solving a specific narrow task, but that can be automatically adapted to specialize themselves for any one of many similar tasks using input and output pairs—in other words, general models trained on data relevant to the specific task at hand. In particular, `PyTorch` is designed to make it easy to create models for which the derivatives of the fitting error, with respect to the parameters, can be expressed analytically. No worries if this last sentence didn’t make any sense at all; coming next, we have a full section that hopefully clears it up for you.\n",
    "\n",
    "This chapter is about how to automate generic function-fitting. After all, this is what we do with deep learning—deep neural networks being the generic functions we’re talking about—and PyTorch makes this process as simple and transparent as possible. In order to make sure we get the key concepts right, we’ll start with a model that is a lot simpler than a deep neural network. This will allow us to understand the mechanics of learning algorithms from first principles in this chapter, so we can move to more complicated models in `chapter 6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Learning is just parameter estimation\n",
    "In this section, we’ll learn how we can take data, choose a model, and estimate the parameters of the model so that it will give good predictions on new data. To do so, we’ll leave the intricacies of planetary motion and divert our attention to the second-hardest problem in physics: calibrating instruments.\n",
    "\n",
    "`Figure 5.2` shows the high-level overview of what we’ll implement by the end of the chapter. Given input data and the corresponding desired outputs (ground truth), as well as initial values for the weights, the model is fed input data (`forward pass`), and a measure of the error is evaluated by comparing the resulting outputs to the ground truth. In order to optimize the parameter of the model—its weights—the change in the error following a unit change in weights (that is, the gradient of the error with respect to the parameters) is computed using the chain rule for the derivative of a composite function (`backward pass`). The value of the weights is then updated in the direction that leads to a decrease in the error. The procedure is repeated until the error, evaluated on unseen data, falls below an acceptable level. If what we just said sounds obscure, we’ve got a whole chapter to clear things up. By the time we’re done, all the pieces will fall into place, and this paragraph will make perfect sense.\n",
    "\n",
    "We’re now going to take a problem with a noisy dataset, build a model, and implement a learning algorithm for it. When we start, we’ll be doing everything by hand, but by the end of the chapter we’ll be letting `PyTorch` do all the heavy lifting for us. When we finish the chapter, we will have covered many of the essential concepts that underlie training deep neural networks, even if our motivating example is very simple and our model isn’t actually a neural network.\n",
    "\n",
    "<img src=\"images/05_02.png\" style=\"width:600px;\"/>\n",
    "\n",
    "### 5.2.1 A hot problem\n",
    "We just got back from a trip to some obscure location, and we brought back a fancy, wall-mounted analog thermometer. It looks great, and it’s a perfect fit for our living room. Its only flaw is that it doesn’t show units. Not to worry, we’ve got a plan: we’ll build a dataset of readings and corresponding temperature values in our favorite units, choose a model, adjust its weights iteratively until a measure of the error is low enough, and finally be able to interpret the new readings in units we understand.\n",
    "\n",
    "### 5.2.2 Gathering some data\n",
    "We’ll start by making a note of temperature data in good old Celsius and measurements from our new thermometer, and figure things out. After a couple of weeks, here’s the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c) \n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the $t\\_c$ values are temperatures in Celsius, and the $t\\_u$ values are our unknown units. We can expect noise in both measurements, coming from the devices themselves and from our approximate readings. For convenience, we’ve already put the data into tensors; we’ll use it in a minute.\n",
    "\n",
    "### 5.2.3 Visualizing the data\n",
    "A quick plot of our data in `figure 5.3` tells us that it’s noisy, but we think there’s a pattern here.\n",
    "\n",
    "<img src=\"images/05_03.png\" style=\"width:600px;\"/>\n",
    "\n",
    "### 5.2.4 Choosing a linear model as a first try\n",
    "In the absence of further knowledge, we assume the simplest possible model for converting between the two sets of measurements, just like Kepler might have done. The two may be linearly related—that is, multiplying $t\\_u$ by a factor and adding a constant, we may get the temperature in Celsius (up to an error that we omit):\n",
    "\n",
    "$$t\\_c = w * t\\_u + b$$\n",
    "\n",
    "Is this a reasonable assumption? Probably; we’ll see how well the final model performs. We chose to name $w$ and $b$ after weight and bias, two very common terms for linear scaling and the additive constant—we’ll bump into those all the time.\n",
    "\n",
    "OK, now we need to estimate $w$ and $b$, the parameters in our model, based on the data we have. We must do it so that temperatures we obtain from running the unknown temperatures $t\\_u$ through the model are close to temperatures we actually measured in Celsius. If that sounds like fitting a line through a set of measurements, well, yes, because that’s exactly what we’re doing. We’ll go through this simple example using `PyTorch` and realize that training a neural network will essentially involve changing the model for a slightly more elaborate one, with a few (or a metric ton) more parameters.\n",
    "\n",
    "Let’s flesh it out again: we have a model with some unknown parameters, and we need to estimate those parameters so that the error between predicted outputs and measured values is as low as possible. We notice that we still need to exactly define a measure of the error. Such a measure, which we refer to as the *loss function*, should be high if the error is high and should ideally be as low as possible for a perfect match. Our optimization process should therefore aim at finding $w$ and $b$ so that the loss function is at a minimum.\n",
    "\n",
    "## 5.3 Less loss is what we want\n",
    "A `loss function` (or `cost function`) is a function that computes a single numerical value that the learning process will attempt to minimize. The calculation of loss typically involves taking the difference between the desired outputs for some training samples and the outputs actually produced by the model when fed those samples. In our case, that would be the difference between the predicted temperatures $t\\_p$ output by our model and the actual measurements: $t\\_p – t\\_c$.\n",
    "\n",
    "We need to make sure the loss function makes the loss positive both when $t\\_p$ is greater than and when it is less than the true $t\\_c$, since the goal is for $t\\_p$ to match $t\\_c$. We have a few choices, the most straightforward being $|t\\_p – t\\_c|$ and $(t\\_p – t\\_c)^2$. Based on the mathematical expression we choose, we can emphasize or discount certain errors. Conceptually, a loss function is a way of prioritizing which errors to fix from our training samples, so that our parameter updates result in adjustments to the outputs for the highly weighted samples instead of changes to some other samples’ output that had a smaller loss.\n",
    "\n",
    "Both of the example loss functions have a clear minimum at zero and grow monotonically as the predicted value moves further from the true value in either direction. Because the steepness of the growth also monotonically increases away from the minimum, both of them are said to be *convex*. Since our model is linear, the loss as a function of $w$ and $b$ is also convex. Cases where the loss is a convex function of the model parameters are usually great to deal with because we can find a minimum very efficiently through specialized algorithms. However, we will instead use less powerful but more generally applicable methods in this chapter. We do so because for the deep neural networks we are ultimately interested in, the loss is not a convex function of the inputs.\n",
    "\n",
    "For our two loss functions $|t\\_p – t\\_c|$ and $(t\\_p – t\\_c)^2$, as shown in `figure 5.4`, we notice that the square of the differences behaves more nicely around the minimum: the derivative of the error-squared loss with respect to $t\\_p$ is zero when $t\\_p$ equals $t\\_c$. The absolute value, on the other hand, has an undefined derivative right where we’d like to converge. This is less of an issue in practice than it looks like, but we’ll stick to the square of differences for the time being.\n",
    "\n",
    "<img src=\"images/05_04.png\" style=\"width:500px;\"/>\n",
    "\n",
    "It’s worth noting that the square difference also penalizes wildly wrong results more than the absolute difference does. Often, having more slightly wrong results is better than having a few wildly wrong ones, and the squared difference helps prioritize those as desired.\n",
    "\n",
    "### 5.3.1 From problem back to PyTorch\n",
    "We’ve figured out the model and the loss function—we’ve already got a good part of the high-level picture in `figure 5.2` figured out. Now we need to set the learning process in motion and feed it actual data. Also, enough with math notation; let’s switch to `PyTorch`—after all, we came here for the fun.\n",
    "\n",
    "We’ve already created our data tensors, so now let’s write out the model as a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re expecting $t\\_u$, $w$, and $b$ to be the input tensor, weight parameter, and bias parameter, respectively. In our model, the parameters will be `PyTorch` scalars (aka zero-dimensional tensors), and the product operation will use broadcasting to yield the returned tensors. Anyway, time to define our loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are building a tensor of differences, taking their square element-wise, and finally producing a scalar loss function by averaging all of the elements in the resulting tensor. It is a *mean square loss*.\n",
    "\n",
    "We can now initialize the parameters, invoke the model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]),\n",
       " torch.Size([]),\n",
       " tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "         48.4000, 60.4000, 68.4000]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(()) \n",
    "b = torch.zeros(())\n",
    "t_p = model(t_u, w, b) \n",
    "w.shape, b.shape, t_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and check the value of the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8846)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c) \n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the model and the loss in this section. We’ve finally reached the meat of the example: how do we estimate $w$ and $b$ such that the loss reaches a minimum? We’ll first work things out by hand and then learn how to use `PyTorch`’s superpowers to solve the same problem in a more general, off-the-shelf way.\n",
    "\n",
    "##### Broadcasting\n",
    "We mentioned broadcasting in `chapter 3`, and we promised to look at it more carefully when we need it. In our example, we have two scalars (zero-dimensional tensors) $w$ and $b$, and we multiply them with and add them to vectors (one-dimensional tensors) of length $b$.\n",
    "\n",
    "Usually—and in early versions of `PyTorch`, too—we can only use element-wise binary operations such as addition, subtraction, multiplication, and division for arguments of the same shape. The entries in matching positions in each of the tensors will be used to calculate the corresponding entry in the result tensor.\n",
    "\n",
    "Broadcasting, which is popular in `NumPy` and adapted by `PyTorch`, relaxes this assumption for most binary operations. It uses the following rules to match tensor elements:\n",
    "+ For each index dimension, counted from the back, if one of the operands is size 1 in that dimension, `PyTorch` will use the single entry along this dimension with each of the entries in the other tensor along this dimension.\n",
    "\n",
    "+ If both sizes are greater than 1, they must be the same, and natural matching is used.\n",
    "\n",
    "+ If one of the tensors has more index dimensions than the other, the entirety of the other tensor will be used for each entry along these dimensions.\n",
    "\n",
    "This sounds complicated (and it can be error-prone if we don’t pay close attention, which is why we have named the tensor dimensions as shown in `section 3.4`), but usually, we can either write down the tensor dimensions to see what happens or picture what happens by using space dimensions to show the broadcasting, as in the following figure.\n",
    "\n",
    "Of course, this would all be theory if we didn’t have some code examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: x: torch.Size([]), y: torch.Size([3, 1])\n",
      "        z: torch.Size([1, 3]), a: torch.Size([2, 1, 1])\n",
      "x * y: torch.Size([3, 1])\n",
      "y * z: torch.Size([3, 3])\n",
      "y * z * a: torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(())\n",
    "y = torch.ones(3,1)\n",
    "z = torch.ones(1,3)\n",
    "a = torch.ones(2, 1, 1) \n",
    "print(f\"shapes: x: {x.shape}, y: {y.shape}\")\n",
    "print(f\"        z: {z.shape}, a: {a.shape}\") \n",
    "print(\"x * y:\", (x * y).shape) \n",
    "print(\"y * z:\", (y * z).shape) \n",
    "print(\"y * z * a:\", (y * z * a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 1]),\n",
       " tensor([[[2]],\n",
       " \n",
       "         [[3]]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.ones(3,3)\n",
    "a = torch.arange(2)+2\n",
    "a = a.view(2,1,1)\n",
    "a.shape, a, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[3., 3., 3.],\n",
       "         [3., 3., 3.],\n",
       "         [3., 3., 3.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z*a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Down along the gradient\n",
    "We’ll optimize the loss function with respect to the parameters using the gradient descent algorithm. In this section, we’ll build our intuition for how gradient descent works from first principles, which will help us a lot in the future. As we mentioned, there are ways to solve our example problem more efficiently, but those approaches aren’t applicable to most deep learning tasks. Gradient descent is actually a very simple idea, and it scales up surprisingly well to large neural network models with millions of parameters.\n",
    "\n",
    "Let’s start with a mental image, which we conveniently sketched out in `figure 5.5`. Suppose we are in front of a machine sporting two knobs, labeled $w$ and $b$. We are allowed to see the value of the loss on a screen, and we are told to minimize that value. Not knowing the effect of the knobs on the loss, we start fiddling with them and decide for each knob which direction makes the loss decrease. We decide to rotate both knobs in their direction of decreasing loss. Suppose we’re far from the optimal value: we’d likely see the loss decrease quickly and then slow down as it gets closer to the minimum. We notice that at some point, the loss climbs back up again, so we invert the direction of rotation for one or both knobs. We also learn that when the loss changes slowly, it’s a good idea to adjust the knobs more finely, to avoid reaching the point where the loss goes back up. After a while, eventually, we converge to a minimum.\n",
    "\n",
    "<img src=\"images/05_05.png\" style=\"width:250px;\"/>\n",
    "\n",
    "### 5.4.1 Decreasing loss\n",
    "Gradient descent is not that different from the scenario we just described. The idea is to compute the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss. Just like when we were fiddling with the knobs, we can estimate the rate of change by adding a small number to $w$ and $b$ and seeing how much the loss changes in that neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "loss_rate_of_change_w = \\\n",
    "    (loss_fn(model(t_u, w+delta, b), t_c)-loss_fn(model(t_u, w-delta, b), t_c)) / (2.0*delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is saying that in the neighborhood of the current values of $w$ and $b$, a unit increase in $w$ leads to some change in the loss. If the change is negative, then we need to increase $w$ to minimize the loss, whereas if the change is positive, we need to decrease $w$. By how much? Applying a change to $w$ that is proportional to the rate of change of the loss is a good idea, especially when the loss has several parameters: we apply a change to those that exert a significant change on the loss. It is also wise to change the parameters slowly in general, because the rate of change could be dramatically different at a distance from the neighborhood of the current $w$ value. Therefore, we typically should scale the rate of change by a small factor. This scaling factor has many names; the one we use in machine learning is `learning_rate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "w = w - learning_rate * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = \\\n",
    "    (loss_fn(model(t_u, w, b+delta), t_c)-loss_fn(model(t_u, w, b-delta), t_c)) / (2.0*delta)\n",
    "\n",
    "b = b - learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents the basic parameter-update step for gradient descent. By reiterating these evaluations (and provided we choose a small enough learning rate), we will converge to an optimal value of the parameters for which the loss computed on the given data is minimal. We’ll show the complete iterative process soon, but the way we just computed our rates of change is rather crude and needs an upgrade before we move on. Let’s see why and how.\n",
    "\n",
    "### 5.4.2 Getting analytical\n",
    "Computing the rate of change by using repeated evaluations of the model and loss in order to probe the behavior of the loss function in the neighborhood of $w$ and $b$ doesn’t scale well to models with many parameters. Also, it is not always clear how large the neighborhood should be. We chose `delta` equal to $0.1$ in the previous section, but it all depends on the shape of the loss as a function of $w$ and $b$. If the loss changes too quickly compared to `delta`, we won’t have a very good idea of in which direction the loss is decreasing the most.\n",
    "\n",
    "What if we could make the neighborhood infinitesimally small, as in `figure 5.6`? That’s exactly what happens when we analytically take the derivative of the loss with respect to a parameter. In a model with two or more parameters like the one we’re dealing with, we compute the individual derivatives of the loss with respect to each parameter and put them in a vector of derivatives: the *gradient*.\n",
    "\n",
    "<img src=\"images/05_06.png\" style=\"width:500px;\"/>\n",
    "\n",
    "##### COMPUTING THE DERIVATIVES\n",
    "In order to compute the derivative of the loss with respect to a parameter, we can apply the chain rule and compute the derivative of the loss with respect to its input (which is the output of the model), times the derivative of the model with respect to the parameter:\n",
    "\n",
    "$$\\frac{\\partial{\\textit{loss_fn}}}{\\partial w}  = \\frac{\\partial{\\textit{loss_fn}}}{\\partial t\\_p} * \\frac{\\partial{t\\_p}}{\\partial w}$$\n",
    "\n",
    "Recall that our model is a linear function, and our loss is a sum of squares. Let’s figure out the expressions for the derivatives. Recalling the expression for the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering that $\\frac{\\partial x^2}{\\partial x} = 2x$, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    # The division is from the derivative of mean\n",
    "    dsq_diffs = 2 *(t_p - t_c)/t_p.size(0) \n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### APPLYING THE DERIVATIVES TO THE MODEL\n",
    "For the model, recalling that our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get these derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DEFINING THE GRADIENT FUNCTION\n",
    "Putting all of this together, the function returning the gradient of the loss with respect to $w$ and $b$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    # The summation is the reverse of the broadcasting we implicitly do when \n",
    "    # applying the parameters to an entire vector of inputs in the model.\n",
    "    dloss_dtp = dloss_fn(t_p, t_c) \n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b) \n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b) \n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same idea expressed in mathematical notation is shown in `figure 5.7`. Again, we’re averaging (that is, summing and dividing by a constant) over all the data points to get a single scalar quantity for each partial derivative of the loss.\n",
    "\n",
    "<img src=\"images/05_07.png\" style=\"width:600px;\"/>\n",
    "\n",
    "### 5.4.3 Iterating to fit the model\n",
    "We now have everything in place to optimize our parameters. Starting from a tentative value for a parameter, we can iteratively apply updates to it for a fixed number of iterations, or until $w$ and $b$ stop changing. There are several stopping criteria; for now, we’ll stick to a fixed number of iterations.\n",
    "\n",
    "##### THE TRAINING LOOP\n",
    "Since we’re at it, let’s introduce another piece of terminology. We call a training iteration during which we update the parameters for all of our training samples an `epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c, print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, w, b)  # <1>\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
    "        params = params - learning_rate * grad\n",
    "        if epoch in {1, 2, 3, 4, 5, 10, 11, 99, 100, 4000, 5000}:  # <3>\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            if print_params:\n",
    "                print('    Params:', params)\n",
    "                print('    Grad:  ', grad)\n",
    "        if epoch in {4, 12, 101}:\n",
    "            print('...')\n",
    "        if not torch.isfinite(loss).all():\n",
    "            print('loss is inf, abort')\n",
    "            break  # <3>\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual logging logic used for the output in this text is more complicated (see `cell 15` in the same [notebook](http://mng.bz/pBB8)), but the differences are unimportant for understanding the core concepts in this chapter.\n",
    "\n",
    "Now, let’s invoke our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884644\n",
      "    Params: tensor([-44.1730,  -0.8260])\n",
      "    Grad:   tensor([4517.2969,   82.6000])\n",
      "Epoch 2, Loss 5802485.500000\n",
      "    Params: tensor([2568.4014,   45.1637])\n",
      "    Grad:   tensor([-261257.4219,   -4598.9712])\n",
      "Epoch 3, Loss 19408035840.000000\n",
      "    Params: tensor([-148527.7344,   -2616.3933])\n",
      "    Grad:   tensor([15109614.0000,   266155.7188])\n",
      "Epoch 4, Loss 64915909902336.000000\n",
      "    Params: tensor([8589999.0000,  151310.8594])\n",
      "    Grad:   tensor([-8.7385e+08, -1.5393e+07])\n",
      "...\n",
      "Epoch 5, Loss 217130559820791808.000000\n",
      "    Params: tensor([-4.9680e+08, -8.7510e+06])\n",
      "    Grad:   tensor([5.0539e+10, 8.9023e+08])\n",
      "Epoch 10, Loss 90901154706620645225508955521810432.000000\n",
      "    Params: tensor([3.2144e+17, 5.6621e+15])\n",
      "    Grad:   tensor([-3.2700e+19, -5.7600e+17])\n",
      "Epoch 11, Loss inf\n",
      "    Params: tensor([-1.8590e+19, -3.2746e+17])\n",
      "    Grad:   tensor([1.8912e+21, 3.3313e+19])\n",
      "loss is inf, abort\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.8590e+19, -3.2746e+17])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100, \n",
    "    learning_rate = 1e-2, params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u, t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OVERTRAINING\n",
    "Wait, what happened? Our training process literally blew up, leading to losses becoming `inf`. This is a clear sign that params is receiving updates that are too large, and their values start oscillating back and forth as each update overshoots and the next overcorrects even more. The optimization process is unstable: it diverges instead of converging to a minimum. We want to see smaller and smaller updates to params, not larger, as shown in `figure 5.8`.\n",
    "\n",
    "<img src=\"images/05_08.png\" style=\"width:550px;\"/>\n",
    "\n",
    "How can we limit the magnitude of $\\textit{learning_rate} * \\textit{grad}$? Well, that looks easy. We could simply choose a smaller `learning_rate`, and indeed, the learning rate is one of the things we typically change when training does not go as well as we would like. We usually change learning rates by orders of magnitude, so we might try with $1e^{-3}$ or $1e^{-4}$, which would decrease the magnitude of the updates by orders of magnitude. Let’s go with $1e^{-4}$ and see how it works out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884644\n",
      "    Params: tensor([ 0.5483, -0.0083])\n",
      "    Grad:   tensor([4517.2969,   82.6000])\n",
      "Epoch 2, Loss 323.090546\n",
      "    Params: tensor([ 0.3623, -0.0118])\n",
      "    Grad:   tensor([1859.5493,   35.7843])\n",
      "Epoch 3, Loss 78.929634\n",
      "    Params: tensor([ 0.2858, -0.0135])\n",
      "    Grad:   tensor([765.4667,  16.5122])\n",
      "Epoch 4, Loss 37.552845\n",
      "    Params: tensor([ 0.2543, -0.0143])\n",
      "    Grad:   tensor([315.0790,   8.5787])\n",
      "...\n",
      "Epoch 5, Loss 30.540285\n",
      "    Params: tensor([ 0.2413, -0.0149])\n",
      "    Grad:   tensor([129.6733,   5.3127])\n",
      "Epoch 10, Loss 29.105242\n",
      "    Params: tensor([ 0.2324, -0.0166])\n",
      "    Grad:   tensor([1.4803, 3.0544])\n",
      "Epoch 11, Loss 29.104168\n",
      "    Params: tensor([ 0.2323, -0.0169])\n",
      "    Grad:   tensor([0.5781, 3.0384])\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2323, -0.0196])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 20, #100, \n",
    "    learning_rate = 1e-4, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_u, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice—the behavior is now stable. But there’s another problem: the updates to parameters are very small, so the loss decreases very slowly and eventually stalls. We could obviate this issue by making `learning_rate` adaptive: that is, change according to the magnitude of updates. There are optimization schemes that do that, and we’ll see one toward the end of this chapter, in `section 5.5.2`.\n",
    "\n",
    "However, there’s another potential troublemaker in the update term: the gradient itself. Let’s go back and look at grad at `epoch 1` during optimization.\n",
    "\n",
    "### 5.4.4 Normalizing inputs\n",
    "We can see that the first-epoch gradient for the weight is about $50$ times larger than the gradient for the bias. This means the weight and bias live in differently scaled spaces. If this is the case, a learning rate that’s large enough to meaningfully update one will be so large as to be unstable for the other; and a rate that’s appropriate for the other won’t be large enough to meaningfully change the first. That means we’re not going to be able to update our parameters unless we change something about our formulation of the problem. We could have individual learning rates for each parameter, but for models with many parameters, this would be too much to bother with; it’s babysitting of the kind we don’t like.\n",
    "\n",
    "There’s a simpler way to keep things in check: changing the inputs so that the gradients aren’t quite so different. We can make sure the range of the input doesn’t get too far from the range of $–1.0 \\sim 1.0$, roughly speaking. In our case, we can achieve something close enough to that by simply multiplying $t\\_u$ by $0.1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can run the training loop on our normalized input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "    Params: tensor([1.7761, 0.1064])\n",
      "    Grad:   tensor([-77.6140, -10.6400])\n",
      "Epoch 2, Loss 37.574917\n",
      "    Params: tensor([2.0848, 0.1303])\n",
      "    Grad:   tensor([-30.8623,  -2.3864])\n",
      "Epoch 3, Loss 30.871077\n",
      "    Params: tensor([2.2094, 0.1217])\n",
      "    Grad:   tensor([-12.4631,   0.8587])\n",
      "Epoch 4, Loss 29.756193\n",
      "    Params: tensor([2.2616, 0.1004])\n",
      "    Grad:   tensor([-5.2218,  2.1327])\n",
      "...\n",
      "Epoch 5, Loss 29.507149\n",
      "    Params: tensor([2.2853, 0.0740])\n",
      "    Grad:   tensor([-2.3715,  2.6310])\n",
      "Epoch 10, Loss 29.030487\n",
      "    Params: tensor([ 2.3232, -0.0710])\n",
      "    Grad:   tensor([-0.5355,  2.9295])\n",
      "Epoch 11, Loss 28.941875\n",
      "    Params: tensor([ 2.3284, -0.1003])\n",
      "    Grad:   tensor([-0.5240,  2.9264])\n",
      "...\n",
      "Epoch 99, Loss 22.214186\n",
      "    Params: tensor([ 2.7508, -2.4910])\n",
      "    Grad:   tensor([-0.4453,  2.5208])\n",
      "Epoch 100, Loss 22.148710\n",
      "    Params: tensor([ 2.7553, -2.5162])\n",
      "    Grad:   tensor([-0.4446,  2.5165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0]), \n",
    "    t_u = t_un, # <1>\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we set our learning rate back to $1e^{-2}$, parameters don’t blow up during iterative updates. Let’s take a look at the gradients: they’re of similar magnitude, so using a single `learning_rate` for both parameters works just fine. We could probably do a better job of normalization than a simple rescaling by a factor of $10$, but since doing so is good enough for our needs, we’re going to stick with that for now.\n",
    "\n",
    "> **NOTE**\n",
    "> \n",
    "> The normalization here absolutely helps get the network trained, but you could make an argument that it’s not strictly needed to optimize the parameters for this particular problem. That’s absolutely true! This problem is small enough that there are numerous ways to beat the parameters into submission. However, for larger, more sophisticated problems, normalization is an easy and effective (if not crucial!) tool to use to improve model convergence.\n",
    "\n",
    "Let’s run the loop for enough iterations to see the changes in params get small. We’ll change `n_epochs` to $5000$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "Epoch 2, Loss 37.574917\n",
      "Epoch 3, Loss 30.871077\n",
      "Epoch 4, Loss 29.756193\n",
      "...\n",
      "Epoch 5, Loss 29.507149\n",
      "Epoch 10, Loss 29.030487\n",
      "Epoch 11, Loss 28.941875\n",
      "...\n",
      "Epoch 99, Loss 22.214186\n",
      "Epoch 100, Loss 22.148710\n",
      "...\n",
      "Epoch 4000, Loss 2.927680\n",
      "Epoch 5000, Loss 2.927648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000, \n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_un, \n",
    "    t_c = t_c, \n",
    "    print_params = False)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good: our loss decreases while we change parameters along the direction of gradient descent. It doesn’t go exactly to zero; this could mean there aren’t enough iterations to converge to zero, or that the data points don’t sit exactly on a line. As we anticipated, our measurements were not perfectly accurate, or there was noise involved in the reading.\n",
    "\n",
    "But look: the values for $w$ and $b$ look an awful lot like the numbers we need to use to convert Celsius to Fahrenheit (after accounting for our earlier normalization when we multiplied our inputs by $0.1$). The exact values would be $w=5.5556$ and $b=17.7778$. Our fancy thermometer was showing temperatures in Fahrenheit the whole time. No big discovery, except that our gradient descent optimization process works!\n",
    "\n",
    "### 5.4.5 Visualizing (again)\n",
    "Let’s revisit something we did right at the start: plotting our data. Seriously, this is the first thing anyone doing data science should do. Always plot the heck out of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1431ffc278>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remember that we’re training on the normalized unknown units. \n",
    "# We also use argument unpacking.\n",
    "t_p = model(t_un, *params)\n",
    "\n",
    "# The plot of our linear-fit model (solid line) vs. our input data (circles)\n",
    "fig = plt.figure(figsize=(9,6)) \n",
    "plt.xlabel(\"Temperature (°Fahrenheit)\") \n",
    "plt.ylabel(\"Temperature (°Celsius)\") \n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy()) \n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a Python trick called *argument unpacking* here: `*params` means to pass the elements of params as individual arguments. In Python, this is usually done with lists or tuples, but we can also use argument unpacking with `PyTorch` tensors, which are split along the leading dimension. So here, `model(t_un, *params)` is equivalent to `model(t_un, params[0], params[1])`.\n",
    "\n",
    "This code produces `figure 5.9`. Our linear model is a good model for the data, it seems. It also seems our measurements are somewhat erratic. We should either call our optometrist for a new pair of glasses or think about returning our fancy thermometer.\n",
    "\n",
    "\n",
    "## 5.5 PyTorch’s autograd: Backpropagating all things\n",
    "In our little adventure, we just saw a simple example of *backpropagation*: we computed the gradient of a composition of functions—the model and the loss—with respect to their innermost parameters ($w$ and $b$) by propagating derivatives backward using the chain rule. The basic requirement here is that all functions we’re dealing with can be differentiated analytically. If this is the case, we can compute the gradient—what we earlier called \"the rate of change of the loss\"—with respect to the parameters in one sweep.\n",
    "\n",
    "Even if we have a complicated model with millions of parameters, as long as our model is differentiable, computing the gradient of the loss with respect to the parameters amounts to writing the analytical expression for the derivatives and evaluating them once. Granted, writing the analytical expression for the derivatives of a very deep composition of linear and nonlinear functions is not a lot of fun. It isn’t particularly quick, either.\n",
    "\n",
    "### 5.5.1 Computing the gradient automatically\n",
    "This is when `PyTorch` tensors come to the rescue, with a `PyTorch` component called `autograd`. `Chapter 3` presented a comprehensive overview of what tensors are and what functions we can call on them. We left out one very interesting aspect, however: `PyTorch` tensors can remember where they come from, in terms of the operations and parent tensors that originated them, and they can automatically provide the chain of derivatives of such operations with respect to their inputs. This means we won’t need to derive our model by hand; given a forward expression, no matter how nested, `PyTorch` will automatically provide the gradient of that expression with respect to its input parameters.\n",
    "\n",
    "##### APPLYING AUTOGRAD\n",
    "At this point, the best way to proceed is to rewrite our thermometer calibration code, this time using `autograd`, and see what happens. First, we recall our model and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s again initialize a parameters tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USING THE GRAD ATTRIBUTE\n",
    "Notice the `requires_grad=True` argument to the tensor constructor? That argument is telling `PyTorch` to track the entire family tree of tensors resulting from operations on `params`. In other words, any tensor that will have `params` as an ancestor will have access to the chain of functions that were called to get from params to that tensor. In case these functions are differentiable (and most `PyTorch` tensor operations will be), the value of the derivative will be automatically populated as a `grad` attribute of the `params` tensor.\n",
    "\n",
    "In general, all `PyTorch` tensors have an attribute named `grad`. Normally, it’s `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have to do to populate it is to start with a tensor with `requires_grad` set to `True`, then call the model and compute the loss, and then call `backward` on the `loss` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c) \n",
    "loss.backward()\n",
    "\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the `grad` attribute of `params` contains the derivatives of the loss with respect to each element of params.\n",
    "\n",
    "When we compute our loss while the parameters $w$ and $b$ require gradients, in addition to performing the actual computation, `PyTorch` creates the autograd graph with the operations (in black circles) as nodes, as shown in the top row of `figure 5.10`. When we call `loss.backward()`, `PyTorch` traverses this graph in the reverse direction to compute the gradients, as shown by the arrows in the bottom row of the figure.\n",
    "\n",
    "<img src=\"images/05_10.png\" style=\"width:600px;\"/>\n",
    "\n",
    "##### ACCUMULATING GRAD FUNCTIONS\n",
    "We could have any number of tensors with `requires_grad` set to `True` and any composition of functions. In this case, `PyTorch` would compute the derivatives of the loss throughout the chain of functions (the computation graph) and accumulate their values in the `grad` attribute of those tensors (the leaf nodes of the graph).\n",
    "\n",
    "Alert! ***Big gotcha ahead***. This is something `PyTorch` newcomers—and a lot of more experienced folks, too—trip up on regularly. We just wrote accumulate, not store.\n",
    "\n",
    "> **WARNING**\n",
    "> \n",
    "> Calling `backward` will lead derivatives to *accumulate* at leaf nodes. We need to zero the gradient explicitly after using it for parameter updates.\n",
    "\n",
    "Let’s repeat together: calling `backward` will lead derivatives to accumulate at leaf nodes. So if `backward` was called earlier, the loss is evaluated again, `backward` is called again (as in any training loop), and the gradient at each leaf is accumulated (that is, summed) on top of the one computed at the previous iteration, which leads to an incorrect value for the gradient.\n",
    "\n",
    "In order to prevent this from occurring, we need to *zero the gradient explicitly* at each iteration. We can do this easily using the in-place `zero_` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    "> \n",
    "> You might be curious why zeroing the gradient is a required step instead of zeroing happening automatically whenever we call `backward`. Doing it this way provides more flexibility and control when working with gradients in complicated models.\n",
    "\n",
    "Having this reminder drilled into our heads, let’s see what our autograd-enabled training code looks like, start to finish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_autograd(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:  # <1>\n",
    "            params.grad.zero_()\n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():  # <2>\n",
    "            params -= learning_rate * params.grad\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our code updating `params` is not quite as straightforward as we might have expected. There are two particularities:\n",
    "\n",
    "+ first, we are encapsulating the update in a `no_grad` context using the Python `with` statement. This means within the `with` block, the `PyTorch` `autograd` mechanism should look away: that is, not add edges to the forward graph. In fact, when we are executing this bit of code, the forward graph that `PyTorch` records is consumed when we call backward, leaving us with the params leaf node. But now we want to change this leaf node before we start building a fresh forward graph on top of it. While this use case is usually wrapped inside the optimizers we discuss in `section 5.5.2`, we will take a closer look when we see another common use of `no_grad` in `section 5.5.4`.\n",
    "\n",
    "+ second, we update params in place. This means we keep the same `params` tensor around but subtract our update from it. When using `autograd`, we usually avoid in-place updates because `PyTorch`’s `autograd` engine might need the values we would be modifying for the backward pass. Here, however, we are operating without `autograd`, and it is beneficial to keep the `params` tensor. Not replacing the parameters by assigning new tensors to their variable name will become crucial when we register our parameters with the optimizer in `section 5.5.2`.\n",
    "\n",
    "Let’s see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860116\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957697\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop_autograd(\n",
    "    n_epochs = 5000, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0], requires_grad=True), # <1> \n",
    "    t_u = t_un, # <2> \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the same as we got previously. Good for us! It means that while we are capable of computing derivatives by hand, we no longer need to.\n",
    "\n",
    "### 5.5.2 Optimizers a la carte\n",
    "In the example code, we used vanilla gradient descent for optimization, which worked fine for our simple case. Needless to say, there are several optimization strategies and tricks that can assist convergence, especially when models get complicated.\n",
    "\n",
    "We’ll dive deeper into this topic in later chapters, but now is the right time to introduce the way `PyTorch` abstracts the optimization strategy away from user code: that is, the training loop we’ve examined. This saves us from the boilerplate busywork of having to update each and every parameter to our model ourselves. The `torch` module has an `optim` submodule where we can find classes implementing different optimization algorithms. Here’s an abridged list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " 'lr_scheduler']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(torch.optim) if not x.startswith(\"__\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every optimizer constructor takes a list of parameters (aka `PyTorch` tensors, typically with `requires_grad` set to `True`) as the first input. All parameters passed to the optimizer are retained inside the optimizer object so the optimizer can update their values and access their `grad` attribute, as represented in `figure 5.11`.\n",
    "\n",
    "<img src=\"images/05_11.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Each optimizer exposes two methods: \n",
    "+ *zero_grad()*: zeroes the grad attribute of all the parameters passed to the optimizer upon construction\n",
    "+ *step()*: updates the value of those parameters according to the optimization strategy implemented by the specific optimizer\n",
    "\n",
    "##### USING A GRADIENT DESCENT OPTIMIZER\n",
    "Let’s create `params` and instantiate a gradient descent optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True) \n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `SGD` stands for `stochastic gradient descent`. Actually, the optimizer itself is exactly a vanilla gradient descent (as long as the momentum argument is set to $0.0$, which is the default). The term stochastic comes from the fact that the gradient is typically obtained by averaging over a random subset of all input samples, called a `minibatch`. However, the optimizer does not know if the loss was evaluated on all the samples (vanilla) or a random subset of them (stochastic), so the algorithm is literally the same in the two cases.\n",
    "\n",
    "Anyway, let’s take our fancy new optimizer for a spin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params) \n",
    "loss = loss_fn(t_p, t_c) \n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of `params` is updated upon calling step without us having to touch it ourselves! What happens is that the optimizer looks into `params.grad` and updates `params`, subtracting `learning_rate` times `grad` from it, exactly as in our former hand-rolled code.\n",
    "\n",
    "Ready to stick this code in a training loop? Nope! The big gotcha almost got us-we forgot to zero out the gradients. Had we called the previous code in a loop, gradients would have accumulated in the leaves at every call to `backward`, and our gradient descent would have been all over the place! Here’s the loop-ready code, with the extra `zero_grad` at the correct spot (right before the call to `backward`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7761, 0.1064], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_un, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad() # <1>\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! See how the `optim` module helps us abstract away the specific optimization scheme? All we have to do is provide a list of params to it (that list can be extremely long, as is needed for very deep neural network models), and we can forget about the details.\n",
    "\n",
    "Let’s update our training loop accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_optim(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860118\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957697\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927680\n",
      "Epoch 4500, Loss 2.927651\n",
      "Epoch 5000, Loss 2.927648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.SGD([params], lr=learning_rate) # <1>\n",
    "\n",
    "training_loop_optim(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    params = params, # <1> \n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get the same result as before. Great: this is further confirmation that we know how to descend a gradient by hand!\n",
    "\n",
    "##### TESTING OTHER OPTIMIZERS\n",
    "In order to test more optimizers, all we have to do is instantiate a different optimizer, say `Adam`, instead of `SGD`. The rest of the code stays as it is. Pretty handy stuff.\n",
    "\n",
    "We won’t go into much detail about `Adam`; suffice to say that it is a more sophisticated optimizer in which the learning rate is set adaptively. In addition, it is a lot less sensitive to the scaling of the parameters—so insensitive that we can go back to using the original (non-normalized) input $t\\_u$, and even increase the learning rate to $1e^{-1}$, and `Adam` won’t even blink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612903\n",
      "Epoch 1000, Loss 3.086700\n",
      "Epoch 1500, Loss 2.928578\n",
      "Epoch 2000, Loss 2.927646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = torch.optim.Adam([params], lr=learning_rate) # <1>\n",
    "\n",
    "training_loop_optim(\n",
    "    n_epochs = 2000, \n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_u, # <2> \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is not the only flexible part of our training loop. Let’s turn our attention to the model. In order to train a neural network on the same data and the same loss, all we would need to change is the model function. It wouldn’t make particular sense in this case, since we know that converting Celsius to Fahrenheit amounts to a linear transformation, but we’ll do it anyway in `chapter 6`. We’ll see quite soon that neural networks allow us to remove our arbitrary assumptions about the shape of the function we should be approximating. Even so, we’ll see how neural networks manage to be trained even when the underlying processes are highly nonlinear (such in the case of describing an image with a sentence, as we saw in `chapter 2`).\n",
    "\n",
    "We have touched on a lot of the essential concepts that will enable us to train complicated deep learning models while knowing what’s going on under the hood: backpropagation to estimate gradients, autograd, and optimizing weights of models using gradient descent or other optimizers. Really, there isn’t a lot more. The rest is mostly filling in the blanks, however extensive they are.\n",
    "\n",
    "Next up, we’re going to offer an aside on how to split our samples, because that sets up a perfect use case for learning how to better control autograd.\n",
    "\n",
    "### 5.5.3 Training, validation, and overfitting\n",
    "Johannes Kepler taught us one last thing that we didn’t discuss so far, remember? He kept part of the data on the side so that he could validate his models on independent observations. This is a vital thing to do, especially when the model we adopt could potentially approximate functions of any shape, as in the case of neural networks. In other words, a highly adaptable model will tend to use its many parameters to make sure the loss is minimal at the data points, but we’ll have no guarantee that the model behaves well away from or in between the data points. After all, that’s what we’re asking the optimizer to do: minimize the loss at the data points. Sure enough, if we had independent data points that we didn’t use to evaluate our loss or descend along its negative gradient, we would soon find out that evaluating the loss at those independent data points would yield higher-than-expected loss. We have already mentioned this phenomenon, called *overfitting*.\n",
    "\n",
    "The first action we can take to combat overfitting is recognizing that it might happen. In order to do so, as Kepler figured out in $1600$, we must take a few data points out of our dataset (the *validation set*) and only fit our model on the remaining data points (the *training set*), as shown in `figure 5.12`. Then, while we’re fitting the model, we can evaluate the loss once on the training set and once on the validation set. When we’re trying to decide if we’ve done a good job of fitting our model to the data, we must look at both!\n",
    "\n",
    "<img src=\"images/05_12.png\" style=\"width:600px;\"/>\n",
    "\n",
    "##### EVALUATING THE TRAINING LOSS\n",
    "The training loss will tell us if our model can fit the training set at all—in other words, if our model has enough capacity to process the relevant information in the data. If our mysterious thermometer somehow managed to measure temperatures using a logarithmic scale, our poor linear model would not have had a chance to fit those measurements and provide us with a sensible conversion to Celsius. In that case, our training loss (the loss we were printing in the training loop) would stop decreasing well before approaching zero.\n",
    "\n",
    "A deep neural network can potentially approximate complicated functions, provided that the number of neurons, and therefore parameters, is high enough. The fewer the number of parameters, the simpler the shape of the function our network will be able to approximate. So, rule 1: if the training loss is not decreasing, chances are the model is too simple for the data. The other possibility is that our data just doesn’t contain meaningful information that lets it explain the output: if the nice folks at the shop sell us a barometer instead of a thermometer, we will have little chance of predicting temperature in Celsius from just pressure, even if we use the latest neural network architecture from [Quebec](www.umontreal.ca/en/artificialintelligence).\n",
    "\n",
    "##### GENERALIZING TO THE VALIDATION SET\n",
    "What about the validation set? Well, if the loss evaluated in the validation set doesn’t decrease along with the training set, it means our model is improving its fit of the samples it is seeing during training, but it is not generalizing to samples outside this precise set. As soon as we evaluate the model at new, previously unseen points, the values of the loss function are poor. So, rule 2: if the training loss and the validation loss diverge, we’re overfitting.\n",
    "\n",
    "Let’s delve into this phenomenon a little, going back to our thermometer example. We could have decided to fit the data with a more complicated function, like a piecewise polynomial or a really large neural network. It could generate a model meandering its way through the data points, as in `figure 5.13`, just because it pushes the loss very close to zero. Since the behavior of the function away from the data points does not increase the loss, there’s nothing to keep the model in check for inputs away from the training data points.\n",
    "\n",
    "<img src=\"images/05_13.png\" style=\"width:600px;\"/>\n",
    "\n",
    "What’s the cure, though? Good question. From what we just said, overfitting really looks like a problem of making sure the behavior of the model in between data points is sensible for the process we’re trying to approximate. First of all, we should make sure we get enough data for the process. If we collected data from a sinusoidal process by sampling it regularly at a low frequency, we would have a hard time fitting a model to it.\n",
    "\n",
    "Assuming we have enough data points, we should make sure the model that is capable of fitting the training data is as regular as possible in between them. There are several ways to achieve this. One is adding penalization terms to the loss function, to make it cheaper for the model to behave more smoothly and change more slowly (up to a point). Another is to add noise to the input samples, to artificially create new data points in between training data samples and force the model to try to fit those, too. There are several other ways, all of them somewhat related to these. But the best favor we can do to ourselves, at least as a first move, is to make our model simpler. From an intuitive standpoint, a simpler model may not fit the training data as perfectly as a more complicated model would, but it will likely behave more regularly in between data points.\n",
    "\n",
    "We’ve got some nice trade-offs here. On the one hand, we need the model to have enough capacity for it to fit the training set. On the other, we need the model to avoid overfitting. Therefore, in order to choose the right size for a neural network model in terms of parameters, the process is based on two steps: increase the size until it fits, and then scale it down until it stops overfitting.\n",
    "\n",
    "We’ll see more about this in `chapter 12`—we’ll discover that our life will be a balancing act between fitting and overfitting. For now, let’s get back to our example and see how we can split the data into a training set and a validation set. We’ll do it by shuffling t_u and t_c the same way and then splitting the resulting shuffled tensors into two parts.\n",
    "\n",
    "##### SPLITTING A DATASET\n",
    "Shuffling the elements of a tensor amounts to finding a permutation of its indices. The `randperm` function does exactly this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9,  3,  7,  4, 10,  8,  5,  0,  1]), tensor([6, 2]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "\n",
    "n_val = int(0.2 * n_samples) \n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:] \n",
    "\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just got index tensors that we can use to build training and validation sets starting from the data tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices] \n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices] \n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u \n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop doesn’t really change. We just want to additionally evaluate the validation loss at every epoch, to have a chance to recognize whether we’re overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_with_val(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params) # <1>\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "                             \n",
    "        val_t_p = model(val_t_u, *params) # <1>\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward() # <2>\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                  f\" Validation loss {val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 709.7167, Validation loss 596.4901\n",
      "Epoch 2, Training loss 96.5514, Validation loss 97.4268\n",
      "Epoch 3, Training loss 15.4583, Validation loss 21.1731\n",
      "Epoch 500, Training loss 2.8369, Validation loss 3.7774\n",
      "Epoch 1000, Training loss 2.7893, Validation loss 4.2608\n",
      "Epoch 1500, Training loss 2.7807, Validation loss 4.4770\n",
      "Epoch 2000, Training loss 2.7792, Validation loss 4.5707\n",
      "Epoch 2500, Training loss 2.7789, Validation loss 4.6107\n",
      "Epoch 3000, Training loss 2.7788, Validation loss 4.6277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.5082, -18.4018], requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop_with_val(\n",
    "    n_epochs = 3000, \n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_un, # <1> \n",
    "    val_t_u = val_t_un, # <1> \n",
    "    train_t_c = train_t_c,\n",
    "    val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are not being entirely fair to our model. The validation set is really small, so the validation loss will only be meaningful up to a point. In any case, we note that the validation loss is higher than our training loss, although not by an order of magnitude. We expect a model to perform better on the training set, since the model parameters are being shaped by the training set. Our main goal is to also see both the training loss and the validation loss decreasing. While ideally both losses would be roughly the same value, as long as the validation loss stays reasonably close to the training loss, we know that our model is continuing to learn generalized things about our data. In `figure 5.14`, case $C$ is ideal, while $D$ is acceptable. In case $A$, the model isn’t learning at all; and in case $B$, we see overfitting. We’ll see more meaningful examples of overfitting in `chapter 12`.\n",
    "\n",
    "<img src=\"images/05_14.png\" style=\"width:500px;\"/>\n",
    "\n",
    "### 5.5.4 Autograd nits and switching it off\n",
    "From the previous training loop, we can appreciate that we only ever call `backward` on `train_loss`. Therefore, errors will only ever backpropagate based on the training set—the validation set is used to provide an independent evaluation of the accuracy of the model’s output on data that wasn’t used for training.\n",
    "\n",
    "The curious reader will have an embryo of a question at this point. The model is evaluated twice—once on `train_t_u` and once on `val_t_u`—and then `backward` is called. Won’t this confuse autograd? Won’t `backward` be influenced by the values generated during the pass on the validation set?\n",
    "\n",
    "Luckily for us, this isn’t the case. The first line in the training loop evaluates model on `train_t_u` to produce `train_t_p`. Then `train_loss` is evaluated from `train_t_p`. This creates a computation graph that links `train_t_u` to `train_t_p` to `train_loss`. When model is evaluated again on `val_t_u`, it produces `val_t_p` and `val_loss`. In this case, a separate computation graph will be created that links `val_t_u` to `val_t_p` to `val_loss`. Separate tensors have been run through the same functions, model and `loss_fn`, generating separate computation graphs, as shown in `figure 5.15`.\n",
    "\n",
    "<img src=\"images/05_15.png\" style=\"width:500px;\"/>\n",
    "\n",
    "The only tensors these two graphs have in common are the parameters. When we call backward on `train_loss`, we run `backward` on the first graph. In other words, we accumulate the derivatives of `train_loss` with respect to the parameters based on the computation generated from `train_t_u`.\n",
    "\n",
    "If we (incorrectly) called `backward` on `val_loss` as well, we would accumulate the derivatives of `val_loss` with respect to the parameters on the same leaf nodes. Remember the `zero_grad` thing, whereby gradients are accumulated on top of each other every time we call `backward` unless we zero out the gradients explicitly? Well, here something very similar would happen: calling backward on `val_loss` would lead to gradients accumulating in the params tensor, on top of those generated during the `train_loss.backward()` call. In this case, we would effectively train our model on the whole dataset (both training and validation), since the gradient would depend on both. Pretty interesting.\n",
    "\n",
    "There’s another element for discussion here. Since we’re not ever calling `backward` on `val_loss`, why are we building the graph in the first place? We could in fact just call `model` and `loss_fn` as plain functions, without tracking the computation. However optimized, building the autograd graph comes with additional costs that we could totally forgo during the validation pass, especially when the model has millions of parameters.\n",
    "\n",
    "In order to address this, `PyTorch` allows us to switch off autograd when we don’t need it, using the `torch.no_grad` context manager. We won’t see any meaningful advantage in terms of speed or memory consumption on our small problem. However, for larger models, the differences can add up. We can make sure this works by checking the value of the `requires_grad` attribute on the `val_loss` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_with_val2(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        with torch.no_grad(): # <1>\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False # <2>\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the related `set_grad_enabled` context, we can also condition the code to run with autograd enabled or disabled, according to a `Boolean` expression—typically indicating whether we are running in training or inference mode. We could, for instance, define a `calc_forward` function that takes data as input and runs `model` and `loss_fn` with or without autograd according to a `Boolean` `train_is` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_forward(t_u, t_c, is_train):\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Conclusion\n",
    "We started this chapter with a big question: how is it that a machine can learn from examples? We spent the rest of the chapter describing the mechanism with which a model can be optimized to fit data. We chose to stick with a simple model in order to see all the moving parts without unneeded complications.\n",
    "\n",
    "Now that we’ve had our fill of appetizers, in `chapter 6` we’ll finally get to the main course: using a neural network to fit our data. We’ll work on solving the same thermometer problem, but with the more powerful tools provided by the `torch.nn` module. We’ll adopt the same spirit of using this small problem to illustrate the larger uses of `PyTorch`. The problem doesn’t need a neural network to reach a solution, but it will allow us to develop a simpler understanding of what’s required to train a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
