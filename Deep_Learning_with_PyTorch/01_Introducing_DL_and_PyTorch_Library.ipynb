{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Introducing deep learning and the PyTorch Library\n",
    "\n",
    "## 1.1 The deep learning revolution\n",
    "To appreciate the paradigm shift ushered in by this deep learning approach, let’s take a step back for a bit of perspective. Until the last decade, the broader class of systems that fell under the label *machine learning* relied heavily on *feature engineering*. Features are transformations on input data that facilitate a downstream algorithm, like a classifier, to produce correct outcomes on new data. Feature engineering consists of coming up with the right transformations so that the downstream algorithm can solve a task. For instance, in order to tell ones from zeros in images of handwritten digits, we would come up with a set of filters to estimate the direction of edges over the image, and then train a classifier to predict the correct digit given a distribution of edge directions. Another useful feature could be the number of enclosed holes, as seen in a zero, an eight, and, particularly, loopy twos.\n",
    "\n",
    "Deep learning, on the other hand, deals with finding such representations automatically, from raw data, in order to successfully perform a task. In the ones versus zeros example, filters would be refined during training by iteratively looking at pairs of examples and target labels. This is not to say that feature engineering has no place with deep learning; we often need to inject some form of prior knowledge in a learning system. However, the ability of a neural network to ingest data and extract useful representations on the basis of examples is what makes deep learning so powerful. The focus of deep learning practitioners is not so much on handcrafting those representations, but on operating on a mathematical entity so that it discovers representations from the training data autonomously. Often, these automatically created features are better than those that are handcrafted! As with many disruptive technologies, this fact has led to a change in perspective.\n",
    "\n",
    "On the left side of `Figure 1.1`, we see a practitioner busy defining engineering features and feeding them to a learning algorithm; the results on the task will be as good as the features the practitioner engineers. On the right, with deep learning, the raw data is fed to an algorithm that extracts hierarchical features automatically, guided by the optimization of its own performance on the task; the results will be as good as the ability of the practitioner to drive the algorithm toward its goal.\n",
    "\n",
    "<img src=\"images/01_01.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Starting from the right side in `figure 1.1`, we already get a glimpse of what we need to execute successful deep learning:\n",
    "+ We need a way to ingest whatever data we have at hand.\n",
    "\n",
    "+ We somehow need to define the deep learning machine.\n",
    "\n",
    "+ We must have an automated way, training, to obtain useful representations and make the machine produce desired outputs.\n",
    "\n",
    "This leaves us with taking a closer look at this training thing we keep talking about. During training, we use a criterion, a real-valued function of model outputs and reference data, to provide a numerical score for the discrepancy between the desired and actual output of our model (by convention, a lower score is typically better). Training consists of driving the criterion toward lower and lower scores by incrementally modifying our deep learning machine until it achieves low scores, even on data not seen during training.\n",
    "\n",
    "## 1.2 PyTorch for deep learning\n",
    "`PyTorch` is a library for Python programs that facilitates building deep learning projects. It emphasizes flexibility and allows deep learning models to be expressed in idiomatic Python. \n",
    "\n",
    "As Python does for programming, `PyTorch` provides an excellent introduction to deep learning. At the same time, `PyTorch` has been proven to be fully qualified for use in professional contexts for real-world, high-profile work. We believe that `PyTorch`’s clear syntax, streamlined API, and easy debugging make it an excellent choice for introducing deep learning. We highly recommend studying `PyTorch` for your first deep learning library. Whether it ought to be the last deep learning library you learn is a decision we leave up to you.\n",
    "\n",
    "At its core, the deep learning machine in `figure 1.1` is a rather complex mathematical function mapping inputs to an output. To facilitate expressing this function, `PyTorch` provides a core data structure, the tensor, which is a multidimensional array that shares many similarities with `NumPy` arrays. Around that foundation, `PyTorch` comes with features to perform accelerated mathematical operations on dedicated hardware, which makes it convenient to design neural network architectures and train them on individual machines or parallel computing resources.\n",
    "\n",
    "`Deep Learning with PyTorch` is organized in three distinct parts:\n",
    "+ **Part 1** covers the foundations, examining in detail the facilities `PyTorch` offers to put the sketch of deep learning in `figure 1.1` into action with code. \n",
    "+ **Part 2** walks you through an end-to-end project involving medical imaging: finding and classifying tumors in CT scans, building on the basic concepts introduced in part 1, and adding more advanced topics. \n",
    "+ **Part 3** rounds off the book with a tour of what `PyTorch` offers for deploying deep learning models to production.\n",
    "\n",
    "Deep learning is a huge space. In this book, we will be covering a tiny part of that space: specifically, using `PyTorch` for smaller-scope classification and segmentation projects, with image processing of 2D and 3D datasets used for most of the motivating examples. This book focuses on practical `PyTorch`, with the aim of covering enough ground to allow you to solve real-world machine learning problems, such as in vision, with deep learning or explore new models as they pop up in research literature. \n",
    "\n",
    "## 1.3 Why PyTorch?\n",
    "Let’s take a look at some of the reasons we decided to use PyTorch:\n",
    "+ `PyTorch` is easy to recommend because of its simplicity. Many researchers and practitioners find it easy to learn, use, extend, and debug. It’s Pythonic, and while like any complicated domain it has caveats and best practices, using the library generally feels familiar to developers who have used Python previously.\n",
    "\n",
    "+ More concretely, programming the deep learning machine is very natural in `PyTorch`. `PyTorch` gives us a data type, the Tensor, to hold numbers, vectors, matrices, or arrays in general. In addition, it provides functions for operating on them. We can program with them incrementally and, if we want, interactively, just like we are used to from Python. If you know `NumPy`, this will be very familiar.\n",
    "\n",
    "+ `PyTorch` offers two things that make it particularly relevant for deep learning: \n",
    "    - first, it provides accelerated computation using `GPU`, often yielding speedups in the range of $50x$ over doing the same calculation on a `CPU`. \n",
    "    - Second, `PyTorch` provides facilities that support numerical optimization on generic mathematical expressions, which deep learning uses for training. Note that both features are useful for scientific computing in general, not exclusively for deep learning. In fact, we can safely characterize `PyTorch` as a high-performance library with optimization support for scientific computing in Python.\n",
    "\n",
    "+ A design driver for PyTorch is expressivity, allowing a developer to implement complicated models without undue complexity being imposed by the library (it’s not a framework!). `PyTorch` arguably offers one of the most seamless translations of ideas into Python code in the deep learning landscape. For this reason, `PyTorch` has seen widespread adoption in research, as witnessed by the high citation counts at international conferences.\n",
    "\n",
    "+ `PyTorch` also has a compelling story for the transition from research and development into production. While it was initially focused on research workflows, `PyTorch` has been equipped with a high-performance C++ runtime that can be used to deploy models for inference without relying on Python, and can be used for designing and training models in C++. It has also grown bindings to other languages and an interface for deploying to mobile devices. These features allow us to take advantage of `PyTorch`’s flexibility and at the same time take our applications where a full Python runtime would be hard to get or would impose expensive overhead.\n",
    "\n",
    "### 1.3.1 The deep learning competitive landscape\n",
    "While all analogies are flawed, it seems that the release of `PyTorch 0.1` in January 2017 marked the transition from a Cambrian-explosion-like proliferation of deep learning libraries, wrappers, and data-exchange formats into an era of consolidation and unification.\n",
    "\n",
    "> **NOTE**\n",
    "> \n",
    "> The deep learning landscape has been moving so quickly lately that by the time you read this in print, it will likely be out of date. If you’re unfamiliar with some of the libraries mentioned here, that’s fine.\n",
    "\n",
    "At the time of `PyTorch`’s first beta release:\n",
    "+ `Theano` and `TensorFlow` were the premiere low-level libraries, working with a model that had the user define a computational graph and then execute it.\n",
    "\n",
    "+ `Lasagne` and `Keras` were high-level wrappers around `Theano`, with `Keras` wrapping `TensorFlow` and `CNTK` as well.\n",
    "\n",
    "+ `Caffe`, `Chainer`, `DyNet`, `Torch` (the Lua-based precursor to `PyTorch`), `MXNet`, `CNTK`, `DL4J`, and others filled various niches in the ecosystem.\n",
    "\n",
    "In the roughly two years that followed, the landscape changed drastically. The community largely consolidated behind either `PyTorch` or `TensorFlow`, with the adoption of other libraries dwindling, except for those filling specific niches. In a nutshell:\n",
    "+ `Theano`, one of the first deep learning frameworks, has ceased active development. \n",
    "\n",
    "+ TensorFlow:\n",
    "    - Consumed `Keras` entirely, promoting it to a first-class API\n",
    "    - Provided an immediate-execution \"eager mode\" that is somewhat similar to how `PyTorch` approaches computation\n",
    "    - Released TF 2.0 with eager mode by default\n",
    "\n",
    "+ `JAX`, a library by Google that was developed independently from `TensorFlow`, has started gaining traction as a `NumPy` equivalent with GPU, autograd and JIT capabilities.\n",
    "\n",
    "+ PyTorch:\n",
    "    - Consumed `Caffe2` for its backend\n",
    "    - Replaced most of the low-level code reused from the Lua-based `Torch` project\n",
    "    - Added support for `ONNX`, a vendor-neutral model description and exchange format\n",
    "    - Added a delayed-execution \"graph mode\" runtime called `TorchScript`\n",
    "    - Released version 1.0\n",
    "    - Replaced `CNTK` and `Chainer` as the framework of choice by their respective corporate sponsors\n",
    "\n",
    "`TensorFlow` has a robust pipeline to production, an extensive industry-wide community, and massive mindshare. `PyTorch` has made huge inroads with the research and teaching communities, thanks to its ease of use, and has picked up momentum since, as researchers and graduates train students and move to industry. It has also built up steam in terms of production solutions. Interestingly, with the advent of `TorchScript` and eager mode, both `PyTorch` and `TensorFlow` have seen their feature sets start to converge with the other’s, though the presentation of these features and the overall experience is still quite different between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 An overview of how PyTorch supports deep learning projects\n",
    "Let’s now take some time to formalize a high-level map of the main components that form `PyTorch`. We can best do this by looking at what a deep learning project needs from `PyTorch`.\n",
    "\n",
    "First, there’s a lot of non-Python code in `PyTorch`. Actually, for performance reasons, most of `PyTorch` is written in C++ and `CUDA`, a C++-like language from `NVIDIA` that can be compiled to run with massive parallelism on GPUs. There are ways to run `PyTorch` directly from C++, and we’ll look into those in `chapter 15`. One of the motivations for this capability is to provide a reliable strategy for deploying models in production. However, most of the time we’ll interact with `PyTorch` from Python, building models, training them, and using the trained models to solve actual problems.\n",
    "\n",
    "Indeed, the Python API is where `PyTorch` shines in term of usability and integration with the wider Python ecosystem. Let’s take a peek at the mental model of what PyTorch is:\n",
    "+ `PyTorch` is a library that provides multidimensional arrays, or tensors in `PyTorch` parlance, and an extensive library of operations on them, provided by the torch module. Both tensors and the operations on them can be used on the CPU or the GPU. Moving computations from the CPU to the GPU in `PyTorch` doesn’t require more than an additional function call or two. \n",
    "\n",
    "+ The second core thing that `PyTorch` provides is the ability of tensors to keep track of the operations performed on them and to analytically compute derivatives of an output of a computation with respect to any of its inputs. This is used for numerical optimization, and it is provided natively by tensors by virtue of dispatching through `PyTorch`’s autograd engine under the hood.\n",
    "\n",
    "+ By having tensors and the autograd-enabled tensor standard library, `PyTorch` can be used for physics, rendering, optimization, simulation, modeling, and more—we’re very likely to see `PyTorch` used in creative ways throughout the spectrum of scientific applications. But `PyTorch` is first and foremost a deep learning library, and as such it provides all the building blocks needed to build neural networks and train them. `Figure 1.2` shows a standard setup that loads data, trains a model, and then deploys that model to production.\n",
    "\n",
    "<img src=\"images/01_02.png\" style=\"width:600px;\"/>\n",
    "\n",
    "The core `PyTorch` modules for building neural networks are located in `torch.nn`, which provides common neural network layers and other architectural components. Fully connected layers, convolutional layers, activation functions, and loss functions can all be found here. These components can be used to build and initialize the untrained model we see in the center of `figure 1.2`. In order to train our model, we need a few additional things: a source of training data, an optimizer to adapt the model to the training data, and a way to get the model and data to the hardware that will actually be performing the calculations needed for training the model.\n",
    "\n",
    "At left in `figure 1.2`, we see that quite a bit of data processing is needed before the training data even reaches our model:\n",
    "+ first we need to physically get the data, most often from some sort of storage as the data source. \n",
    "\n",
    "+ then we need to convert each sample from our data into a something `PyTorch` can actually handle: tensors. This bridge between our custom data (in whatever format it might be) and a standardized `PyTorch` tensor is the `Dataset` class `PyTorch` provides in `torch.utils.data`. As this process is wildly different from one problem to the next, we will have to implement this data sourcing ourselves. We will look in detail at how to represent various type of data we might want to work with as tensors in `chapter 4`.\n",
    "\n",
    "As data storage is often slow, in particular due to access latency, we want to parallelize data loading. `PyTorch` `DataLoader` class can spawn child processes to load data from a dataset in the background so that it’s ready and waiting for the training loop as soon as the loop can use it. We will meet and use `Dataset` and `DataLoader` in `chapter 7`.\n",
    "\n",
    "With the mechanism for getting batches of samples in place, we can turn to the training loop itself at the center of `figure 1.2`. Typically, the training loop is implemented as a standard Python for loop. In the simplest case, the model runs the required calculations on the local CPU or a single GPU, and once the training loop has the data, computation can start immediately. Chances are this will be your basic setup, too, and it’s the one we’ll assume in this book.\n",
    "\n",
    "At each step in the training loop, we evaluate our model on the samples we got from the data loader. We then compare the outputs of our model to the desired output (the targets) using some criterion or loss function. Just as it offers the components from which to build our model, `PyTorch` also has a variety of loss functions at our disposal. They, too, are provided in `torch.nn`. After we have compared our actual outputs to the ideal with the loss functions, we need to push the model a little to move its outputs to better resemble the target. As mentioned earlier, this is where the `PyTorch` autograd engine comes in; but we also need an optimizer doing the updates, and that is what `PyTorch` offers us in `torch.optim`. \n",
    "\n",
    "It’s increasingly common to use more elaborate hardware like multiple GPUs or multiple machines that contribute their resources to training a large model, as seen in the bottom center of `figure 1.2`. In those cases, `torch.nn.parallel.DistributedDataParallel` and the `torch.distributed` submodule can be employed to use the additional hardware.\n",
    "\n",
    "The training loop might be the most unexciting yet most time-consuming part of a deep learning project. At the end of it, we are rewarded with a model whose parameters have been optimized on our task: the trained model depicted to the right of the training loop in the figure. Having a model to solve a task is great, but in order for it to be useful, we must put it where the work is needed. This deployment part of the process, depicted on the right in `figure 1.2`, may involve putting the model on a server or exporting it to load it to a cloud engine, as shown in the figure. Or we might integrate it with a larger application, or run it on a phone.\n",
    "\n",
    "One particular step of the deployment exercise can be to export the model. As mentioned earlier, `PyTorch` defaults to an immediate execution model (eager mode). Whenever an instruction involving `PyTorch` is executed by the Python interpreter, the corresponding operation is immediately carried out by the underlying C++ or `CUDA` implementation. As more instructions operate on tensors, more operations are executed by the backend implementation.\n",
    "\n",
    "`PyTorch` also provides a way to compile models ahead of time through `TorchScript`. Using `TorchScript`, `PyTorch` can serialize a model into a set of instructions that can be invoked independently from Python: say, from C++ programs or on mobile devices. We can think about it as a virtual machine with a limited instruction set, specific to tensor operations. This allows us to export our model, either as `TorchScript` to be used with the `PyTorch` runtime, or in a standardized format called `ONNX`. These features are at the basis of the production deployment capabilities of `PyTorch`. We’ll cover this in `chapter 15`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Hardware and software requirements\n",
    "This book will require coding and running tasks that involve heavy numerical computing, such as multiplication of large numbers of matrices. As it turns out, running a pretrained network on new data is within the capabilities of any recent laptop or personal computer. Even taking a pretrained network and retraining a small portion of it to specialize it on a new dataset doesn’t necessarily require specialized hardware. You can follow along with everything we do in `part 1` of this book using a standard personal computer or laptop.\n",
    "\n",
    "However, we anticipate that completing a full training run for the more advanced examples in `part 2` will require a GPU. The default parameters used in `part 2` assume a GPU with 8 GB of RAM (we suggest an NVIDIA GTX 1070 or better), but those can be adjusted if your hardware has less RAM available. To be clear: such hardware is not mandatory if you’re willing to wait, but running on a GPU cuts training time by at least an order of magnitude (and usually it’s $40\\sim50x$ faster). Taken individually, the operations required to compute parameter updates are fast (from fractions of a second to a few seconds) on modern hardware like a typical laptop CPU. The issue is that training involves running these operations over and over, many, many times, incrementally updating the network parameters to minimize the training error.\n",
    "\n",
    "Moderately large networks can take hours to days to train from scratch on large, real-world datasets on workstations equipped with a good GPU. That time can be reduced by using multiple GPUs on the same machine, and even further on clusters of machines equipped with multiple GPUs. These setups are less prohibitive to access than it sounds, thanks to the offerings of cloud computing providers. [DAWNBench](https://dawn.cs.stanford.edu/benchmark/index.html) is an interesting initiative from Stanford University aimed at providing benchmarks on training time and cloud computing costs related to common deep learning tasks on publicly available datasets.\n",
    "\n",
    "So, if there’s a GPU around by the time you reach `part 2`, then great. Otherwise, we suggest checking out the offerings from the various cloud platforms, many of which offer GPU-enabled Jupyter Notebooks with `PyTorch` preinstalled, often with a free quota. Google [Colaboratory](https://colab.research.google.com) is a great place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
