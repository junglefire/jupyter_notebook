{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Pytorch env\n",
    "import os\n",
    "os.environ['TORCH_HOME']=\"/home/alex/data/pytorch\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "import datetime\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision as torchv\n",
    "import torch as torch\n",
    "import PIL as pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Using convolutions to generalize\n",
    "In the previous chapter, we built a simple neural network that could fit (or overfit) the data, thanks to the many parameters available for optimization in the linear layers. We had issues with our model, however, in that it was better at memorizing the training set than it was at generalizing properties of birds and airplanes. Based on our model architecture, we’ve got a guess as to why that’s the case. Due to the fully connected setup needed to detect the various possible translations of the bird or airplane in the image, we have both too many parameters (making it easier for the model to memorize the training set) and no position independence (making it harder to generalize). As we discussed in the last chapter, we could augment our training data by using a wide variety of recropped images to try to force generalization, but that won’t address the issue of having too many parameters.\n",
    "\n",
    "There is a better way! It consists of replacing the dense, fully connected affine transformation in our neural network unit with a different linear operation: **convolution**.\n",
    "\n",
    "## 8.1 The case for convolutions\n",
    "Let’s get to the bottom of what convolutions are and how we can use them in our neural networks. \n",
    "\n",
    "In this section, we’ll see how convolutions deliver locality and translation invariance. We’ll do so by taking a close look at the formula defining convolutions and applying it using pen and paper—but don’t worry, the gist will be in pictures, not formulas.\n",
    "\n",
    "We said earlier that taking a 1D view of our input image and multiplying it by an $\\textit{n_output_features} × \\textit{n_input_features}$ weight matrix, as is done in `nn.Linear`, means for each channel in the image, computing a weighted sum of all the pixels multiplied by a set of weights, one per output feature.\n",
    "\n",
    "We also said that, if we want to recognize patterns corresponding to objects, like an airplane in the sky, we will likely need to look at how nearby pixels are arranged, and we will be less interested in how pixels that are far from each other appear in combination. Essentially, it doesn’t matter if our image of a Spitfire has a tree or cloud or kite in the corner or not.\n",
    "\n",
    "In order to translate this intuition into mathematical form, we could compute the weighted sum of a pixel with its immediate neighbors, rather than with all other pixels in the image. This would be equivalent to building weight matrices, one per output feature and output pixel location, in which all weights beyond a certain distance from a center pixel are zero. This will still be a weighted sum: that is, a linear operation.\n",
    "\n",
    "### 8.1.1 What convolutions do\n",
    "We identified one more desired property earlier: we would like these localized patterns to have an effect on the output regardless of their location in the image: that is, to be translation invariant. To achieve this goal in a matrix applied to the image-as-a-vector we used in chapter 7 would require implementing a rather complicated pattern of weights (don’t worry if it is too complicated; it’ll get better shortly): most of the weight matrix would be zero (for entries corresponding to input pixels too far away from the output pixel to have an influence). For other weights, we would have to find a way to keep entries in sync that correspond to the same relative position of input and output pixels. This means we would need to initialize them to the same values and ensure that all these tied weights stayed the same while the network is updated during training. This way, we would ensure that weights operate in neighborhoods to respond to local patterns, and local patterns are identified no matter where they occur in the image.\n",
    "\n",
    "Of course, this approach is more than impractical. Fortunately, there is a readily available, local, translation-invariant linear operation on the image: a *convolution*. We can come up with a more compact description of a convolution, but what we are going to describe is exactly what we just delineated—only taken from a different angle.\n",
    "\n",
    "Convolution, or more precisely, *discrete convolution* (there’s an analogous continuous version that we won’t go into here), is defined for a 2D image as the scalar product of a weight matrix, the kernel, with every neighborhood in the input. Consider a $3 × 3$ kernel (in deep learning, we typically use small kernels; we’ll see why later on) as a 2D tensor:\n",
    "\n",
    "```python\n",
    "weight = torch.tensor([\n",
    "    [w00, w01, w02], \n",
    "    [w10, w11, w12],\n",
    "    [w20, w21, w22]])\n",
    "```\n",
    "\n",
    "and a $1$-channel, $M \\times N$ image:\n",
    "\n",
    "```python\n",
    "image = torch.tensor([\n",
    "    [i00, i01, i02, i03, ..., i0N],\n",
    "    [i10, i11, i12, i13, ..., i1N],\n",
    "    [i20, i21, i22, i23, ..., i2N],\n",
    "    [i30, i31, i32, i33, ..., i3N], \n",
    "    #...\n",
    "    [iM0, iM1m iM2, iM3, ..., iMN]])\n",
    "```\n",
    "\n",
    "We can compute an element of the output image (without bias) as follows:\n",
    "\n",
    "```python\n",
    "o11 = i11 * w00 + i12 * w01 + i22 * w02 + \n",
    "        i21 * w10 + i22 * w11 + i23 * w12 + \n",
    "        i31 * w20 + i32 * w21 + i33 * w22\n",
    "```\n",
    "\n",
    "`Figure 8.1` shows this computation in action.\n",
    "\n",
    "That is, we \"translate\" the kernel on the $i11$ location of the input image, and we multiply each weight by the value of the input image at the corresponding location. Thus, the output image is created by translating the kernel on all input locations and performing the weighted sum. For a multichannel image, like our `RGB` image, the weight matrix would be a $3 × 3 × 3$ matrix: one set of weights for every channel, contributing together to the output values.\n",
    "\n",
    "Note that, just like the elements in the weight matrix of `nn.Linear`, the weights in the kernel are not known in advance, but they are initialized randomly and updated through backpropagation. Note also that the same kernel, and thus each weight in the kernel, is reused across the whole image. Thinking back to autograd, this means the use of each weight has a history spanning the entire image. Thus, the derivative of the loss with respect to a convolution weight includes contributions from the entire image.\n",
    "\n",
    "<img src=\"images/08_01.png\" style=\"width:600px;\"/>\n",
    "\n",
    "It’s now possible to see the connection to what we were stating earlier: a convolution is equivalent to having multiple linear operations whose weights are zero almost everywhere except around individual pixels and that receive equal updates during training.\n",
    "\n",
    "Summarizing, by switching to convolutions, we get:\n",
    "+ Local operations on neighborhoods  \n",
    "+ Translation invariance  \n",
    "+ Models with a lot fewer parameters\n",
    "\n",
    "The key insight underlying the third point is that, with a convolution layer, the number of parameters depends not on the number of pixels in the image, as was the case in our fully connected model, but rather on the size of the convolution kernel ($3 × 3$, $5 × 5$, and so on) and on how many convolution filters (or output channels) we decide to use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Convolutions in action\n",
    "Well, it looks like we’ve spent enough time down a rabbit hole! Let’s see some `PyTorch` in action on our birds versus airplanes challenge. The `torch.nn` module provides convolutions for $1$, $2$, and $3$ dimensions: \n",
    "+ `nn.Conv1d` for time series\n",
    "+ `nn.Conv2d` for images\n",
    "+ `nn.Conv3d` for volumes or videos\n",
    "\n",
    "For our `CIFAR-10` data, we’ll resort to `nn.Conv2d`. At a minimum, the arguments we provide to `nn.Conv2d` are:\n",
    "+ the number of input features (or channels, since we’re dealing with multichannel images: that is, more than one value per pixel)\n",
    "+ the number of output features\n",
    "+ the size of the kernel\n",
    "\n",
    "For instance, for our first convolutional module, we’ll have $3$ input features per pixel (the RGB channels) and an arbitrary number of channels in the output—say, $16$. The more channels in the output image, the more the capacity of the network. We need the channels to be able to detect many different types of features. Also, because we are randomly initializing them, some of the features we’ll get, even after training, will turn out to be useless. Let’s stick to a kernel size of $3 × 3$.\n",
    "\n",
    "It is very common to have kernel sizes that are the same in all directions, so `PyTorch` has a shortcut for this: whenever `kernel_size=3` is specified for a 2D convolution, it means $3 × 3$ (provided as a tuple $(3, 3)$ in Python). For a 3D convolution, it means $3 × 3 × 3$. The CT scans we will see in part 2 of the book have a different voxel (volumetric pixel) resolution in one of the three axes. In such a case, it makes sense to consider kernels that have a different size for the exceptional dimension. But for now, we stick with having the same size of convolutions across all dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of the shortcut kernel_size=3, we could equivalently pass in the tuple that we see \n",
    "# in the output: kernel_size=(3, 3).\n",
    "conv = torch.nn.Conv2d(3, 16, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we expect to be the shape of the weight tensor? The kernel is of size $3 × 3$, so we want the weight to consist of $3 × 3$ parts. For a single output pixel value, our kernel would consider, say, $\\textit{in_ch} = 3$ input channels, so the weight component for a single output pixel value (and by translation the invariance for the entire output channel) is of shape $\\textit{in_ch} × 3 × 3$. Finally, we have as many of those as we have output channels, here $\\textit{out_ch} = 16$, so the complete weight tensor is $\\textit{out_ch} × \\textit{in_ch} × 3 × 3$, in our case $16 × 3 × 3 × 3$. The bias will have size $16$ (we haven’t talked about bias for a while for simplicity, but just as in the linear module case, it’s a constant value we add to each channel of the output image). Let’s verify our assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how convolutions are a convenient choice for learning from images. We have smaller models looking for local patterns whose weights are optimized across the entire image.\n",
    "\n",
    "A 2D convolution pass produces a 2D image as output, whose pixels are a weighted sum over neighborhoods of the input image. In our case, both the kernel weights and the bias `conv.weight` are initialized randomly, so the output image will not be particularly meaningful. As usual, we need to add the zeroth batch dimension with unsqueeze if we want to call the `conv` module with one input image, since `nn.Conv2d` expects a $B × C × H × W$ shaped tensor as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiates a dataset for the training data; \n",
    "# TorchVision downloads the data if it is not present.\n",
    "data_path = os.environ['TORCH_HOME'] + '/cifar_10/'\n",
    "\n",
    "cifar10 = torchv.datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=torchv.transforms.Compose([\n",
    "        torchv.transforms.ToTensor(),\n",
    "        torchv.transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = torchv.datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=torchv.transforms.Compose([\n",
    "        torchv.transforms.ToTensor(),\n",
    "        torchv.transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "label_map = {0:0, 2:1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]\n",
    "\n",
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0)) \n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re curious, so we can display the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute. Let’s take a look a the size of output: it’s `torch.Size([1, 16, 30, 30])`. Huh; we lost a few pixels in the process. How did that happen?\n",
    "\n",
    "### 8.2.1 Padding the boundary\n",
    "The fact that our output image is smaller than the input is a side effect of deciding what to do at the boundary of the image. Applying a convolution kernel as a weighted sum of pixels in a $3 × 3$ neighborhood requires that there are neighbors in all directions. If we are at $i00$, we only have pixels to the right of and below us. By default, `PyTorch` will slide the convolution kernel within the input picture, getting $\\textit{width} - \\textit{kernel_width} + 1$ horizontal and vertical positions. For odd-sized kernels, this results in images that are one-half the convolution kernel’s width (in our case, $\\frac{3}{2} = 1$) smaller on each side. This explains why we’re missing two pixels in each dimension.\n",
    "\n",
    "However, `PyTorch` gives us the possibility of padding the image by creating ghost pixels around the border that have value zero as far as the convolution is concerned. `Figure 8.3` shows padding in action.\n",
    "\n",
    "<img src=\"images/08_03.png\" style=\"width:600px;\"/>\n",
    "\n",
    "In our case, specifying $\\textit{padding}=1$ when $\\textit{kernel_size}=3$ means $i00$ has an extra set of neighbors above it and to its left, so that an output of the convolution can be computed even in the corner of our original image. The net result is that the output has now the exact same size as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0)) \n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the sizes of weight and bias don’t change, regardless of whether padding is used.\n",
    "\n",
    "There are two main reasons to pad convolutions. First, doing so helps us separate the matters of convolution and changing image sizes, so we have one less thing to remember. And second, when we have more elaborate structures such as skip connections (discussed in `section 8.5.3`) or the `U-Nets` we’ll cover in `part 2`, we want the tensors before and after a few convolutions to be of compatible size so that we can add them or take differences.\n",
    "\n",
    "### 8.2.2 Detecting features with convolutions\n",
    "We said earlier that weight and bias are parameters that are learned through backpropagation, exactly as it happens for weight and bias in `nn.Linear`. However, we can play with convolution by setting weights by hand and see what happens.\n",
    "\n",
    "Let’s first zero out bias, just to remove any confounding factors, and then set weights to a constant value so that each pixel in the output gets the mean of its neighbors. For each $3 × 3$ neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0/9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have gone with `conv.weight.one_()`—that would result in each pixel in the output being the sum of the pixels in the neighborhood. Not a big difference, except that the values in the output image would have been nine times larger.\n",
    "\n",
    "Anyway, let’s see the effect on our `CIFAR` image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could have predicted, the filter produces a blurred version of the image. After all, every pixel of the output is the average of a neighborhood of the input, so pixels in the output are correlated and change more smoothly.\n",
    "\n",
    "Next, let’s try something different. The following kernel may look a bit mysterious at first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torch.nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0], [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working out the weighted sum for an arbitrary pixel in position $2,2$, as we did earlier for the generic convolution kernel, we get:\n",
    "$o22 = i13 - i11 + i23 - i21 + i33 - i31$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which performs the difference of all pixels on the right of $i22$ minus the pixels on the left of $i22$. If the kernel is applied on a vertical boundary between two adjacent regions of different intensity, $o22$ will have a high value. If the kernel is applied on a region of uniform intensity, $o22$ will be zero. It’s an edge-detection kernel: the kernel highlights the vertical edge between two horizontally adjacent regions.\n",
    "\n",
    "Applying the convolution kernel to our image, we see the result shown in `figure 8.5`. As expected, the convolution kernel enhances the vertical edges. We could build lots more elaborate filters, such as for detecting horizontal or diagonal edges, or crosslike or checkerboard patterns, where \"detecting\" means the output has a high magnitude. In fact, the job of a computer vision expert has historically been to come up with the most effective combination of filters so that certain features are highlighted in images and objects can be recognized.\n",
    "\n",
    "<img src=\"images/08_05.png\" style=\"width:600px;\"/>\n",
    "\n",
    "With deep learning, we let kernels be estimated from data in whatever way the discrimination is most effective: for instance, in terms of minimizing the negative crossentropy loss between the output and the ground truth that we introduced in `section 7.2.5`. From this angle, the job of a convolutional neural network is to estimate the kernel of a set of filter banks in successive layers that will transform a multichannel image into another multichannel image, where different channels correspond to different features (such as one channel for the average, another channel for vertical edges, and so on). `Figure 8.6` shows how the training automatically learns the kernels.\n",
    "\n",
    "<img src=\"images/08_06.png\" style=\"width:600px;\"/>\n",
    "\n",
    "### 8.2.3 Looking further with depth and pooling\n",
    "This is all well and good, but conceptually there’s an elephant in the room. We got all excited because by moving from fully connected layers to convolutions, we achieve locality and translation invariance. Then we recommended the use of small kernels, like $3 × 3$, or $5 × 5$: that’s peak locality, all right. What about the big picture? How do we know that all structures in our images are $3$ pixels or $5$ pixels wide? Well, we don’t, because they aren’t. And if they aren’t, how are our networks going to be equipped to see those patterns with larger scope? This is something we’ll really need if we want to solve our birds versus airplanes problem effectively, since although `CIFAR-10` images are small, the objects still have a (wing-)span several pixels across.\n",
    "\n",
    "One possibility could be to use large convolution kernels. Well, sure, at the limit we could get a $32 × 32$ kernel for a $32 × 32$ image, but we would converge to the old fully connected, affine transformation and lose all the nice properties of convolution. Another option, which is used in convolutional neural networks, is stacking one convolution after the other and at the same time downsampling the image between successive convolutions.\n",
    "\n",
    "##### FROM LARGE TO SMALL: DOWNSAMPLING\n",
    "Downsampling could in principle occur in different ways. Scaling an image by half is the equivalent of taking four neighboring pixels as input and producing one pixel as output. How we compute the value of the output based on the values of the input is up to us. We could:\n",
    "+ *Average the four pixels*. This average pooling was a common approach early on but has fallen out of favor somewhat.\n",
    "+ *Take the maximum of the four pixels*. This approach, called max pooling, is currently the most commonly used approach, but it has a downside of discarding the other three-quarters of the data.\n",
    "+ *Perform a strided convolution, where only every $N$-th pixel is calculated*. A $3 × 4$ convolution with stride $2$ still incorporates input from all pixels from the previous layer. The literature shows promise for this approach, but it has not yet supplanted max pooling.\n",
    "\n",
    "We will be focusing on max pooling, illustrated in `figure 8.7`, going forward. The figure shows the most common setup of taking non-overlapping $2 x 2$ tiles and taking the maximum over each of them as the new pixel at the reduced scale.\n",
    "\n",
    "<img src=\"images/08_07.png\" style=\"width:500px;\"/>\n",
    "\n",
    "Intuitively, the output images from a convolution layer, especially since they are followed by an activation just like any other linear layer, tend to have a high magnitude where certain features corresponding to the estimated kernel are detected (such as vertical lines). By keeping the highest value in the $2 × 2$ neighborhood as the downsampled output, we ensure that the features that are found survive the downsampling, at the expense of the weaker responses.\n",
    "\n",
    "Max pooling is provided by the `nn.MaxPool2d` module (as with convolution, there are versions for 1D and 3D data). It takes as input the size of the neighborhood over which to operate the pooling operation. If we wish to downsample our image by half, we’ll want to use a size of $2$. Let’s verify that it works as expected directly on our input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = torch.nn.MaxPool2d(2) \n",
    "output = pool(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### COMBINING CONVOLUTIONS AND DOWNSAMPLING FOR GREAT GOOD\n",
    "Let’s now see how combining convolutions and downsampling can help us recognize larger structures. In `figure 8.8`, we start by applying a set of $3 × 3$ kernels on our $8 × 8$ image, obtaining a multichannel output image of the same size. Then we scale down the output image by half, obtaining a $4 × 4$ image, and apply another set of $3 × 3$ kernels to it. This second set of kernels operates on a $3 × 3$ neighborhood of something that has been scaled down by half, so it effectively maps back to $8 × 8$ neighborhoods of the input. In addition, the second set of kernels takes the output of the first set of kernels (features like averages, edges, and so on) and extracts additional features on top of those.\n",
    "\n",
    "So, on one hand, the first set of kernels operates on small neighborhoods on first-order, low-level features, while the second set of kernels effectively operates on wider neighborhoods, producing features that are compositions of the previous features. This is a very powerful mechanism that provides convolutional neural networks with the ability to see into very complex scenes—much more complex than our $32 × 32$ images from the `CIFAR-10` dataset.\n",
    "\n",
    "<img src=\"images/08_08.png\" style=\"width:600px;\"/>\n",
    "\n",
    "> **The receptive field of output pixels**\n",
    ">\n",
    "> When the second $3 × 3$ convolution kernel produces 21 in its conv output in `figure 8.8`, this is based on the top-left $3 × 3$ pixels of the first max pool output. They, in turn, correspond to the $6 × 6$ pixels in the top-left corner in the first conv output, which in turn are computed by the first convolution from the top-left $7 × 7$ pixels. So the pixel in the second convolution output is influenced by a $7 × 7$ input square. The first convolution also uses an implicitly \"padded\" column and row to produce the output in the corner; otherwise, we would have an $8 × 8$ square of input pixels informing a given pixel (away from the boundary) in the second convolution’s output. In fancy language, we say that a given output neuron of the $3 × 3$-conv, $2 × 2$-max-pool, $3 × 3$-conv construction has a receptive field of $8 × 8$.\n",
    "\n",
    "### 8.2.4 Putting it all together for our network\n",
    "With these building blocks in our hands, we can now proceed to build our convolutional neural network for detecting birds and airplanes. Let’s take our previous fully connected model as a starting point and introduce `nn.Conv2d` and `nn.MaxPool2d` as described previously:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.Tanh(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    nn.Tanh(), nn.MaxPool2d(2), \n",
    "    # ...\n",
    ")\n",
    "```\n",
    "\n",
    "The first convolution takes us from $3$ RGB channels to $16$, thereby giving the network a chance to generate $16$ independent features that operate to (hopefully) discriminate low-level features of birds and airplanes. Then we apply the `Tanh` activation function. The resulting $16$-channel $32 × 32$ image is pooled to a $16$-channel $16 × 16$ image by the first `MaxPool2d(2)`. At this point, the downsampled image undergoes another convolution that generates an $8$-channel $16 × 16$ output. With any luck, this output will consist of higher-level features. Again, we apply a `Tanh` activation and then pool to an $8$-channel $8 × 8$ output.\n",
    "\n",
    "Where does this end? After the input image has been reduced to a set of $8 × 8$ features, we expect to be able to output some probabilities from the network that we can feed to our negative log likelihood. However, probabilities are a pair of numbers in a 1D vector (one for airplane, one for bird), but here we’re still dealing with multichannel 2D features.\n",
    "\n",
    "Thinking back to the beginning of this chapter, we already know what we need to do: turn the $8$-channel $8 × 8$ image into a 1D vector and complete our network with a set of fully connected layers:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.Tanh(), nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    nn.Tanh(), nn.MaxPool2d(2), \n",
    "    # Warning: Something important is missing here!\n",
    "    # ...\n",
    "    nn.Linear(8 * 8 * 8, 32), \n",
    "    nn.Tanh(), \n",
    "    nn.Linear(32, 2))\n",
    "```\n",
    "\n",
    "This code gives us a neural network as shown in `figure 8.9`.\n",
    "\n",
    "<img src=\"images/08_09.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Ignore the \"something missing\" comment for a minute. Let’s first notice that the size of the linear layer is dependent on the expected size of the output of `MaxPool2d`: $8 × 8 × 8 = 512$. Let’s count the number of parameters for this small model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    torch.nn.Tanh(), torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    torch.nn.Tanh(), torch.nn.MaxPool2d(2), \n",
    "    # Warning: Something important is missing here!\n",
    "    # ...\n",
    "    torch.nn.Linear(8 * 8 * 8, 32), \n",
    "    torch.nn.Tanh(), \n",
    "    torch.nn.Linear(32, 2))\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()] \n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s very reasonable for a limited dataset of such small images. In order to increase the capacity of the model, we could increase the number of output channels for the convolution layers (that is, the number of features each convolution layer generates), which would lead the linear layer to increase its size as well.\n",
    "\n",
    "We put the \"Warning\" note in the code for a reason. The model has zero chance of running without complaining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size mismatch, m1: [64 x 8], m2: [512 x 32] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model(img.unsqueeze(0))\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, the error message is a bit obscure, but not too much so. We find references to linear in the traceback: looking back at the model, we see that only module that has to have a $512 × 32$ tensor is `nn.Linear(512, 32)`, the first linear module after the last convolution block.\n",
    "\n",
    "What’s missing there is the reshaping step from an $8$-channel $8 × 8$ image to a $512$-element, 1D vector (1D if we ignore the batch dimension, that is). This could be achieved by calling `view` on the output of the last `nn.MaxPool2d`, but unfortunately, we don’t have any explicit visibility of the output of each module when we use `nn.Sequential`. \n",
    "\n",
    "\n",
    "## 8.3 Subclassing nn.Module\n",
    "At some point in developing neural networks, we will find ourselves in a situation where we want to compute something that the premade modules do not cover. Here, it is something very simple like reshaping, but in `section 8.5.3`, we use the same construction to implement residual connections. So in this section, we learn how to make our own `nn.Module` subclasses that we can then use just like the prebuilt ones or `nn.Sequential`.\n",
    "\n",
    "When we want to build models that do more complex things than just applying one layer after another, we need to leave `nn.Sequential` for something that gives us added flexibility. `PyTorch` allows us to use any computation in our model by subclassing `nn.Module`.\n",
    "\n",
    "In order to subclass `nn.Module`, at a minimum we need to define a `forward` function that takes the inputs to the module and returns the output. This is where we define our module’s computation. The name `forward` here is reminiscent of a distant past, when modules needed to define both the `forward` and `backward` passes we met in `section 5.5.1`. With `PyTorch`, if we use standard torch operations, autograd will take care of the backward pass automatically; and indeed, an `nn.Module` never comes with a `backward`.\n",
    "\n",
    "Typically, our computation will use other modules—premade like convolutions or customized. To include these submodules, we typically define them in the constructor `__init__` and assign them to `self` for use in the `forward` function. They will, at the same time, hold their parameters throughout the lifetime of our module. Note that you need to call `super().__init__()` before you can do that (or `PyTorch` will remind you).\n",
    "\n",
    "### 8.3.1 Our network as an nn.Module\n",
    "Let’s write our network as a submodule. To do so, we instantiate all the `nn.Conv2d`, `nn.Linear`, and so on that we previously passed to `nn.Sequential` in the constructor, and then use their instances one after another in forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = torch.nn.Tanh()\n",
    "        self.pool1 = torch.nn.MaxPool2d(2)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = torch.nn.Tanh()\n",
    "        self.pool2 = torch.nn.MaxPool2d(2)\n",
    "        self.fc1 = torch.nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = torch.nn.Tanh()\n",
    "        self.fc2 = torch.nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        # This reshape is what we were missing earlier.\n",
    "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Net` class is equivalent to the `nn.Sequential` model we built earlier in terms of submodules; but by writing the `forward` function explicitly, we can manipulate the output of `self.pool3` directly and call `view` on it to turn it into a $B × N$ vector. Note that we leave the batch dimension as $–1$ in the call to view, since in principle we don’t know how many samples will be in the batch.\n",
    "\n",
    "Here we use a subclass of `nn.Module` to contain our entire model. We could also use subclasses to define new building blocks for more complex networks. Picking up on the diagram style in `chapter 6`, our network looks like the one shown in `figure 8.10`. We are making some ad hoc choices about what information to present where.\n",
    "\n",
    "<img src=\"images/08_10.png\" style=\"width:200px;\"/>\n",
    "\n",
    "Recall that the goal of classification networks typically is to compress information in the sense that we start with an image with a sizable number of pixels and compress it into (a vector of probabilities of) classes. Two things about our architecture deserve some commentary with respect to this goal:\n",
    "\n",
    "+ first, our goal is reflected by the size of our intermediate values generally shrinking—this is done by reducing the number of channels in the convolutions, by reducing the number of pixels through pooling, and by having an output dimension lower than the input dimension in the linear layers. This is a common trait of classification networks. However, in many popular architectures like the `ResNets` we saw in `chapter 2` and discuss more in `section 8.5.3`, the reduction is achieved by pooling in the spatial resolution, but the number of channels increases (still resulting in a reduction in size). It seems that our pattern of fast information reduction works well with networks of limited depth and small images; but for deeper networks, the decrease is typically slower.\n",
    "\n",
    "+ second, in one layer, there is not a reduction of output size with regard to input size: the initial convolution. If we consider a single output pixel as a vector of $32$ elements (the channels), it is a linear transformation of $27$ elements (as a convolution of $3-\\textit{channels} × 3 × 3$ kernel size)—only a moderate increase. In `ResNet`, the initial convolution generates $64$ channels from $147$ elements ($3-\\textit{channels} × 7 × 7$ kernel size). So the first layer is exceptional in that it greatly increases the overall dimension (as in channels times pixels) of the data flowing through it, but the mapping for each output pixel considered in isolation still has approximately as many outputs as inputs.\n",
    "\n",
    "### 8.3.2 How PyTorch keeps track of parameters and submodules\n",
    "Interestingly, assigning an instance of `nn.Module` to an attribute in an `nn.Module`, as we did in the earlier constructor, automatically registers the module as a submodule.\n",
    "\n",
    "> __NOTE__\n",
    "> \n",
    "> The submodules must be top-level attributes, not buried inside list or dict instances! Otherwise the optimizer will not be able to locate the submodules (and, hence, their parameters). For situations where your model requires a list or dict of submodules, `PyTorch` provides `nn.ModuleList` and `nn.ModuleDict`.\n",
    "\n",
    "We can call arbitrary methods of an `nn.Module` subclass. For example, for a model where training is substantially different than its use, say, for prediction, it may make sense to have a `predict` method. Be aware that calling such methods will be similar to calling `forward` instead of the module itself—they will be ignorant of hooks, and the JIT does not see the module structure when using them because we are missing the equivalent of the `__call__` bits shown in `section 6.2.1`.\n",
    "\n",
    "This allows `Net` to have access to the parameters of its submodules without further action by the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "numel_list = [p.numel() for p in model.parameters()] \n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here is that the `parameters()` call delves into all submodules assigned as attributes in the constructor and recursively calls `parameters()` on them. No matter how nested the submodule, any `nn.Module` can access the list of all child parameters. By accessing their `grad` attribute, which has been populated by `autograd`, the optimizer will know how to change parameters to minimize the loss. We know that story from `chapter 5`.\n",
    "\n",
    "We now know how to implement our own modules—and we will need this a lot for `part 2`. Looking back at the implementation of the `Net` class, and thinking about the utility of registering submodules in the constructor so that we can access their parameters, it appears a bit of a waste that we are also registering submodules that have no parameters, like `nn.Tanh` and `nn.MaxPool2d`. Wouldn’t it be easier to call these directly in the forward function, just as we called view?\n",
    "\n",
    "### 8.3.3 The functional API\n",
    "It sure would! And that’s why `PyTorch` has functional counterparts for every `nn` module. By \"functional\" here we mean \"having no internal state\"—in other words, \"whose output value is solely and fully determined by the value input arguments\". Indeed, `torch.nn.functional` provides many functions that work like the modules we find in `nn`. But instead of working on the input arguments and stored parameters like the module counterparts, they take inputs and parameters as arguments to the function call. For instance, the functional counterpart of `nn.Linear` is `nn.functional.linear`, which is a function that has signature `linear(input, weight, bias=None)`. The `weight` and `bias` parameters are arguments to the function.\n",
    "\n",
    "Back to our model, it makes sense to keep using `nn` modules for `nn.Linear` and `nn.Conv2d` so that `Net` will be able to manage their `Parameters` during training. However, we can safely switch to the functional counterparts of pooling and activation, since they have no parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot more concise than and fully equivalent to our previous definition of `Net` in `section 8.3.1`. Note that it would still make sense to instantiate modules that require several parameters for their initialization in the constructor.\n",
    "\n",
    "Thus, the functional way also sheds light on what the `nn.Module` API is all about: a `Module` is a container for state in the forms of `Parameters` and submodules combined with the instructions to do a forward.\n",
    "\n",
    "Whether to use the functional or the modular API is a decision based on style and taste. When part of a network is so simple that we want to use `nn.Sequential`, we’re in the modular realm. When we are writing our own forwards, it may be more natural to use the functional interface for things that do not need state in the form of parameters.\n",
    "\n",
    "In `chapter 15`, we will briefly touch on quantization. Then stateless bits like activations suddenly become stateful because information about the quantization needs to be captured. This means if we aim to quantize our model, it might be worthwhile to stick with the modular API if we go for non-JITed quantization. There is one style matter that will help you avoid surprises with (originally unforeseen) uses: if you need several applications of stateless modules (like `nn.HardTanh` or `nn.ReLU`), it is probably a good idea to have a separate instance for each. Reusing the same module appears to be clever and will give correct results with our standard Python usage here, but tools analyzing your model may trip over it.\n",
    "\n",
    "So now we can make our own `nn.Module` if we need to, and we also have the functional API for cases when instantiating and then calling an `nn.Module` is overkill. This has been the last bit missing to understand how the code organization works in just about any neural network implemented in `PyTorch`.\n",
    "\n",
    "Let’s double-check that our model runs, and then we’ll get to the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1829, -0.0383]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net() \n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got two numbers! Information flows correctly. We might not realize it right now, but in more complex models, getting the size of the first linear layer right is sometimes a source of frustration. We’ve heard stories of famous practitioners putting in arbitrary numbers and then relying on error messages from `PyTorch` to backtrack the correct sizes for their linear layers. Lame, eh? Nah, it’s all legit!\n",
    "\n",
    "\n",
    "## 8.4 Training our convnet\n",
    "We’re now at the point where we can assemble our complete training loop. We already developed the overall structure in `chapter 5`, and the training loop looks much like the one from `chapter 6`, but here we will revisit it to add some details like some tracking for accuracy. After we run our model, we will also have an appetite for a little more speed, so we will learn how to run our models fast on a GPU. But first let’s look at the training loop.\n",
    "\n",
    "Recall that the core of our convnet is two nested loops: an outer one over the epochs and an inner one of the DataLoader that produces batches from our Dataset. In each loop, we then have to:\n",
    "1. Feed the inputs through the model (the forward pass).\n",
    "\n",
    "2. Compute the loss (also part of the forward pass).\n",
    "\n",
    "3. Zero any old gradients.\n",
    "\n",
    "4. Call `loss.backward()` to compute the gradients of the loss with respect to all parameters (the backward pass).\n",
    "\n",
    "5. Have the optimizer take a step in toward lower loss.\n",
    "\n",
    "Also, we collect and print some information. So here is our training loop, looking almost as it does in the previous chapter—but it is good to remember what each thing is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    # Our loop over the epochs, numbered from 1 to n_epochs rather than starting at 0\n",
    "    for epoch in range(1, n_epochs + 1):  \n",
    "        loss_train = 0.0\n",
    "        # Loops over our dataset in the batches the data loader creates for us\n",
    "        for imgs, labels in train_loader: \n",
    "            # Feeds a batch through our model …\n",
    "            outputs = model(imgs)\n",
    "            # … and computes the loss we wish to minimize\n",
    "            loss = loss_fn(outputs, labels) \n",
    "            # After getting rid of the gradients from the last round …\n",
    "            optimizer.zero_grad()\n",
    "            # … performs the backward step. That is, we compute the gradients of \n",
    "            # all parameters we want the network to learn.\n",
    "            loss.backward()\n",
    "            # Updates the model\n",
    "            optimizer.step()\n",
    "            # Sums the losses we saw over the epoch. Recall that it is important \n",
    "            # to transform the loss to a Python number with .item(), to escape the gradients.\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                # Divides by the length of the training data loader to get the \n",
    "                # average loss per batch. This is a much more intuitive measure than the sum.\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Dataset from `chapter 7`; wrap it into a `DataLoader`; instantiate our network, an optimizer, and a loss function as before; and call our training loop.\n",
    "\n",
    "The substantial changes in our model from the last chapter are that now our model is a custom subclass of `nn.Module` and that we’re using convolutions. Let’s run training for $100$ epochs while printing the loss. Depending on your hardware, this may take $20$ minutes or more to finish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-12 19:13:02.710809 Epoch 1, Training loss 0.592625346153405\n",
      "2020-10-12 19:13:37.733347 Epoch 10, Training loss 0.34051155398605737\n",
      "2020-10-12 19:14:16.149562 Epoch 20, Training loss 0.29870195164801966\n",
      "2020-10-12 19:14:54.988027 Epoch 30, Training loss 0.2710364457148655\n",
      "2020-10-12 19:15:32.853346 Epoch 40, Training loss 0.25268748032439287\n",
      "2020-10-12 19:16:11.148557 Epoch 50, Training loss 0.2340328470347034\n",
      "2020-10-12 19:16:49.586016 Epoch 60, Training loss 0.21912603972444109\n",
      "2020-10-12 19:17:28.393614 Epoch 70, Training loss 0.20204851458406753\n",
      "2020-10-12 19:18:07.023305 Epoch 80, Training loss 0.18943334067133583\n",
      "2020-10-12 19:18:46.017025 Epoch 90, Training loss 0.1753200623355094\n",
      "2020-10-12 19:19:25.040515 Epoch 100, Training loss 0.1609509041544738\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can train our network. But again, our friend the bird watcher will likely not be impressed when we tell her that we trained to very low training loss.\n",
    "\n",
    "### 8.4.1 Measuring accuracy\n",
    "In order to have a measure that is more interpretable than the loss, we can take a look at our accuracies on the training and validation datasets. We use the same code as in `chapter 7`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.94\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "all_acc_dict = collections.OrderedDict()\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # We do not want gradients here, as we will not want to update the parameters.\n",
    "        with torch.no_grad():  # <1>\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                # Gives us the index of the highest value as output\n",
    "                _, predicted = torch.max(outputs, dim=1) \n",
    "                # Counts the number of examples, so total is increased by the batch size\n",
    "                total += labels.shape[0]  \n",
    "                # Comparing the predicted class that had the maximum probability and the \n",
    "                # ground-truth labels, we first get a Boolean array. Taking the sum gives \n",
    "                # the number of items in the batch where the prediction and ground truth agree.\n",
    "                correct += int((predicted == labels).sum())  # <4>\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cast to a Python `int`—for `integer` tensors, this is equivalent to using `.item()`, similar to what we did in the training loop.\n",
    "\n",
    "This is quite a lot better than the fully connected model, which achieved only $79\\%$ accuracy. We about halved the number of errors on the validation set. Also, we used far fewer parameters. This is telling us that the model does a better job of generalizing its task of recognizing the subject of images from a new sample, through locality and translation invariance. We could now let it run for more epochs and see what performance we could squeeze out.\n",
    "\n",
    "### 8.4.2 Saving and loading our model\n",
    "Since we’re satisfied with our model so far, it would be nice to actually save it, right? It’s easy to do. Let’s save the model to a file:\n",
    "\n",
    "```python\n",
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')\n",
    "```\n",
    "\n",
    "The `birds_vs_airplanes.pt` file now contains all the parameters of model: that is, weights and biases for the two convolution modules and the two linear modules. So, no structure—just the weights. This means when we deploy the model in production for our friend, we’ll need to keep the model class handy, create an instance, and then load the parameters back into it:\n",
    "\n",
    "```python\n",
    "loaded_model = Net() \n",
    "loaded_model.load_state_dict(torch.load(data_path + 'birds_vs_airplanes.pt'))\n",
    "```\n",
    "\n",
    "We have also included a pretrained model in our code repository, saved to `../data/ p1ch7/birds_vs_airplanes.pt`.\n",
    "\n",
    "### 8.4.3 Training on the GPU\n",
    "We have a net and can train it! But it would be good to make it a bit faster. It is no surprise by now that we do so by moving our training onto the GPU. Using the `.to` method we saw in `chapter 3`, we can move the tensors we get from the data loader to the GPU, after which our computation will automatically take place there. But we also need to move our parameters to the GPU. Happily, `nn.Module` implements a `.to` function that moves all of its parameters to the GPU (or casts the type when you pass a `dtype` argument).\n",
    "\n",
    "There is a somewhat subtle difference between `Module.to` and `Tensor.to`. `Module.to` is in place: the module instance is modified. But `Tensor.to` is out of place (in some ways computation, just like `Tensor.tanh`), returning a new tensor. One implication is that it is good practice to create the `Optimizer` after moving the parameters to the appropriate device.\n",
    "\n",
    "It is considered good style to move things to the GPU if one is available. A good pattern is to set the a variable device depending on `torch.cuda.is_available`:\n",
    "\n",
    "```python\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')) \n",
    "print(f\"Training on device {device}.\")\n",
    "```\n",
    "\n",
    "Then we can amend the training loop by moving the tensors we get from the data loader to the GPU by using the `Tensor.to` method. Note that the code is exactly like our first version at the beginning of this section except for the two lines moving the inputs to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same amendment must be made to the validate function. We can then instantiate our model, move it to device, and run it as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-12 19:26:46.260180 Epoch 1, Training loss 0.5710921585559845\n",
      "2020-10-12 19:27:20.746007 Epoch 10, Training loss 0.3327562434088652\n",
      "2020-10-12 19:27:59.870829 Epoch 20, Training loss 0.29132924765158613\n",
      "2020-10-12 19:28:38.435039 Epoch 30, Training loss 0.27073171848704103\n",
      "2020-10-12 19:29:17.059968 Epoch 40, Training loss 0.25181285766469447\n",
      "2020-10-12 19:29:55.666718 Epoch 50, Training loss 0.2331376377564327\n",
      "2020-10-12 19:30:35.992690 Epoch 60, Training loss 0.21757347549602485\n",
      "2020-10-12 19:31:14.309925 Epoch 70, Training loss 0.2018082511557895\n",
      "2020-10-12 19:31:53.303380 Epoch 80, Training loss 0.1865134483120244\n",
      "2020-10-12 19:32:30.892165 Epoch 90, Training loss 0.1743557083711123\n",
      "2020-10-12 19:33:08.897599 Epoch 100, Training loss 0.1618964498874488\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "# Moves our model (all parameters) to the GPU. If you forget to move \n",
    "# either the model or the inputs to the GPU, you will get errors about \n",
    "# tensors not being on the same device, because the PyTorch operators \n",
    "# do not support mixing GPU and CPU inputs.\n",
    "model = Net().to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for our small network here, we do see a sizable increase in speed. The advantage of computing on GPUs is more visible for larger models.\n",
    "\n",
    "There is a slight complication when loading network weights: `PyTorch` will attempt to load the weight to the same device it was saved from—that is, weights on the GPU will be restored to the GPU. As we don’t know whether we want the same device, we have two options: we could move the network to the CPU before saving it, or move it back after restoring. It is a bit more concise to instruct `PyTorch` to override the device information when loading weights. This is done by passing the `map_location` keyword argument to `torch.load`:\n",
    "\n",
    "```python\n",
    "loaded_model = Net().to(device=device) \n",
    "loaded_model.load_state_dict(torch.load(data_path+'birds_vs_airplanes.pt', map_location=device))\n",
    "```\n",
    "\n",
    "\n",
    "## 8.5 Model design\n",
    "We built our model as a subclass of nn.Module, the de facto standard for all but the simplest models. Then we trained it successfully and saw how to use the GPU to train our models. We’ve reached the point where we can build a feed-forward convolutional neural network and train it successfully to classify images. The natural question is, what now? What if we are presented with a more complicated problem? Admittedly, our birds versus airplanes dataset wasn’t that complicated: the images were very small, and the object under investigation was centered and took up most of the viewport.\n",
    "\n",
    "If we moved to, say, ImageNet, we would find larger, more complex images, where the right answer would depend on multiple visual clues, often hierarchically organized. For instance, when trying to predict whether a dark brick shape is a remote control or a cell phone, the network could be looking for something like a screen.\n",
    "\n",
    "Plus images may not be our sole focus in the real world, where we have tabular data, sequences, and text. The promise of neural networks is sufficient flexibility to solve problems on all these kinds of data given the proper architecture (that is, the interconnection of layers or modules) and the proper loss function.\n",
    "\n",
    "`PyTorch` ships with a very comprehensive collection of modules and loss functions to implement state-of-the-art architectures ranging from feed-forward components to `long short-term memory` (`LSTM`) modules and `transformer` networks (two very popular architectures for sequential data). Several models are available through `PyTorch Hub` or as part of `torchvision` and other vertical community efforts.\n",
    "\n",
    "We’ll see a few more advanced architectures in part 2, where we’ll walk through an end-to-end problem of analyzing CT scans, but in general, it is beyond the scope of this book to explore variations on neural network architectures. However, we can build on the knowledge we’ve accumulated thus far to understand how we can implement almost any architecture thanks to the expressivity of `PyTorch`. The purpose of this section is precisely to provide conceptual tools that will allow us to read the latest research paper and start implementing it in `PyTorch`—or, since authors often release `PyTorch` implementations of their papers, to read the implementations without choking on our coffee.\n",
    "\n",
    "### 8.5.1 Adding memory capacity: Width\n",
    "Given our feed-forward architecture, there are a couple of dimensions we’d likely want to explore before getting into further complications. The first dimension is the width of the network: the number of neurons per layer, or channels per convolution. We can make a model wider very easily in `PyTorch`. We just specify a larger number of output channels in the first convolution and increase the subsequent layers accordingly, taking care to change the `forward` function to reflect the fact that we’ll now have a longer vector once we switch to fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(16 * 8 * 8, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to avoid hardcoding numbers in the definition of the model, we can easily pass a parameter to `init` and parameterize the width, taking care to also parameterize the call to `view` in the forward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(torch.nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = torch.nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-13 08:46:04.010523 Epoch 1, Training loss 0.5536910754859827\n",
      "2020-10-13 08:47:15.219878 Epoch 10, Training loss 0.31736865089197824\n",
      "2020-10-13 08:48:32.365175 Epoch 20, Training loss 0.2746933828683416\n",
      "2020-10-13 08:49:51.194793 Epoch 30, Training loss 0.245982843409678\n",
      "2020-10-13 08:51:12.048748 Epoch 40, Training loss 0.21997745809661354\n",
      "2020-10-13 08:52:30.819458 Epoch 50, Training loss 0.19058751879604\n",
      "2020-10-13 08:53:48.667805 Epoch 60, Training loss 0.1696906492683538\n",
      "2020-10-13 08:55:06.120170 Epoch 70, Training loss 0.14395841339211554\n",
      "2020-10-13 08:56:23.317404 Epoch 80, Training loss 0.12814667683308292\n",
      "2020-10-13 08:57:40.141537 Epoch 90, Training loss 0.10700947195766078\n",
      "2020-10-13 08:58:57.015642 Epoch 100, Training loss 0.08990365778137545\n",
      "Accuracy train: 0.92\n",
      "Accuracy val: 0.85\n"
     ]
    }
   ],
   "source": [
    "model = NetWidth(n_chans1=32).to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "all_acc_dict[\"width\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers specifying channels and features for each layer are directly related to the number of parameters in a model; all other things being equal, they increase the capacity of the model. As we did previously, we can look at how many parameters our model has now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38386"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater the capacity, the more variability in the inputs the model will be able to manage; but at the same time, the more likely overfitting will be, since the model can use a greater number of parameters to memorize unessential aspects of the input. We already went into ways to combat overfitting, the best being increasing the sample size or, in the absence of new data, augmenting existing data through artificial modifications of the same data.\n",
    "\n",
    "There are a few more tricks we can play at the model level (without acting on the data) to control overfitting. Let’s review the most common ones.\n",
    "\n",
    "### 8.5.2 Helping our model to converge and generalize: Regularization\n",
    "Training a model involves two critical steps: \n",
    "+ **optimization**, when we need the loss to decrease on the training set\n",
    "+ **generalization**, when the model has to work not only on the training set but also on data it has not seen before, like the validation set\n",
    "\n",
    "The mathematical tools aimed at easing these two steps are sometimes subsumed under the label ***regularization***.\n",
    "\n",
    "##### KEEPING THE PARAMETERS IN CHECK: WEIGHT PENALTIES\n",
    "The first way to stabilize generalization is to add a regularization term to the loss. This term is crafted so that the weights of the model tend to be small on their own, limiting how much training makes them grow. In other words, it is a penalty on larger weight values. This makes the loss have a smoother topography, and there’s relatively less to gain from fitting individual samples.\n",
    "\n",
    "The most popular regularization terms of this kind are $L2$ regularization, which is the sum of squares of all weights in the model, and $L1$ regularization, which is the sum of the absolute values of all weights in the model. Both of them are scaled by a (small) factor, which is a hyperparameter we set prior to training.\n",
    "\n",
    "$L2$ regularization is also referred to as weight decay. The reason for this name is that, thinking about SGD and backpropagation, the negative gradient of the $L2$ regularization term with respect to a parameter $w\\_i$ is $-2*\\textit{lambda} * w\\_i$, where lambda is the aforementioned hyperparameter, simply named weight decay in `PyTorch`. So, adding $L2$ regularization to the loss function is equivalent to decreasing each weight by an amount proportional to its current value during the optimization step (hence, the name weight decay). Note that weight decay applies to all parameters of the network, such as biases.\n",
    "\n",
    "In `PyTorch`, we could implement regularization pretty easily by adding a term to the loss. After computing the loss, whatever the loss function is, we can iterate the parameters of the model, sum their respective square (for $L2$) or `abs` (for $L1$), and backpropagate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())  # <1>\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the `SGD` optimizer in `PyTorch` already has a `weight_decay` parameter that corresponds to $2 * \\textit{lambda}$, and it directly performs weight decay during the update as described previously. It is fully equivalent to adding the $L2$ norm of weights to the loss, without the need for accumulating terms in the loss and involving autograd.\n",
    "\n",
    "##### NOT RELYING TOO MUCH ON A SINGLE INPUT: DROPOUT\n",
    "An effective strategy for combating overfitting was originally proposed in 2014 by `Nitish Srivastava` and coauthors from `Geoff Hinton`’s group in Toronto, in a paper aptly entitled [Dropout: a Simple Way to Prevent Neural Networks from Overfitting](http://mng.bz/nPMa). Sounds like pretty much exactly what we’re looking for, right? The idea behind dropout is indeed simple: zero out a random fraction of outputs from neurons across the network, where the randomization happens at each training iteration.\n",
    "\n",
    "This procedure effectively generates slightly different models with different neuron topologies at each iteration, giving neurons in the model less chance to coordinate in the memorization process that happens during overfitting. An alternative point of view is that dropout perturbs the features being generated by the model, exerting an effect that is close to augmentation, but this time throughout the network.\n",
    "\n",
    "In `PyTorch`, we can implement dropout in a model by adding an `nn.Dropout` module between the nonlinear activation function and the linear or convolutional module of the subsequent layer. As an argument, we need to specify the probability with which inputs will be zeroed out. In case of convolutions, we’ll use the specialized `nn.Dropout2d` or `nn.Dropout3d`, which zero out entire channels of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(torch.nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = torch.nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = torch.nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = torch.nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = torch.nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = torch.nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-13 09:55:51.724050 Epoch 1, Training loss 0.5798109513559159\n",
      "2020-10-13 09:57:02.181025 Epoch 10, Training loss 0.376686789617417\n",
      "2020-10-13 09:58:20.303216 Epoch 20, Training loss 0.34357539056592684\n",
      "2020-10-13 09:59:42.102478 Epoch 30, Training loss 0.3275296620692417\n",
      "2020-10-13 10:01:00.875406 Epoch 40, Training loss 0.31438348228764385\n",
      "2020-10-13 10:02:19.698825 Epoch 50, Training loss 0.3001353777707762\n",
      "2020-10-13 10:03:37.678208 Epoch 60, Training loss 0.28627793851551736\n",
      "2020-10-13 10:04:56.124055 Epoch 70, Training loss 0.2777881635602113\n",
      "2020-10-13 10:06:14.548450 Epoch 80, Training loss 0.2645267863182505\n",
      "2020-10-13 10:07:33.657520 Epoch 90, Training loss 0.2532129621809455\n",
      "2020-10-13 10:08:51.978478 Epoch 100, Training loss 0.24521004949595518\n",
      "Accuracy train: 0.89\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetDropout(n_chans1=32).to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"dropout\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that dropout is normally active during training, while during the evaluation of a trained model in production, dropout is bypassed or, equivalently, assigned a probability equal to zero. This is controlled through the train property of the `Dropout` module. Recall that `PyTorch` lets us switch between the two modalities by calling:\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "```\n",
    "\n",
    "or:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "on any `nn.Model` subclass. The call will be automatically replicated on the submodules so that if `Dropout` is among them, it will behave accordingly in subsequent forward and backward passes.\n",
    "\n",
    "##### KEEPING ACTIVATIONS IN CHECK: BATCH NORMALIZATION\n",
    "Dropout was all the rage when, in 2015, another seminal paper was published by `Sergey Ioffe` and `Christian Szegedy` from Google, entitled [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167). The paper described a technique that had multiple beneficial effects on training: allowing us to increase the learning rate and make training less dependent on initialization and act as a regularizer, thus representing an alternative to dropout.\n",
    "\n",
    "The main idea behind batch normalization is to rescale the inputs to the activations of the network so that minibatches have a certain desirable distribution. Recalling the mechanics of learning and the role of nonlinear activation functions, this helps avoid the inputs to activation functions being too far into the saturated portion of the function, thereby killing gradients and slowing training.\n",
    "\n",
    "In practical terms, batch normalization shifts and scales an intermediate input using the mean and standard deviation collected at that intermediate location over the samples of the minibatch. The regularization effect is a result of the fact that an individual sample and its downstream activations are always seen by the model as shifted and scaled, depending on the statistics across the randomly extracted minibatch. This is in itself a form of principled augmentation. The authors of the paper suggest that using batch normalization eliminates or at least alleviates the need for dropout.\n",
    "\n",
    "Batch normalization in `PyTorch` is provided through the `nn.BatchNorm1D`, `nn.BatchNorm2d`, and `nn.BatchNorm3d` modules, depending on the dimensionality of the input. Since the aim for batch normalization is to rescale the inputs of the activations, the natural location is after the linear transformation (convolution, in this case) and the activation, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(torch.nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = torch.nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = torch.nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = torch.nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv2_batchnorm = torch.nn.BatchNorm2d(num_features=n_chans1//2)\n",
    "        self.fc1 = torch.nn.Linear(8 * 8 * n_chans1//2, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as for dropout, batch normalization needs to behave differently during training and inference. In fact, at inference time, we want to avoid having the output for a specific input depend on the statistics of the other inputs we’re presenting to the model. As such, we need a way to still normalize, but this time fixing the normalization parameters once and for all.\n",
    "\n",
    "As minibatches are processed, in addition to estimating the mean and standard deviation for the current minibatch, PyTorch also updates the running estimates for mean and standard deviation that are representative of the whole dataset, as an approximation. This way, when the user specifies `model.eval()` and the model contains a batch normalization module, the running estimates are frozen and used for normalization. To unfreeze running estimates and return to using the minibatch statistics, we call `model.train()`, just as we did for dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-13 10:11:10.780695 Epoch 1, Training loss 0.4943502850972923\n",
      "2020-10-13 10:12:51.680733 Epoch 10, Training loss 0.27708669927469487\n",
      "2020-10-13 10:14:46.057962 Epoch 20, Training loss 0.2227382768586183\n",
      "2020-10-13 10:16:39.704808 Epoch 30, Training loss 0.17143371713104522\n",
      "2020-10-13 10:18:31.784975 Epoch 40, Training loss 0.13791321777993706\n",
      "2020-10-13 10:20:24.313245 Epoch 50, Training loss 0.101423488111253\n",
      "2020-10-13 10:22:17.477531 Epoch 60, Training loss 0.07281714942377464\n",
      "2020-10-13 10:24:11.874364 Epoch 70, Training loss 0.057245968746104436\n",
      "2020-10-13 10:26:06.103215 Epoch 80, Training loss 0.03647458966773976\n",
      "2020-10-13 10:28:00.405149 Epoch 90, Training loss 0.025992244928422722\n",
      "2020-10-13 10:29:53.381628 Epoch 100, Training loss 0.03234898549902022\n",
      "Accuracy train: 0.99\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"batch_norm\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3 Going deeper to learn more complex structures: Depth\n",
    "Earlier, we talked about width as the first dimension to act on in order to make a model larger and, in a way, more capable. The second fundamental dimension is obviously depth. Since this is a deep learning book, depth is something we’re supposedly into. After all, deeper models are always better than shallow ones, aren’t they? Well, it depends. With depth, the complexity of the function the network is able to approximate generally increases. In regard to computer vision, a shallower network could identify a person’s shape in a photo, whereas a deeper network could identify the person, the face on their top half, and the mouth within the face. Depth allows a model to deal with hierarchical information when we need to understand the context in order to say something about some input.\n",
    "\n",
    "There’s another way to think about depth: increasing depth is related to increasing the length of the sequence of operations that the network will be able to perform when processing input. This view—of a deep network that performs sequential operations to carry out a task—is likely fascinating to software developers who are used to thinking about algorithms as sequences of operations like \"find the person’s boundaries, look for the head on top of the boundaries, look for the mouth within the head\".\n",
    "\n",
    "##### SKIP CONNECTIONS\n",
    "Depth comes with some additional challenges, which prevented deep learning models from reaching 20 or more layers until late 2015. Adding depth to a model generally makes training harder to converge. Let’s recall backpropagation and think about it in the context of a very deep network. The derivatives of the loss function with respect to the parameters, especially those in early layers, need to be multiplied by a lot of other numbers originating from the chain of derivative operations between the loss and the parameter. Those numbers being multiplied could be small, generating ever-smaller numbers, or large, swallowing smaller numbers due to floating-point approximation. The bottom line is that a long chain of multiplications will tend to make the contribution of the parameter to the gradient vanish, leading to ineffective training of that layer since that parameter and others like it won’t be properly updated.\n",
    "\n",
    "In December 2015, `Kaiming He` and coauthors presented [residual networks](https://arxiv.org/abs/1512.03385) (`ResNets`), an architecture that uses a simple trick to allow very deep networks to be successfully trained. That work opened the door to networks ranging from tens of layers to $100$ layers in depth, surpassing the then state of the art in computer vision benchmark problems. We encountered residual networks when we were playing with pretrained models in `chapter 2`. The trick we mentioned is the following: using a skip connection to short-circuit blocks of layers, as shown in `figure 8.11`.\n",
    "\n",
    "<img src=\"images/08_11.png\" style=\"width:300px;\"/>\n",
    "\n",
    "A skip connection is nothing but the addition of the input to the output of a block of layers. This is exactly how it is done in `PyTorch`. Let’s add one layer to our simple convolutional model, and let’s use `ReLU` as the activation for a change. The vanilla module with an extra layer looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(torch.nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = torch.nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(n_chans1, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(n_chans1//2, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-13 10:32:17.947669 Epoch 1, Training loss 0.6637455228787319\n",
      "2020-10-13 10:33:18.054053 Epoch 10, Training loss 0.36201196975389105\n",
      "2020-10-13 10:34:25.563349 Epoch 20, Training loss 0.3137385354490037\n",
      "2020-10-13 10:35:33.664991 Epoch 30, Training loss 0.27355838676167143\n",
      "2020-10-13 10:36:41.461484 Epoch 40, Training loss 0.24121068446499527\n",
      "2020-10-13 10:37:49.745765 Epoch 50, Training loss 0.21660669238134556\n",
      "2020-10-13 10:38:57.777061 Epoch 60, Training loss 0.1928450659059795\n",
      "2020-10-13 10:40:06.641626 Epoch 70, Training loss 0.162196642843781\n",
      "2020-10-13 10:41:14.661880 Epoch 80, Training loss 0.13910720269581314\n",
      "2020-10-13 10:42:24.163038 Epoch 90, Training loss 0.11182258318469024\n",
      "2020-10-13 10:43:32.149078 Epoch 100, Training loss 0.09337187915518405\n",
      "Accuracy train: 0.94\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetDepth(n_chans1=32).to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"depth\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a skip connection a la `ResNet` to this model amounts to adding the output of the first layer in the forward function to the input of the third layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(torch.nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = torch.nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-13 10:54:29.643479 Epoch 1, Training loss 0.6634344177640927\n",
      "2020-10-13 10:55:30.249055 Epoch 10, Training loss 0.32874337806823145\n",
      "2020-10-13 10:56:37.259055 Epoch 20, Training loss 0.2836593065861684\n",
      "2020-10-13 10:57:43.332880 Epoch 30, Training loss 0.24903440252421008\n",
      "2020-10-13 10:58:50.024719 Epoch 40, Training loss 0.22052331619961246\n",
      "2020-10-13 10:59:57.157863 Epoch 50, Training loss 0.19334827819067962\n",
      "2020-10-13 11:01:04.245351 Epoch 60, Training loss 0.17238318640145528\n",
      "2020-10-13 11:02:12.656489 Epoch 70, Training loss 0.1407812101778331\n",
      "2020-10-13 11:03:20.568116 Epoch 80, Training loss 0.12219572914349046\n",
      "2020-10-13 11:04:28.907460 Epoch 90, Training loss 0.10291379436281077\n",
      "2020-10-13 11:05:36.940031 Epoch 100, Training loss 0.0727462557257171\n",
      "Accuracy train: 0.96\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetRes(n_chans1=32).to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"res\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we’re using the output of the first activations as inputs to the last, in addition to the standard feed-forward path. This is also referred to as *identity mapping*. So, how does this alleviate the issues with vanishing gradients we were mentioning earlier?\n",
    "\n",
    "Thinking about backpropagation, we can appreciate that a skip connection, or a sequence of skip connections in a deep network, creates a direct path from the deeper parameters to the loss. This makes their contribution to the gradient of the loss more direct, as partial derivatives of the loss with respect to those parameters have a chance not to be multiplied by a long chain of other operations.\n",
    "\n",
    "It has been observed that skip connections have a beneficial effect on convergence especially in the initial phases of training. Also, the loss landscape of deep residual networks is a lot smoother than feed-forward networks of the same depth and width.\n",
    "\n",
    "It is worth noting that skip connections were not new to the world when `ResNets` came along. `Highway` networks and `U-Net` made use of skip connections of one form or another. However, the way `ResNets` used skip connections enabled models of depths greater than $100$ to be amenable to training.\n",
    "\n",
    "Since the advent of `ResNets`, other architectures have taken skip connections to the next level. One in particular, `DenseNet`, proposed to connect each layer with several other layers downstream through skip connections, achieving state-of-the-art results with fewer parameters. By now, we know how to implement something like `DenseNets`: just arithmetically add earlier intermediate outputs to downstream intermediate outputs.\n",
    "\n",
    "##### BUILDING VERY DEEP MODELS IN PYTORCH\n",
    "We talked about exceeding $100$ layers in a convolutional neural network. How can we build that network in `PyTorch` without losing our minds in the process? The standard strategy is to define a building block, such as a $(\\textit{Conv2d}, \\textit{ReLU}, \\textit{Conv2d}) + \\textit{skip connection block}$, and then build the network dynamically in a `for` loop. Let’s see it done in practice. We will create the network depicted in `figure 8.12`.\n",
    "\n",
    "<img src=\"images/08_12.png\" style=\"width:600px;\"/>\n",
    "\n",
    "We first create a module subclass whose sole job is to provide the computation for one block—that is, one group of convolutions, activation, and skip connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        # The BatchNorm layer would cancel the effect of bias, so it is customarily left out.\n",
    "        self.conv = torch.nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)  # <1>\n",
    "        self.batch_norm = torch.nn.BatchNorm2d(num_features=n_chans)\n",
    "        # Uses custom initializations. \n",
    "        # `kaiming_normal_ initializes` with normal random elements with standard deviation as \n",
    "        # computed in the ResNet paper. \n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')  # <2>\n",
    "        # The batch norm is initialized to produce output distributions that initially \n",
    "        # have 0 mean and 0.5 variance.\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’re planning to generate a deep model, we are including batch normalization in the block, since this will help prevent gradients from vanishing during training. We’d now like to generate a $100$-block network. Does this mean we have to prepare for some serious cutting and pasting? Not at all; we already have the ingredients for imagining how this could look like.\n",
    "\n",
    "First, in init, we create `nn.Sequential` containing a list of `ResBlock` instances. `nn.Sequential` will ensure that the output of one block is used as input to the next. It will also ensure that all the parameters in the block are visible to `Net`. Then, in `forward`, we just call the sequential to traverse the $100$ blocks and generate the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(torch.nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = torch.nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = torch.nn.Sequential(*(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = torch.nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetResDeep(n_chans1=32, n_blocks=100).to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"res deep\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation, we parameterize the actual number of layers, which is important for experimentation and reuse. Also, needless to say, backpropagation will work as expected. Unsurprisingly, the network is quite a bit slower to converge. It is also more fragile in convergence. This is why we used more-detailed initializations and trained our `NetRes` with a learning rate of $3e^{-3}$ instead of the $1e^{–2}$ we used for the other networks. We trained none of the networks to convergence, but we would not have gotten anywhere without these tweaks.\n",
    "\n",
    "All this shouldn’t encourage us to seek depth on a dataset of $32 × 32$ images, but it clearly demonstrates how this can be achieved on more challenging datasets like ImageNet. It also provides the key elements for understanding existing implementations for models like `ResNet`, for instance, in `torchvision`.\n",
    "\n",
    "##### INITIALIZATION\n",
    "Let’s briefly comment about the earlier initialization. Initialization is one of the important tricks in training neural networks. Unfortunately, for historical reasons, `PyTorch` has default weight initializations that are not ideal. People are looking at fixing the situation; if progress is made, it can be tracked on [GitHub](https:// github.com/pytorch/pytorch/issues/18182). In the meantime, we need to fix the weight initialization ourselves. We found that our model did not converge and looked at what people commonly choose as initialization (a smaller variance in weights; and zero mean and unit variance outputs for batch norm), and then we halved the output variance in the batch norm when the network would not converge.\n",
    "\n",
    "Weight initialization could fill an entire chapter on its own, but we think that would be excessive. In `chapter 11`, we’ll bump into initialization again and use what arguably could be `PyTorch` defaults without much explanation. Once you’ve progressed to the point where the details of weight initialization are of specific interest to you—probably not before finishing this book—you might revisit this topic.\n",
    "\n",
    "### 8.5.4 Comparing the designs from this section\n",
    "We summarize the effect of each of our design modifications in isolation in `figure 8.13`. We should not overinterpret any of the specific numbers—our problem setup and experiments are simplistic, and repeating the experiment with different random seeds will probably generate variation at least as large as the differences in validation accuracy. For this demonstration, we left all other things equal, from learning rate to number of epochs to train; in practice, we would try to get the best results by varying those. Also, we would likely want to combine some of the additional design elements.\n",
    "\n",
    "But a qualitative observation may be in order: as we saw in` section 5.5.3`, when discussing validatioin and overfitting, The weight decay and dropout regularizations, which have a more rigorous statistical estimation interpretation as regularization than batch norm, have a much narrower gap between the two accuracies. Batch norm, which serves more as a convergence helper, lets us train the network to nearly $100\\%$ training accuracy, so we interpret the first two as regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-bad1c2031b94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrn_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_acc_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_acc_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-bad1c2031b94>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrn_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_acc_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_acc_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trn_acc = [v['train'] for k, v in all_acc_dict.items()]\n",
    "val_acc = [v['val'] for k, v in all_acc_dict.items()]\n",
    "\n",
    "width =0.3\n",
    "plt.bar(np.arange(len(trn_acc)), trn_acc, width=width, label='train')\n",
    "plt.bar(np.arange(len(val_acc))+ width, val_acc, width=width, label='val')\n",
    "plt.xticks(np.arange(len(val_acc))+ width/2, list(all_acc_dict.keys()), rotation=60)\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0.7, 1)\n",
    "plt.savefig('accuracy_comparison.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.5 It’s already outdated\n",
    "The curse and blessing of a deep learning practitioner is that neural network architectures evolve at a very rapid pace. This is not to say that what we’ve seen in this chapter is necessarily old school, but a thorough illustration of the latest and greatest architectures is a matter for another book (and they would cease to be the latest and the greatest pretty quickly anyway). The take-home message is that we should make every effort to proficiently translate the math behind a paper into actual PyTorch code, or at least understand the code that others have written with the same intention. In the last few chapters, you have hopefully gathered quite a few of the fundamental skills to translate ideas into implemented models in PyTorch.\n",
    "\n",
    "\n",
    "## 8.6 Conclusion\n",
    "After quite a lot of work, we now have a model that our fictional friend Jane can use to filter images for her blog. All we have to do is take an incoming image, crop and resize it to $32 × 32$, and see what the model has to say about it. Admittedly, we have solved only part of the problem, but it was a journey in itself.\n",
    "\n",
    "We have solved just part of the problem because there are a few interesting unknowns we would still have to face. One is picking out a bird or airplane from a larger image. Creating bounding boxes around objects in an image is something a model like ours can’t do.\n",
    "\n",
    "Another hurdle concerns what happens when Fred the cat walks in front of the camera. Our model will not refrain from giving its opinion about how bird-like the cat is! It will happily output \"airplane\" or \"bird\", perhaps with $0.99$ probability. This issue of being very confident about samples that are far from the training distribution is called *overgeneralization*. It’s one of the main problems when we take a (presumably good) model to production in those cases where we can’t really trust the input (which, sadly, is the majority of real-world cases).\n",
    "\n",
    "In this chapter, we have built reasonable, working models in `PyTorch` that can learn from images. We did it in a way that helped us build our intuition around convolutional networks. We also explored ways in which we can make our models wider and deeper, while controlling effects like overfitting. Although we still only scratched the surface, we have taken another significant step ahead from the previous chapter. We now have a solid basis for facing the challenges we’ll encounter when working on deep learning projects.\n",
    "\n",
    "Now that we’re familiar with `PyTorch` conventions and common features, we’re ready to tackle something bigger. We’re going to transition from a mode where each chapter or two presents a small problem, to spending multiple chapters breaking down a bigger, real-world problem. `Part 2` uses automatic detection of lung cancer as an ongoing example; we will go from being familiar with the `PyTorch` API to being able to implement entire projects using `PyTorch`. We’ll start in the next chapter by explaining the problem from a high level, and then we’ll get into the details of the data we’ll be using.\n",
    "\n",
    "## 8.7 Exercises\n",
    "[skip]\n",
    "\n",
    "## 8.8 Summary\n",
    "+ Convolution can be used as the linear operation of a feed-forward network dealing with images. Using convolution produces networks with fewer parameters, exploiting locality and featuring translation invariance. \n",
    "\n",
    "+ Stacking multiple convolutions with their activations one after the other, and using max pooling in between, has the effect of applying convolutions to increasingly smaller feature images, thereby effectively accounting for spatial relationships across larger portions of the input image as depth increases. \n",
    "\n",
    "+ Any `nn.Module` subclass can recursively collect and return its and its children’s parameters. This technique can be used to count them, feed them into the optimizer, or inspect their values. \n",
    "\n",
    "+ The functional API provides modules that do not depend on storing internal state. It is used for operations that do not hold parameters and, hence, are not trained. \n",
    "\n",
    "+ Once trained, parameters of a model can be saved to disk and loaded back in with one line of code each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
