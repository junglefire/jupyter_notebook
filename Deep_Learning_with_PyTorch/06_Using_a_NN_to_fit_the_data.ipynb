{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Pytorch env\n",
    "import os\n",
    "os.environ['TORCH_HOME']=\"/home/alex/data/pytorch\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision as torchv\n",
    "import torch as torch\n",
    "import PIL as pil\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Using a neural network to fit the data\n",
    "So far, we’ve taken a close look at how a linear model can learn and how to make that happen in `PyTorch`. We’ve focused on a very simple regression problem that used a linear model with only one input and one output. Such a simple example allowed us to dissect the mechanics of a model that learns, without getting overly distracted by the implementation of the model itself. As we saw in the overview diagram in `chapter 5`, `figure 5.2` (repeated here as `figure 6.1`), the exact details of a model are not needed to understand the high-level process that trains the model. Backpropagating errors to parameters and then updating those parameters by taking the gradient with respect to the loss is the same no matter what the underlying model is.\n",
    "\n",
    "<img src=\"images/05_02.png\" style=\"width:600px;\"/>\n",
    "\n",
    "In this chapter, we will make some changes to our model architecture: we’re going to implement a full artificial neural network to solve our temperature-conversion problem. We’ll continue using our training loop from the last chapter, along with our `Fahrenheit-to-Celsius` samples split into training and validation sets. We could start to use a quadratic model: rewriting model as a quadratic function of its input (for example, $y = a*x^2 + b*x + c$). Since such a model would be differentiable, `PyTorch` would take care of computing gradients, and the training loop would work as usual. That wouldn’t be too interesting for us, though, because we would still be fixing the shape of the function.\n",
    "\n",
    "This is the chapter where we begin to hook together the foundational work we’ve put in and the `PyTorch` features you’ll be using day in and day out as you work on your projects. You’ll gain an understanding of what’s going on underneath the porcelain of the `PyTorch` API, rather than it just being so much black magic. Before we get into the implementation of our new model, though, let’s cover what we mean by artificial neural network.\n",
    "\n",
    "## 6.1 Artificial neurons\n",
    "At the core of deep learning are neural networks: mathematical entities capable of representing complicated functions through a composition of simpler functions. \n",
    "\n",
    "The basic building block of these complicated functions is the `neuron`, as illustrated in `figure 6.2`. At its core, it is nothing but a linear transformation of the input (for example, multiplying the input by a number (the weight) and adding a constant (the bias)) followed by the application of a fixed nonlinear function (referred to as the *activation function8).\n",
    "\n",
    "<img src=\"images/06_02.png\" style=\"width:500px;\"/>\n",
    "\n",
    "Mathematically, we can write this out as $o = f(w * x + b)$, with $x$ as our input, $w$ our weight or scaling factor, and $b$ as our bias or offset. $f$ is our activation function, set to the hyperbolic tangent, or $tanh$ function here. In general, $x$ and, hence, $o$ can be simple scalars, or vector-valued (meaning holding many scalar values); and similarly, $w$ can be a single scalar or matrix, while $b$ is a scalar or vector (the dimensionality of the inputs and weights must match, however). In the latter case, the previous expression is referred to as a layer of neurons, since it represents many neurons via the multidimensional weights and biases.\n",
    "\n",
    "### 6.1.1 Composing a multilayer network\n",
    "A multilayer neural network, as represented in `figure 6.3`, is made up of a composition of functions like those we just discussed:\n",
    "\n",
    "\\begin{align} \n",
    "x_1 = f(w_0 * x + b_0) \\\\\n",
    "x_2 = f(w_1 * x_1 + b_1) \\\\ \n",
    "... \\\\\n",
    "y = f(w_n * x_n + b_n)\n",
    "\\end{align}\n",
    "\n",
    "where the output of a layer of neurons is used as an input for the following layer. Remember that $w\\_0$ here is a matrix, and $x$ is a vector! Using a vector allows $w\\_0$ to hold an entire layer of neurons, not just a single weight.\n",
    "\n",
    "<img src=\"images/06_03.png\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Understanding the error function\n",
    "An important difference between our earlier linear model and what we’ll actually be using for deep learning is the shape of the error function. Our linear model and error-squared loss function had a convex error curve with a singular, clearly defined minimum. If we were to use other methods, we could solve for the parameters minimizing the error function automatically and definitively. That means that our parameter updates were attempting to estimate that singular correct answer as best they could.\n",
    "\n",
    "Neural networks do not have that same property of a convex error surface, even when using the same error-squared loss function! There’s no single right answer for each parameter we’re attempting to approximate. Instead, we are trying to get all of the parameters, when acting in concert, to produce a useful output. Since that useful output is only going to approximate the truth, there will be some level of imperfection. Where and how imperfections manifest is somewhat arbitrary, and by implication the parameters that control the output (and, hence, the imperfections) are somewhat arbitrary as well. This results in neural network training looking very much like parameter estimation from a mechanical perspective, but we must remember that the theoretical underpinnings are quite different.\n",
    "\n",
    "A big part of the reason neural networks have non-convex error surfaces is due to the activation function. The ability of an ensemble of neurons to approximate a very wide range of useful functions depends on the combination of the linear and nonlinear behavior inherent to each neuron.\n",
    "\n",
    "### 6.1.3 All we need is activation\n",
    "As we have seen, the simplest unit in (deep) neural networks is a linear operation (scaling + offset) followed by an activation function. We already had our linear operation in our latest model—the linear operation was the entire model. The activation function plays two important roles:\n",
    "\n",
    "+ In the inner parts of the model, it allows the output function to have different slopes at different values—something a linear function by definition cannot do. By trickily composing these differently sloped parts for many outputs, neural networks can approximate arbitrary functions. \n",
    "\n",
    "+ At the last layer of the network, it has the role of concentrating the outputs of the preceding linear operation into a given range.\n",
    "\n",
    "Let’s talk about what the second point means. Pretend that we’re assigning a \"good doggo\" score to images. Pictures of retrievers and spaniels should have a high score, while images of airplanes and garbage trucks should have a low score. Bear pictures should have a lowish score, too, although higher than garbage trucks.\n",
    "\n",
    "The problem is, we have to define a \"high score\": we’ve got the entire range of float32 to work with, and that means we can go pretty high. Even if we say \"it’s a $10$-point scale\", there’s still the issue that sometimes our model is going to produce a score of $11$ out of $10$. Remember that under the hood, it’s all sums of $(w*x+b)$ matrix multiplications, and those won’t naturally limit themselves to a specific range of outputs.\n",
    "\n",
    "<img src=\"images/06_04.png\" style=\"width:600px;\"/>\n",
    "\n",
    "This results in garbage trucks being flagged as \"not dogs\", our good dog mapping to \"clearly a dog\", and our bear ending up somewhere in the middle. In code, we can see the exact values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9757431300314515, 0.09966799462495582, 0.9866142981514303)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage truck, Bear, Good doggo\n",
    "math.tanh(-2.2), math.tanh(0.1), math.tanh(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bear in the sensitive range, small changes to the bear will result in a noticeable change to the result. For example, we could switch from a grizzly to a polar bear (which has a vaguely more traditionally canine face) and see a jump up the $Y$-axis as we slide toward the \"very much a dog\" end of the graph. Conversely, a koala bear would register as less dog-like, and we would see a drop in the activated output. There isn’t much we could do to the garbage truck to make it register as dog-like, though: even with drastic changes, we might only see a shift from $–0.97$ to $–0.8$ or so.\n",
    "\n",
    "### 6.1.4 More activation functions\n",
    "There are quite a few activation functions, some of which are shown in `figure 6.5`. In the first column, we see the smooth functions `Tanh` and `Softplus`, while the second column has \"hard\" versions of the activation functions to their left: `Hardtanh` and `ReLU`. `ReLU` (for `rectified linear unit`) deserves special note, as it is currently considered one of the best-performing general activation functions; many state-of-the-art results have used it. The `Sigmoid` activation function, also known as the `logistic` function, was widely used in early deep learning work but has since fallen out of common use except where we explicitly want to move to the $0\\sim1$ range: for example, when the output should be a probability. Finally, the `LeakyReLU` function modifies the standard `ReLU` to have a small positive slope, rather than being strictly zero for negative inputs (typically this slope is $0.01$, but it’s shown here with slope $0.1$ for clarity).\n",
    "\n",
    "<img src=\"images/06_05.png\" style=\"width:600px;\"/>\n",
    "\n",
    "### 6.1.5 Choosing the best activation function\n",
    "Activation functions are curious, because with such a wide variety of proven successful ones (many more than shown in `figure 6.5`), it’s clear that there are few, if any, strict requirements. As such, we’re going to discuss some generalities about activation functions that can probably be trivially disproved in the specific. That said, by definition, activation functions\n",
    "\n",
    "+ Are nonlinear. Repeated applications of $(w*x+b)$ without an activation function results in a function of the same (affine linear) form. The nonlinearity allows the overall network to approximate more complex functions.\n",
    "\n",
    "+ Are differentiable, so that gradients can be computed through them. Point discontinuities, as we can see in `Hardtanh` or `ReLU`, are fine.\n",
    "\n",
    "Without these characteristics, the network either falls back to being a linear model or becomes difficult to train.\n",
    "\n",
    "The following are true for the functions:\n",
    "\n",
    "+ They have at least one sensitive range, where nontrivial changes to the input result in a corresponding nontrivial change to the output. This is needed for training.\n",
    "\n",
    "+ Many of them have an insensitive (or saturated) range, where changes to the input result in little or no change to the output.\n",
    "\n",
    "By way of example, the `Hardtanh` function could easily be used to make piecewise-linear approximations of a function by combining the sensitive range with different weights and biases on the input.\n",
    "\n",
    "Often (but far from universally so), the activation function will have at least one of these:\n",
    "\n",
    "+ A lower bound that is approached (or met) as the input goes to negative infinity \n",
    "\n",
    "+ A similar-but-inverse upper bound for positive infinity\n",
    "\n",
    "Thinking of what we know about how backpropagation works, we can figure out that the errors will propagate backward through the activation more effectively when the inputs are in the response range, while errors will not greatly affect neurons for which the input is saturated (since the gradient will be close to zero, due to the flat area around the output).\n",
    "\n",
    "Put together, all this results in a pretty powerful mechanism: we’re saying that in a network built out of linear + activation units, when different inputs are presented to the network:\n",
    "\n",
    "+ (a) different units will respond in different ranges for the same inputs\n",
    "\n",
    "+ (b) the errors associated with those inputs will primarily affect the neurons operating in the sensitive range, leaving other units more or less unaffected by the learning process\n",
    "\n",
    "In addition, thanks to the fact that derivatives of the activation with respect to its inputs are often close to 1 in the sensitive range, estimating the parameters of the linear transformation through gradient descent for the units that operate in that range will look a lot like the linear fit we have seen previously.\n",
    "\n",
    "We are starting to get a deeper intuition for how joining many linear + activation units in parallel and stacking them one after the other leads us to a mathematical object that is capable of approximating complicated functions. Different combinations of units will respond to inputs in different ranges, and those parameters for those units are relatively easy to optimize through gradient descent, since learning will behave a lot like that of a linear function until the output saturates.\n",
    "\n",
    "### 6.1.6 What learning means for a neural network\n",
    "Building models out of stacks of linear transformations followed by differentiable activations leads to models that can approximate highly nonlinear processes and whose parameters we can estimate surprisingly well through gradient descent. This remains true even when dealing with models with millions of parameters. What makes using deep neural networks so attractive is that it saves us from worrying too much about the exact function that represents our data—whether it is quadratic, piecewise polynomial, or something else. With a deep neural network model, we have a universal approximator and a method to estimate its parameters. This approximator can be customized to our needs, in terms of model capacity and its ability to model complicated input/output relationships, just by composing simple building blocks. We can see some examples of this in `figure 6.6`.\n",
    "\n",
    "<img src=\"images/06_06.png\" style=\"width:600px;\"/>\n",
    "\n",
    "The four upper-left graphs show four neurons—$A$, $B$, $C$, and $D$—each with its own (arbitrarily chosen) weight and bias. Each neuron uses the `Tanh` activation function with a min of $–1$ and a max of $1$. The varied weights and biases move the center point and change how drastically the transition from min to max happens, but they clearly all have the same general shape. The columns to the right of those show both pairs of neurons added together ($A + B$ and then $C + D$). Here, we start to see some interesting properties that mimic a single layer of neurons. $A + B$ shows a slight $S$ curve, with the extremes approaching $0$, but both a positive bump and a negative bump in the middle. Conversely, $C + D$ has only a large positive bump, which peaks at a higher value than our single-neuron max of $1$.\n",
    "\n",
    "In the third row, we begin to compose our neurons as they would be in a two-layer network. Both $C(A + B)$ and $D(A + B)$ have the same positive and negative bumps that $A + B$ shows, but the positive peak is more subtle. The composition of $C(A + B) + D(A + B)$ shows a new property: two clearly negative bumps, and possibly a very subtle second positive peak as well, to the left of the main area of interest. All this with only four neurons in two layers!\n",
    "\n",
    "Again, these neurons’ parameters were chosen only to have a visually interesting result. Training consists of finding acceptable values for these weights and biases so that the resulting network correctly carries out a task, such as predicting likely temperatures given geographic coordinates and time of the year. By carrying out a task successfully, we mean obtaining a correct output on unseen data produced by the same data-generating process used for training data. A successfully trained network, through the values of its weights and biases, will capture the inherent structure of the data in the form of meaningful numerical representations that work correctly for previously unseen data.\n",
    "\n",
    "Let’s take another step in our realization of the mechanics of learning: deep neural networks give us the ability to approximate highly nonlinear phenomena without having an explicit model for them. Instead, starting from a generic, untrained model, we specialize it on a task by providing it with a set of inputs and outputs and a loss function from which to backpropagate. Specializing a generic model to a task using examples is what we refer to as learning, because the model wasn’t built with that specific task in mind—no rules describing how that task worked were encoded in the model.\n",
    "\n",
    "For our thermometer example, we assumed that both thermometers measured temperatures linearly. That assumption is where we implicitly encoded a rule for our task: we hardcoded the shape of our input/output function; we couldn’t have approximated anything other than data points sitting around a line. As the dimensionality of a problem grows (that is, many inputs to many outputs) and input/output relationships get complicated, assuming a shape for the input/output function is unlikely to work. The job of a physicist or an applied mathematician is often to come up with a functional description of a phenomenon from first principles, so that we can estimate the unknown parameters from measurements and get an accurate model of the world. Deep neural networks, on the other hand, are families of functions that have the ability to approximate a wide range of input/output relationships without necessarily requiring us to come up with an explanatory model of a phenomenon. In a way, we’re renouncing an explanation in exchange for the possibility of tackling increasingly complicated problems. In another way, we sometimes lack the ability, information, or computational resources to build an explicit model of what we’re presented with, so data-driven methods are our only way forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 The PyTorch nn module\n",
    "All this talking about neural networks is probably making you really curious about building one from scratch with `PyTorch`. Our first step will be to replace our linear model with a neural network unit. This will be a somewhat useless step backward from a correctness perspective, since we’ve already verified that our calibration only required a linear function, but it will still be instrumental for starting on a sufficiently simple problem and scaling up later.\n",
    "\n",
    "`PyTorch` has a whole submodule dedicated to neural networks, called `torch.nn`. It contains the building blocks needed to create all sorts of neural network architectures. Those building blocks are called `modules` in `PyTorch` parlance (such building blocks are often referred to as `layers` in other frameworks). A `PyTorch` module is a Python class deriving from the `nn.Module` base class. A module can have one or more `Parameter` instances as attributes, which are tensors whose values are optimized during the training process (think $w$ and $b$ in our linear model). A module can also have one or more submodules (subclasses of `nn.Module`) as attributes, and it will be able to track their parameters as well.\n",
    "\n",
    "> **NOTE**\n",
    "> \n",
    "> The submodules must be top-level attributes, not buried inside list or dict instances! Otherwise, the optimizer will not be able to locate the submodules (and, hence, their parameters). For situations where your model requires a list or dict of submodules, `PyTorch` provides `nn.ModuleList` and `nn.ModuleDict`.\n",
    "\n",
    "Unsurprisingly, we can find a subclass of `nn.Module` called `nn.Linear`, which applies an affine transformation to its input (via the parameter attributes weight and bias) and is equivalent to what we implemented earlier in our thermometer experiments. We’ll now start precisely where we left off and convert our previous code to a form that uses `nn`.\n",
    "\n",
    "### 6.2.1 Using `__call__` rather than forward\n",
    "\n",
    "All `PyTorch`-provided subclasses of `nn.Module` have their `__call__` method defined. This allows us to instantiate an `nn.Linear` and call it as if it was a function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # <1>\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # <1>\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "t_u_train = t_u[train_indices]\n",
    "t_c_train = t_c[train_indices]\n",
    "\n",
    "t_u_val = t_u[val_indices]\n",
    "t_c_val = t_c[val_indices]\n",
    "\n",
    "t_un_train = 0.1 * t_u_train\n",
    "t_un_val = 0.1 * t_u_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2943],\n",
       "        [-0.6388]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = torch.nn.Linear(1, 1) \n",
    "linear_model(t_un_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling an instance of `nn.Module` with a set of arguments ends up calling a method named `forward` with the same arguments. The `forward` method is what executes the forward computation, while `__call__` does other rather important chores before and after calling `forward`. So, it is technically possible to call `forward` directly, and it will produce the same output as `__call__`, but this should not be done from user code:\n",
    "\n",
    "<img src=\"images/c_06_01.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Here’s the implementation of `Module._call_` (we left out the bits related to the JIT and made some simplifications for clarity; `torch/nn/modules/module.py`, line 483`):\n",
    "\n",
    "```python\n",
    "def __call__(self, *input, **kwargs):\n",
    "    for hook in self._forward_pre_hooks.values():\n",
    "        hook(self, input)\n",
    "\n",
    "    result = self.forward(*input, **kwargs)\n",
    "    for hook in self._forward_hooks.values():\n",
    "        hook_result = hook(self, input, result)\n",
    "    # ...\n",
    "    for hook in self._backward_hooks.values():\n",
    "    # ...\n",
    "    return result\n",
    "```\n",
    "\n",
    "As we can see, there are a lot of hooks that won’t get called properly if we just use `.forward()` directly.\n",
    "\n",
    "### 6.2.2 Returning to the linear model\n",
    "Back to our linear model. The constructor to `nn.Linear` accepts three arguments: \n",
    "+ the number of input features\n",
    "+ the number of output features\n",
    "+ and whether the linear model includes a bias or not (defaulting to `True`, here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2748],\n",
       "        [-1.3076]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = torch.nn.Linear(1, 1) \n",
    "linear_model(t_un_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features in our case just refers to the size of the input and the output tensor for the module, so $1$ and $1$. If we used both temperature and barometric pressure as input, for instance, we would have two features in input and one feature in output. As we will see, for more complex models with several intermediate modules, the number of features will be associated with the capacity of the model.\n",
    "\n",
    "We have an instance of `nn.Linear` with one input and one output feature. That only requires one weight and one bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7259]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.2749], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the module with some input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4510], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1) \n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although `PyTorch` lets us get away with it, we don’t actually provide an input with the right dimensionality. We have a model that takes one input and produces one output, but `PyTorch` `nn.Module` and its subclasses are designed to do so on multiple samples at the same time. To accommodate multiple samples, modules expect the zeroth dimension of the input to be the number of samples in the batch. We encountered this concept in `chapter 4`, when we learned how to arrange real-world data into tensors.\n",
    "\n",
    "##### BATCHING INPUTS\n",
    "Any module in `nn` is written to produce outputs for a batch of multiple inputs at the same time. Thus, assuming we need to run `nn.Linear` on $10$ samples, we can create an input tensor of size $B × \\textit{Nin}$, where $B$ is the size of the batch and $\\textit{Nin}$ is the number of input features, and run it once through the model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4510],\n",
       "        [-0.4510],\n",
       "        [-0.4510],\n",
       "        [-0.4510],\n",
       "        [-0.4510],\n",
       "        [-0.4510],\n",
       "        [-0.4510],\n",
       "        [-0.4510],\n",
       "        [-0.4510],\n",
       "        [-0.4510]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, 1) \n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s dig into what’s going on here, with `figure 6.7` showing a similar situation with batched image data. Our input is $B × C × H × W$ with a batch size of $3$ (say, images of a dog, a bird, and then a car), three channel dimensions (red, green, and blue), and an unspecified number of pixels for height and width. As we can see, the output is a tensor of size $B × \\textit{Nout}$, where $\\textit{Nout}$ is the number of output features: four, in this case.\n",
    "\n",
    "<img src=\"images/06_07.png\" style=\"width:600px;\"/>\n",
    "\n",
    "##### OPTIMIZING BATCHES\n",
    "The reason we want to do this batching is multifaceted. One big motivation is to make sure the computation we’re asking for is big enough to saturate the computing resources we’re using to perform the computation. GPUs in particular are highly parallelized, so a single input on a small model will leave most of the computing units idle. By providing batches of inputs, the calculation can be spread across the otherwise-idle units, which means the batched results come back just as quickly as a single result would. Another benefit is that some advanced models use statistical information from the entire batch, and those statistics get better with larger batch sizes.\n",
    "\n",
    "Back to our thermometer data, $t\\_u$ and $t\\_c$ were two 1D tensors of size $B$. Thanks to broadcasting, we could write our linear model as $w * x + b$, where $w$ and $b$ were two scalar parameters. This worked because we had a single input feature: if we had two, we would need to add an extra dimension to turn that 1D tensor into a matrix with samples in the rows and features in the columns.\n",
    "\n",
    "That’s exactly what we need to do to switch to using `nn.Linear`. We reshape our $B$ inputs to $B × \\textit{Nin}$, where $\\textif{Nin}$ is $1$. That is easily done with `unsqueeze`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 1]), torch.Size([11, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) \n",
    "t_u = torch.tensor(t_u).unsqueeze(1)\n",
    "\n",
    "t_c.shape, t_u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re done; let’s update our training code. First, we replace our handmade model with `nn.Linear(1,1)`, and then we need to pass the linear model parameters to the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a redefinition from earlier.\n",
    "linear_model = torch.nn.Linear(1, 1) \n",
    "# This method call replaces [params]\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, it was our responsibility to create parameters and pass them as the first argument to `optim.SGD`. Now we can use the parameters method to ask any `nn.Module` for a list of parameters owned by it or any of its submodules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fc44188d830>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.5625]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.7066], requires_grad=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call recurses into submodules defined in the module’s init constructor and returns a flat list of all parameters encountered, so that we can conveniently pass it to the optimizer constructor as we did previously.\n",
    "\n",
    "We can already figure out what happens in the training loop. The optimizer is provided with a list of tensors that were defined with `requires_grad = True`—all `Parameters` are defined this way by definition, since they need to be optimized by gradient descent. When `training_loss.backward()` is called, grad is accumulated on the leaf nodes of the graph, which are precisely the parameters that were passed to the optimizer.\n",
    "\n",
    "At this point, the `SGD` optimizer has everything it needs. When `optimizer.step()` is called, it will iterate through each `Parameter` and change it by an amount proportional to what is stored in its grad attribute. Pretty clean design.\n",
    "\n",
    "Let’s take a look a the training loop now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val,  t_c_train, t_c_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p_train = model(t_u_train) # <1>\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        t_p_val = model(t_u_val) # <1>\n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward() # <2>\n",
    "        optimizer.step()\n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
    "                  f\" Validation loss {loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 290.1517, Validation loss 54.5854\n",
      "Epoch 1000, Training loss 5.3850, Validation loss 2.4140\n",
      "Epoch 2000, Training loss 3.2639, Validation loss 1.4742\n",
      "Epoch 3000, Training loss 3.0679, Validation loss 2.7517\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[5.5357]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-18.2757], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "linear_model = torch.nn.Linear(1, 1) # <1>\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 3000, \n",
    "    optimizer = optimizer,\n",
    "    model = linear_model,\n",
    "    loss_fn = loss_fn,\n",
    "    t_u_train = t_un_train,\n",
    "    t_u_val = t_un_val, \n",
    "    t_c_train = t_c_train,\n",
    "    t_c_val = t_c_val)\n",
    "\n",
    "print()\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It hasn’t changed practically at all, except that now we don’t pass params explicitly to model since the model itself holds its `Parameters` internally.\n",
    "\n",
    "There’s one last bit that we can leverage from `torch.nn`: the loss. Indeed, `nn` comes with several common loss functions, among them `nn.MSELoss` (`MSE` stands for `Mean Square Error`), which is exactly what we defined earlier as our `loss_fn`. Loss functions in `nn` are still subclasses of `nn.Module`, so we will create an instance and call it as a function. In our case, we get rid of the handwritten `loss_fn` and replace it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 236.3707, Validation loss 42.9848\n",
      "Epoch 1000, Training loss 5.0399, Validation loss 1.9233\n",
      "Epoch 2000, Training loss 3.2320, Validation loss 1.5793\n",
      "Epoch 3000, Training loss 3.0649, Validation loss 2.8071\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[5.5431]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-18.3195], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear_model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 3000, \n",
    "    optimizer = optimizer,\n",
    "    model = linear_model,\n",
    "    # We are no longer using our handwritten loss function from earlier.\n",
    "    loss_fn = torch.nn.MSELoss(), # <1>\n",
    "    t_u_train = t_un_train,\n",
    "    t_u_val = t_un_val, \n",
    "    t_c_train = t_c_train,\n",
    "    t_c_val = t_c_val)\n",
    "\n",
    "print()\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else input into our training loop stays the same. Even our results remain the same as before. Of course, getting the same results is expected, as a difference would imply a bug in one of the two implementations.\n",
    "\n",
    "\n",
    "## 6.3 Finally a neural network\n",
    "It’s been a long journey—there has been a lot to explore for these $20$-something lines of code we require to define and train a model. Hopefully by now the magic involved in training has vanished and left room for the mechanics. What we learned so far will allow us to own the code we write instead of merely poking at a black box when things get more complicated.\n",
    "\n",
    "There’s one last step left to take: replacing our linear model with a neural network as our approximating function. We said earlier that using a neural network will not result in a higher-quality model, since the process underlying our calibration problem was fundamentally linear. However, it’s good to make the leap from linear to neural network in a controlled environment so we won’t feel lost later.\n",
    "\n",
    "### 6.3.1 Replacing the linear model\n",
    "We are going to keep everything else fixed, including the loss function, and only redefine model. Let’s build the simplest possible neural network: a linear module, followed by an activation function, feeding into another linear module. The first linear + activation layer is commonly referred to as a hidden layer for historical reasons, since its outputs are not observed directly but fed into the output layer. While the input and output of the model are both of size $1$ (they have one input and one output feature), the size of the output of the first linear module is usually larger than $1$. Recalling our earlier explanation of the role of activations, this can lead different units to respond to different ranges of the input, which increases the capacity of our model. The last linear layer will take the output of activations and combine them linearly to produce the output value.\n",
    "\n",
    "There is no standard way to depict neural networks. `Figure 6.8` shows two ways that seem to be somewhat prototypical: the left side shows how our network might be depicted in basic introductions, whereas a style similar to that on the right is often used in the more advanced literature and research papers. It is common to make diagram blocks that roughly correspond to the neural network modules `PyTorch` offers (though sometimes things like the `Tanh` activation layer are not explicitly shown). Note that one somewhat subtle difference between the two is that the graph on the left has the inputs and (intermediate) results in the circles as the main elements. On the right, the computational steps are more prominent.\n",
    "\n",
    "<img src=\"images/06_08.png\" style=\"width:600px;\"/>\n",
    "\n",
    "`nn` provides a simple way to concatenate modules through the `nn.Sequential` container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=13, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = torch.nn.Sequential(\n",
    "    # We chose 13 arbitrarily. We wanted a number that was a different size from the \n",
    "    # other tensor shapes we have floating around.\n",
    "    torch.nn.Linear(1, 13), \n",
    "    torch.nn.Tanh(), \n",
    "    # This 13 must match the first size, however.\n",
    "    torch.nn.Linear(13, 1)) \n",
    "seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result is a model that takes the inputs expected by the first module specified as an argument of `nn.Sequential`, passes intermediate outputs to subsequent modules, and produces the output returned by the last module. The model fans out from $1$ input feature to $13$ hidden features, passes them through a `tanh` activation, and linearly combines the resulting $13$ numbers into $1$ output feature.\n",
    "\n",
    "### 6.3.2 Inspecting the parameters\n",
    "Calling `model.parameters()` will collect weight and bias from both the first and second linear modules. It’s instructive to inspect the parameters in this case by printing their shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([13, 1]), torch.Size([13]), torch.Size([1, 13]), torch.Size([1])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.shape for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the tensors that the optimizer will get. Again, after we call `model.backward()`, all parameters are populated with their `grad`, and the optimizer then updates their values accordingly during the `optimizer.step()` call. Not that different from our previous linear model, eh? After all, they’re both differentiable models that can be trained using gradient descent.\n",
    "\n",
    "A few notes on parameters of `nn.Modules`. When inspecting parameters of a model made up of several submodules, it is handy to be able to identify parameters by name. There’s a method for that, called `named_parameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([13, 1])\n",
      "0.bias torch.Size([13])\n",
      "2.weight torch.Size([1, 13])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of each module in `Sequential` is just the ordinal with which the module appears in the arguments. Interestingly, `Sequential` also accepts an `OrderedDict`, in which we can name each module passed to `Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (hidden_linear): Linear(in_features=1, out_features=8, bias=True)\n",
       "  (hidden_activation): Tanh()\n",
       "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "seq_model = torch.nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('hidden_linear', torch.nn.Linear(1, 8)), \n",
    "        ('hidden_activation', torch.nn.Tanh()), \n",
    "        ('output_linear', torch.nn.Linear(8, 1))]))\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to get more explanatory names for submodules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight torch.Size([8, 1])\n",
      "hidden_linear.bias torch.Size([8])\n",
      "output_linear.weight torch.Size([1, 8])\n",
      "output_linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more descriptive; but it does not give us more flexibility in the flow of data through the network, which remains a purely sequential pass-through—the `nn.Sequential` is very aptly named. We will see how to take full control of the processing of input data by subclassing `nn.Module` ourselves in `chapter 8`.\n",
    "\n",
    "We can also access a particular `Parameter` by using submodules as attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1161], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful for inspecting parameters or their gradients: for instance, to monitor gradients during training, as we did at the beginning of this chapter. Say we want to print out the gradients of weight of the linear portion of the hidden layer. We can run the training loop for the new neural network model and then look at the resulting gradients after the last epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 234.4196, Validation loss 42.4358\n",
      "Epoch 1000, Training loss 12.6647, Validation loss 6.4894\n",
      "Epoch 2000, Training loss 5.4795, Validation loss 1.7986\n",
      "Epoch 3000, Training loss 3.1810, Validation loss 1.9572\n",
      "Epoch 4000, Training loss 2.5051, Validation loss 2.6206\n",
      "Epoch 5000, Training loss 2.2813, Validation loss 3.2688\n",
      "output tensor([[ 6.9662],\n",
      "        [-1.5200]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[ 8.],\n",
      "        [-4.]])\n",
      "hidden tensor([[ 6.3478e-01],\n",
      "        [ 3.3002e-01],\n",
      "        [-1.5023e+01],\n",
      "        [-1.2942e+01],\n",
      "        [ 8.4959e-04],\n",
      "        [ 1.5994e+01],\n",
      "        [-5.1378e-01],\n",
      "        [-1.3629e+01]])\n"
     ]
    }
   ],
   "source": [
    "# We’ve dropped the learning rate a bit to help with stability.\n",
    "optimizer = torch.optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer, \n",
    "    model = seq_model, \n",
    "    loss_fn = torch.nn.MSELoss(), \n",
    "    t_u_train = t_un_train, \n",
    "    t_u_val = t_un_val, \n",
    "    t_c_train = t_c_train, \n",
    "    t_c_val = t_c_val)\n",
    "\n",
    "print('output', seq_model(t_un_val)) \n",
    "print('answer', t_c_val) \n",
    "print('hidden', seq_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 Comparing to the linear model\n",
    "We can also evaluate the model on all of the data and see how it differs from a line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_range = torch.arange(20., 90.).unsqueeze(1)\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "plt.xlabel(\"Fahrenheit\") \n",
    "plt.ylabel(\"Celsius\") \n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o') \n",
    "plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-') \n",
    "plt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is shown in `figure 6.9`. We can appreciate that the neural network has a tendency to overfit, as we discussed in `chapter 5`, since it tries to chase the measurements, including the noisy ones. Even our tiny neural network has too many parameters to fit the few measurements we have. It doesn’t do a bad job, though, overall.\n",
    "\n",
    "\n",
    "## 6.4 Conclusion\n",
    "We’ve covered a lot in `chapters 5` and `6`, although we have been dealing with a very simple problem. We dissected building differentiable models and training them using gradient descent, first using raw `autograd` and then relying on `nn`. By now you should have confidence in your understanding of what’s going on behind the scenes. Hopefully this taste of `PyTorch` has given you an appetite for more!\n",
    "\n",
    "## 6.5 Exercises\n",
    "[skip]\n",
    "\n",
    "## 6.6 Summary\n",
    "+ Neural networks can be automatically adapted to specialize themselves on the problem at hand.\n",
    "\n",
    "+ Neural networks allow easy access to the analytical derivatives of the loss with respect to any parameter in the model, which makes evolving the parameters very efficient. Thanks to its automated differentiation engine, `PyTorch` provides such derivatives effortlessly.\n",
    "\n",
    "+ Activation functions around linear transformations make neural networks capable of approximating highly nonlinear functions, at the same time keeping them simple enough to optimize.\n",
    "\n",
    "+ The `nn` module together with the tensor standard library provide all the building blocks for creating neural networks.\n",
    "\n",
    "+ To recognize overfitting, it’s essential to maintain the training set of data points separate from the validation set. There’s no one recipe to combat overfitting, but getting more data, or more variability in the data, and resorting to simpler models are good starts.\n",
    "\n",
    "+ Anyone doing data science should be plotting data all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
