{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import random\n",
    "import numpy\n",
    "import time\n",
    "import math\n",
    "import d2l\n",
    "import os\n",
    "\n",
    "from mxnet import autograd, np, npx, gluon, init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet.gluon import rnn\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Computational Performance\n",
    "In deep learning, datasets are usually large and model computation is complex. Therefore, we are always very concerned about computing performance. This chapter will focus on the important factors that affect computing performance: imperative programming, symbolic programming, asynchronous programing, automatic parallel computation, and multi-GPU computation. By studying this chapter, you should be able to further improve the computing performance of the models that have been implemented in the previous chapters, for example, by reducing the model training time without affecting the accuracy of the model.\n",
    "\n",
    "## 12.1 Compilers and Interpreters\n",
    "So far, this book has focused on imperative programming, which makes use of statements such as `print`, `+` or `if` to change a program’s state. Consider the following example of a simple imperative program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b) \n",
    "    f = add(c, d) \n",
    "    g = add(e, f) \n",
    "    return g\n",
    "\n",
    "print(fancy_func(1, 2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is an interpreted language. When evaluating `fancy_func` it performs the operations making up the function's body in `sequence`. That is, it will evaluate `e = add(a, b)` and it will store the results as variable `e`, thereby changing the program's state. The next two statements `f = add(c, d)` and `g = add(e, f)` will be excecuted similarly, performing additions and storing the results as variables. `Fig. 12.1.1` illustrates the flow of data.\n",
    "\n",
    "<img src=\"images/12_01.png\" style=\"width:350px;\"/>\n",
    "\n",
    "Although imperative programming is convenient, it may be inefficient. On one hand, even if the `add` function is repeatedly called throughout `fancy_func`, Python will execute the three function calls individually. If these are executed, say, on a GPU (or even on multiple GPUs), the overhead arising from the Python interpreter can become overwhelming. Moreover, it will need to save the variable values of `e` and `f` until all the statements in `fancy_func` have been executed. This is because we do not know whether the variables `e` and `f` will be used by other parts of the program after the statements `e = add(a, b)` and `f = add(c, d)` have been executed.\n",
    "\n",
    "### 12.1.1 Symbolic Programming\n",
    "Consider the alternative, symbolic programming where computation is usually performed only once the process has been fully defined. This strategy is used by multiple deep learning frameworks, including `Theano`, `Keras` and `TensorFlow` (the latter two have since acquired imperative extensions). It usually involves the following steps:\n",
    "1. Define the operations to be executed.\n",
    "2. Compile the operations into an executable program.\n",
    "3. Provide the required inputs and call the compiled program for execution.\n",
    "\n",
    "This allows for a significant amount of optimization. First off, we can skip the Python interpreter in many cases, thus removing a performance bottleneck that can become significant on multiple fast GPUs paired with a single Python thread on a CPU. Secondly, a compiler might optimize and rewrite the above code into `print((1 + 2) + (3 + 4))` or even `print(10)`. This is possible since a compiler gets to see the full code before turning it into machine instructions. For instance, it can release memory (or never allocate it) whenever a variable is no longer needed. Or it can transform the code entirely into an equivalent piece. To get a better idea consider the following simulation of imperative programming (it is Python after all) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "def fancy_func(a, b, c, d):\n",
      "    e = add(a, b)\n",
      "    f = add(c, d)\n",
      "    g = add(e, f)\n",
      "    return g\n",
      "print(fancy_func(1, 2, 3, 4))\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def add_():\n",
    "    return '''\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "'''\n",
    "\n",
    "def fancy_func_():\n",
    "    return '''\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "'''\n",
    "\n",
    "def evoke_():\n",
    "    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'\n",
    "\n",
    "prog = evoke_()\n",
    "print(prog)\n",
    "y = compile(prog, '', 'exec')\n",
    "exec(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences between imperative (interpreted) programming and symbolic programming are as follows:\n",
    "+ Imperative programming is easier. When imperative programming is used in Python, the majority of the code is straightforward and easy to write. It is also easier to debug imperative programming code. This is because it is easier to obtain and print all relevant intermediate variable values, or use Python’s built-in debugging tools.\n",
    "+ Symbolic programming is more efficient and easier to port. It makes it easier to optimize the code during compilation, while also having the ability to port the program into a format independent of Python. This allows the program to be run in a non-Python environment, thus avoiding any potential performance issues related to the Python interpreter.\n",
    "\n",
    "### 12.1.2 Hybrid Programming\n",
    "Historically most deep learning frameworks choose between an imperative or a symbolic approach. For example, `Theano`, `TensorFlow` (inspired by the latter), `Keras` and `CNTK` formulate models symbolically. Conversely, `Chainer` and `PyTorch` take an imperative approach. An imperative mode was added to `TensorFlow 2.0` (via `Eager`) and `Keras` in later revisions. When designing `Gluon`, developers considered whether it would be possible to combine the benefits of both programming models. This led to a hybrid model that lets users develop and debug using pure imperative programming, while having the ability to convert most programs into symbolic programs to be run when product-level computing performance and deployment are required.\n",
    "\n",
    "In practice this means that we build models using either the `HybridBlock` or the `HybridSequential` and `HybridConcurrent` classes. By default, they are executed in the same way `Block` or `Sequential` and `Concurrent` classes are executed in imperative programming. `HybridSequential` is a subclass of `HybridBlock` (just like `Sequential` subclasses `Block`). When the hybridize function is called, `Gluon` compiles the model into the form used in symbolic programming. This allows one to optimize the compute-intensive components without sacrifices in the way a model is implemented. We will illustrate the benefits below, focusing on sequential models and blocks only (the concurrent composition works analogously).\n",
    "\n",
    "### 12.1.3 HybridSequential\n",
    "The easiest way to get a feel for how hybridization works is to consider deep networks with multiple layers. Conventionally the Python interpreter will need to execute the code for all layers to generate an instruction that can then be forwarded to a CPU or a GPU. For a single (fast) compute device this does not cause any major issues. On the other hand, if we use an advanced 8-GPU server such as an `AWS P3dn.24xlarge` instance Python will struggle to keep all GPUs busy. The single-threaded Python interpreter becomes the bottleneck here. Let us see how we can address this for significant parts of the code by replacing `Sequential` by `HybridSequential`. We begin by defining a simple `MLP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16526175, -0.14005634]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# factory for networks\n",
    "def get_net():\n",
    "    net = nn.HybridSequential()  \n",
    "    net.add(nn.Dense(256, activation='relu'),\n",
    "            nn.Dense(128, activation='relu'),\n",
    "            nn.Dense(2))\n",
    "    net.initialize()\n",
    "    return net\n",
    "\n",
    "x = np.random.normal(size=(1, 512))\n",
    "net = get_net()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the `hybridize` function, we are able to compile and optimize the computation in the `MLP`. The model’s computation result remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16526175, -0.14005634]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.hybridize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems almost too good to be true: simply designate a block to be `HybridSequential`, write the same code as before and invoke `hybridize`. Once this happens the network is optimized (we will benchmark the performance below). Unfortunately this does not work magically for every layer. That said, the blocks provided by `Gluon` are by default subclasses of `HybridBlock` and thus hybridizable. A layer will not be optimized if it inherits from the `Block` instead.\n",
    "\n",
    "##### Acceleration by Hybridization\n",
    "To demonstrate the performance improvement gained by compilation we compare the time needed to evaluate `net(x)` before and after hybridization. Let us define a function to measure this time first. It will come handy throughout the chapter as we set out to measure (and improve) performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class Benchmark:    \n",
    "    def __init__(self, description='Done'):\n",
    "        self.description = description\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.timer = d2l.Timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(f'{self.description}: {self.timer.stop():.4f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can invoke the network twice, once with and once without hybridization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without hybridization: 0.6225 sec\n",
      "With hybridization: 0.2112 sec\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "with Benchmark('Without hybridization'):\n",
    "    for i in range(1000): \n",
    "        net(x)\n",
    "    npx.waitall()\n",
    "\n",
    "net.hybridize()\n",
    "with Benchmark('With hybridization'):\n",
    "    for i in range(1000): \n",
    "        net(x)\n",
    "    npx.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is observed in the above results, after a `HybridSequential` instance calls the `hybridize` function, computing performance is improved through the use of symbolic programming.\n",
    "\n",
    "##### Serialization\n",
    "One of the benefits of compiling the models is that we can serialize (save) the model and its parameters to disk. This allows us to store a model in a manner that is independent of the front-end language of choice. This allows us to deploy trained models to other devices and easily use other front-end programming languages. At the same time the code is often faster than what can be achieved in imperative programming. Let us see the `export` method in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.export('./models/my_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is decomposed into a (large binary) parameter file and a JSON description of the program required to execute to compute the model. The files can be read by other front-end languages supported by Python or `MXNet`, such as C++, R, Scala, and Perl. Let us have a look at the model description.\n",
    "\n",
    "```python\n",
    "# head my_mlp-symbol.json\n",
    "{\n",
    "    \"nodes\": [\n",
    "        { \"op\": \"null\", \"name\": \"data\", \"inputs\": [] }, \n",
    "        { \"op\": \"null\", \"name\": \"dense3_weight\", ...\n",
    "```\n",
    "\n",
    "Things are slightly more tricky when it comes to models that resemble code more closely. Basically hybridization needs to deal with control flow and Python overhead in a much more immediate manner. Moreover, contrary to the `Block` instance, which needs to use the `forward` function, for a `HybridBlock` instance we need to use the `hybrid_forward` function.\n",
    "\n",
    "Earlier, we demonstrated that, after calling the `hybridize` method, the model is able to achieve superior computing performance and portability. Note, though that hybridization can affect model flexibility, in particular in terms of control flow. We will illustrate how to design more general models and also how compilation will remove spurious Python elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridNet(nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(HybridNet, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(4)\n",
    "        self.output = nn.Dense(2)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        print('module F: ', F)\n",
    "        print('value  x: ', x)\n",
    "        x = F.npx.relu(self.hidden(x))\n",
    "        print('result  : ', x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above implements a simple network with 4 hidden units and 2 outputs. `hybrid_forward` takes an additional argument - the module `F`. This is needed since, depending on whether the code has been hybridized or not, it will use a slightly different library (`ndarray` or `symbol`) for processing. Both classes perform very similar functions and `MXNet` automatically determines the argument. To understand what is going on we print the arguments as part of the function invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module F:  <module 'mxnet.ndarray' from '/home/alex/3rd/py-venv/lib/python3.6/site-packages/mxnet/ndarray/__init__.py'>\n",
      "value  x:  [[-0.6338663   0.40156594  0.46456942]]\n",
      "result  :  [[0.01641375 0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00097611, 0.00019453]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = HybridNet()\n",
    "net.initialize()\n",
    "x = np.random.normal(size=(1, 3))\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the forward computation will lead to the same output (we omit details). Now let us see what happens if we invoke the hybridize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module F:  <module 'mxnet.symbol' from '/home/alex/3rd/py-venv/lib/python3.6/site-packages/mxnet/symbol/__init__.py'>\n",
      "value  x:  <_Symbol data>\n",
      "result  :  <_Symbol hybridnet0_relu0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00097611, 0.00019453]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.hybridize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using `ndarray` we now use the `symbol` module for `F`. Moreover, even though the input is of `ndarray` type, the data flowing through the network is now converted to `symbol` type as part of the compilation process. Repeating the function call leads to a surprising outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00097611, 0.00019453]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite different from what we saw previously. All print statements, as defined in `hybrid_forward` are omitted. Indeed, after hybridization the execution of `net(x)` does not involve the Python interpreter any longer. This means that any spurious Python code is omitted (such as `print` statements) in favor of a much more streamlined execution and better performance. Instead, `MXNet` directly calls the C++ backend. Also note that some functions are not supported in the symbol module (like `asnumpy`) and operations in-place like `a += b` and `a[:] = a + b` must be rewritten as `a = a + b`. Nonetheless, compilation of models is worth the effort whenever speed matters. The benefit can range from small percentage points to more than twice the speed, depending on the complexity of the model, the speed of the CPU and the speed and number of GPUs.\n",
    "\n",
    "##### Summary\n",
    "+ Imperative programming makes it easy to design new models since it is possible to write code with control flow and the ability to use a large amount of the Python software ecosystem.\n",
    "+ Symbolic programming requires that we specify the program and compile it before executing it. The benefit is improved performance.\n",
    "+ `MXNet` is able to combine the advantages of both approaches as needed.\n",
    "+ Models constructed by the `HybridSequential` and `HybridBlock` classes are able to convert imperative programs into symbolic programs by calling the hybridize method.\n",
    "\n",
    "##### Exercises\n",
    "1. Design a network using the `HybridConcurrent` class. Alternatively look at `Networks with Parallel Concatenations (GoogLeNet)` (page 271) for a network to compose.\n",
    "2. Add `x.asnumpy()` to the first line of the `hybrid_forward` function of the `HybridNet` class in this section. Execute the code and observe the errors you encounter. Why do they happen?\n",
    "3. What happens if we add control flow, i.e., the Python statements `if` and `for` in the `hybrid_forward` function?\n",
    "4. Review the models that interest you in the previous chapters and use the HybridBlock class or HybridSequential class to implement them.\n",
    "\n",
    "\n",
    "## 12.2 Asynchronous Computation\n",
    "Today's computers are highly parallel systems, consisting of multiple CPU cores (often multiple threads per core), multiple processing elements per GPU and often multiple GPUs per device. In short, we can process many different things at the same time, often on different devices. Unfortunately Python is not a great way of writing parallel and asynchronous code, at least not with some extra help. After all, Python is single-threaded and this is unlikely to change in the future. Deep learning frameworks such as `MXNet` and `TensorFlow` utilize an asynchronous programming model to improve performance (`PyTorch` uses Python's own scheduler leading to a different performance trade-off). Hence, understanding how asynchronous programming works helps us to develop more efficient programs, by proactively reducing computational requirements and mutual dependencies. This allows us to reduce memory overhead and increase processor utilization. We begin by importing the necessary libraries.\n",
    "\n",
    "### 12.2.1 Asynchrony via Backend\n",
    "For a warmup consider the following toy problem - we want to generate a random matrix and multiply it. Let us do that both in `NumPy` and in `MXNet` `NP` to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 0.7330 sec\n",
      "mxnet.np: 0.0084 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark('numpy'):\n",
    "    for _ in range(10):\n",
    "        a = numpy.random.normal(size=(1000, 1000))\n",
    "        b = numpy.dot(a, a)\n",
    "\n",
    "with Benchmark('mxnet.np'):\n",
    "    for _ in range(10):\n",
    "        a = np.random.normal(size=(1000, 1000))\n",
    "        b = np.dot(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is orders of magnitude faster. At least it seems to be so. Since both are executed on the same processor something else must be going on. Forcing `MXNet` to finish all computation prior to returning shows what happened previously: computation is being executed by the backend while the frontend returns control to Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0.5406 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark():\n",
    "    for _ in range(10):\n",
    "        a = np.random.normal(size=(1000, 1000))\n",
    "        b = np.dot(a, a)\n",
    "    npx.waitall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, `MXNet` has a frontend for direct interaction with the users, e.g., via Python, as well as a backend used by the system to perform the computation. The backend possesses its own threads that continuously collect and execute queued tasks. Note that for this to work the backend must be able to keep track of the dependencies between various steps in the computational graph. Hence it is ony possible to parallelize operations that do not depend on each other.\n",
    "\n",
    "As shown in `Fig. 12.2.1`, users can write `MXNet` programs in various frontend languages, such as Python, R, Scala and C++. Regardless of the front-end programming language used, the execution of MXNet programs occurs primarily in the back-end of C++ implementations. Operations issued by the frontend language are passed on to the backend for execution. The backend manages its own threads that continuously collect and execute queued tasks. Note that for this to work the backend must be able to keep track of the dependencies between various steps in the computational graph. That is, it is not possible to parallelize operations that depend on each other.\n",
    "\n",
    "<img src=\"images/12_02.png\" style=\"width:350px;\"/>\n",
    "\n",
    "Let us look at another toy example to understand the dependency graph a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((1, 2))\n",
    "y = np.ones((1, 2))\n",
    "z = x * y + 2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/12_03.png\" style=\"width:200px;\"/>\n",
    "\n",
    "The code snippet above is also illustrated in `Fig. 12.2.2`. Whenever the Python frontend thread executes one of the first three statements, it simply returns the task to the backend queue. When the last statement’s results need to be printed, the Python frontend thread will wait for the C++ backend thread to finish computing result of the variable `z`. One benefit of this design is that the Python frontend thread does not need to perform actual computations. Thus, there is little impact on the program’s overall performance, regardless of Python’s performance. `Fig. 12.2.3` illustrates how frontend and backend interact.\n",
    "\n",
    "<img src=\"images/12_04.png\" style=\"width:500px;\"/>\n",
    "\n",
    "### 12.2.2 Barriers and Blockers\n",
    "There are a number of operations that will force Python to wait for completion:\n",
    "+ Most obviously `npx.waitall()` waits until all computation has completed, regardless of when the compute instructions were issued. In practice it is a bad idea to use this operator unless absolutely necessary since it can lead to poor performance.\n",
    "+ If we just want to wait until a specific variable is available we can call `z.wait_to_read()`. In this case MXNet blocks return to Python until the variable `z` has been computed. Other computation may well continue afterwards.\n",
    "\n",
    "Let us see how this works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waitall: 0.0353 sec\n",
      "wait_to_read: 0.0342 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark('waitall'):\n",
    "    b = np.dot(a, a)\n",
    "    npx.waitall()\n",
    "\n",
    "with Benchmark('wait_to_read'):\n",
    "    b = np.dot(a, a)\n",
    "    b.wait_to_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both operations take approximately the same time to complete. Besides the obvious blocking operations we recommend that the reader is aware of implicit blockers. Printing a variable clearly requires the variable to be available and is thus a blocker. Lastly, conversions to `NumPy` via `z.asnumpy()` and conversions to scalars via `z.item()` are blocking, since `NumPy` has no notion of asynchrony. It needs access to the values just like the `print` function. Copying small amounts of data frequently from `MXNet`'s scope to `NumPy` and back can destroy performance of an otherwise efficient code, since each such operation requires the compute graph to evaluate all intermediate results needed to get the relevant term before anything else can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy conversion: 0.0372 sec\n",
      "scalar conversion: 0.0451 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark('numpy conversion'):\n",
    "    b = np.dot(a, a)\n",
    "    b.asnumpy()\n",
    "\n",
    "with Benchmark('scalar conversion'):\n",
    "    b = np.dot(a, a)\n",
    "    b.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3 Improving Computation\n",
    "On a heavily multithreaded system (even regular laptops have 4 threads or more and on multi-socket servers this number can exceed 256) the overhead of scheduling operations can become significant. This is why it is highly desirable to have computation and scheduling occur asynchronously and in parallel. To illustrate the benefit of doing this let us see what happens if we increment a variable by 1 multiple times, both in sequence or asynchronously. We simulate synchronous execution by inserting a `wait_to_read()` barrier in between each addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synchronous: 0.0656 sec\n",
      "asynchronous: 0.0569 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark('synchronous'):\n",
    "    for _ in range(1000):\n",
    "        y = x + 1\n",
    "        y.wait_to_read()\n",
    "\n",
    "with Benchmark('asynchronous'):\n",
    "    for _ in range(1000):\n",
    "        y = x + 1\n",
    "    y.wait_to_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly simplified interaction between the Python front-end thread and the C++ back-end thread can be summarized as follows:\n",
    "+ The front-end orders the back-end to insert the calculation task `y = x + 1` into the queue.\n",
    "+ The back-end then receives the computation tasks from the queue and performs the actual computations.\n",
    "+ The back-end then returns the computation results to the front-end.\n",
    "\n",
    "Assume that the durations of these three stages are $t_1, t_2$ and $t_3$, respectively. If we do not use asynchronous programming, the total time taken to perform 1000 computations is approximately $1000 (t_1+ t_2 + t_3)$. If asynchronous programming is used, the total time taken to perform 1000 computations can be reduced to $t_1 + 1000 t_2 + t_3$ (assuming $1000 t_2 > 999t_1$), since the front-end does not have to wait for the back-end to return computation results for each loop.\n",
    "\n",
    "### 12.2.4 Improving Memory Footprint\n",
    "Imagine a situation where we keep on inserting operations into the backend by executing Python code on the frontend. For instance, the frontend might insert a large number of minibatch tasks within a very short time. After all, if no meaningful computation happens in Python this can be done quite quickly. If each of these tasks can be launched quickly at the same time this may cause a spike in memory usage. Given a finite amount of memory available on GPUs (and even on CPUs) this can lead to resource contention or even program crashes. Some readers might have noticed that previous training routines made use of synchronization methods such as `item` or even `asnumpy`.\n",
    "\n",
    "We recommend to use these operations carefully, e.g., for each minibatch, such as to balance computational efficiency and memory footprint. To illustrate what happens let us implement a simple training loop for a deep network and measure its memory consumption and timing. Below is the mock data generator and deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter():\n",
    "    timer = d2l.Timer()\n",
    "    num_batches, batch_size = 150, 1024\n",
    "    for i in range(num_batches):\n",
    "        X = np.random.normal(size=(batch_size, 512))\n",
    "        y = np.ones((batch_size,))\n",
    "        yield X, y\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'batch {i + 1}, time {timer.stop():.4f} sec')\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(2048, activation='relu'),\n",
    "        nn.Dense(512, activation='relu'), \n",
    "        nn.Dense(1))\n",
    "net.initialize()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd')\n",
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a tool to measure the memory footprint of our code. We use a relatively primitive `ps` call to accomplish this (note that the latter only works on Linux and MacOS). For a much more detailed analysis of what is going on here use e.g., `Nvidia`'s [Nsight](https://developer.nvidia.com/nsight-compute-2019_5) or `Intel`'s [vTune](https://software.intel.com/content/www/us/en/develop/tools/vtune-profiler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mem():\n",
    "    res = subprocess.check_output(['ps', 'u', '-p', str(os.getpid())])\n",
    "    return int(str(res).split()[15]) / 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin testing we need to initialize the parameters of the network and process one batch. Otherwise it would be tricky to see what the additional memory consumption is. See `Section 5.3` for further details related to initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in data_iter():\n",
    "    break\n",
    "loss(y, net(X)).wait_to_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that we do not overflow the task buffer on the backend we insert a `wait_to_read` call for the `loss` function at the end of each loop. This forces the forward pass to complete before a new forward pass is commenced. Note that a (possibly more elegant) alternative would have been to track the loss in a scalar variable and to force a barrier via the `item` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 50, time 13.3457 sec\n",
      "batch 100, time 26.3482 sec\n",
      "batch 150, time 39.3816 sec\n",
      "time per epoch: 39.3937 sec\n",
      "increased memory: -0.184000 MB\n"
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "with Benchmark('time per epoch'):\n",
    "    for X, y in data_iter():\n",
    "        with autograd.record():\n",
    "            l = loss(y, net(X))\n",
    "        l.backward()\n",
    "        trainer.step(X.shape[0])\n",
    "        l.wait_to_read()  # Barrier before a new batch\n",
    "    npx.waitall()\n",
    "print(f'increased memory: {get_mem() - mem:f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the timing of the minibatches lines up quite nicely with the overall runtime of the optimization code. Moreover, memory footprint only increases slightly. Now let us see what happens if we drop the barrier at the end of each minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 50, time 0.1038 sec\n",
      "batch 100, time 0.2037 sec\n",
      "batch 150, time 0.3027 sec\n",
      "time per epoch: 40.2418 sec\n",
      "increased memory: 0.220000 MB\n"
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "with Benchmark('time per epoch'):\n",
    "    for X, y in data_iter():\n",
    "        with autograd.record():\n",
    "            l = loss(y, net(X))\n",
    "        l.backward()\n",
    "        trainer.step(X.shape[0])\n",
    "    npx.waitall()\n",
    "print(f'increased memory: {get_mem() - mem:f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the time to issue instructions for the backend is an order of magnitude smaller, we still need to perform computation. Consequently a large amount of intermediate results cannot be released and may pile up in memory. While this didn't cause any issues in the toy example above, it might well have resulted in out of memory situations when left unchecked in real world scenarios.\n",
    "\n",
    "##### Summary\n",
    "+ `MXNet` decouples the Python frontend from an execution backend. This allows for fast asynchronous insertion of commands into the backend and associated parallelism.\n",
    "+ Asynchrony leads to a rather responsive frontend. However, use caution not to overfill the task queue since it may lead to excessive memory consumption.\n",
    "+ It is recommended to synchronize for each minibatch to keep frontend and backend approximately synchronized.\n",
    "+ Be aware of the fact that conversions from `MXNet`'s memory management to Python will force the backend to wait until the specific variable is ready. `print`, `asnumpy` and `item` all have this effect. This can be desirable but a carless use of synchronization can ruin performance.\n",
    "+ Chip vendors offer sophisticated performance analysis tools to obtain a much more fine-grained insight into the efficiency of deep learning.\n",
    "\n",
    "##### Exercises\n",
    "1. We mentioned above that using asynchronous computation can reduce the total amount of time needed to perform $1000$ computations to $t_1 + 1000 t_2 + t_3$. Why do we have to assume $1000 t_2 > 999 t_1$ here?\n",
    "2. How would you need to modify the training loop if you wanted to have an overlap of one minibatch each? I.e., if you wanted to ensure that batch $b_t$ finishes before batch $b_{t+2}$ commences?\n",
    "3. What might happen if we want to execute code on CPUs and GPUs simultaneously? Should you still insist on synchronizing after every minibatch has been issued?\n",
    "4. Measure the difference between waitall and wait_to_read. Hint: perform a number of instructions and synchronize for an intermediate result.\n",
    "\n",
    "\n",
    "## 12.3 Automatic Parallelism\n",
    "`MXNet` automatically constructs computational graphs at the backend. Using a computational graph, the system is aware of all the dependencies, and can selectively execute multiple non-interdependent tasks in parallel to improve speed. For instance, `Fig. 12.2.2` in `Section 12.2` initializes two variables independently. Consequently the system can choose to execute them in parallel.\n",
    "\n",
    "Typically, a single operator will use all the computational resources on all CPUs or on a single GPU. For example, the `dot` operator will use all cores (and threads) on all CPUs, even if there are multiple CPU processors on a single machine. The same applies to a single GPU. Hence parallelization is not quite so useful single-device computers. With multiple devices things matter more. While parallelization is typically most relevant between multiple GPUs, adding the local CPU will increase performance slightly. See e.g., (`Hadjis et al., 2016`) for a paper that focuses on training computer vision models combining a GPU and a CPU. With the convenience of an automatically parallelizing framework we can accomplish the same goal in a few lines of Python code. More broadly, our discussion of automatic parallel computation focuses on parallel computation using both CPUs and GPUs, as well as the parallelization of computation and communication. We begin by importing the required packages and modules. Note that we need at least one GPU to run the experiments in this section.\n",
    "\n",
    "### 12.3.1 Parallel Computation on CPUs and GPUs\n",
    "Let us start by defining a reference workload to test - the run function below performs 10 matrix-matrix multiplications on the device of our choosing using data allocated into two variables, `x_cpu` and `x_gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
