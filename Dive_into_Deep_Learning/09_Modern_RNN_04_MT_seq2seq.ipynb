{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import d2l\n",
    "import os\n",
    "\n",
    "from mxnet import autograd, np, npx, gluon, init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet.gluon import rnn\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  09. Modern Recurrent Neural Networks\n",
    "Although we have learned the basics of recurrent neural networks, they are not sufficient for a practitioner to solve today's sequence learning problems. For instance, given the numerical unstability during gradient calculation, gated recurrent neural networks are much more common in practice. We will begin by introducing two of such widely-used networks, namely `gated recurrent units` (`GRU`) and `long short term memory` (`LSTM`), with illustrations using the same language modeling problem as introduced in `Chapter 8`.\n",
    "\n",
    "Furthermore, we will modify recurrent neural networks with a single undirectional hidden layer. We will describe deep architectures, and discuss the bidirectional design with both forward and backward recursion. They are frequently adopted in modern recurrent networks.\n",
    "\n",
    "In fact, a large portion of sequence learning problems such as automatic speech recognition, text to speech, and machine translation, consider both inputs and outputs to be sequences of arbitrary length. Finally, we will take machine translation as an example, and introduce the encoder-decoder architecture based on recurrent neural networks and modern practices for such sequence to sequence learning problems.\n",
    "\n",
    "\n",
    "## 9.5 Machine Translation and the Dataset\n",
    "So far we see how to use recurrent neural networks for language models, in which we predict the next token given all previous tokens in an article. Now let us have a look at a different application, machine translation, whose predict output is no longer a single token, but a list of tokens.\n",
    "\n",
    "`Machine translation` (`MT`) refers to the automatic translation of a segment of text from one language to another. Solving this problem with neural networks is often called `neural machine translation` (`NMT`). Compared to language models (`Section 8.3`), in which the corpus only contains a single language, machine translation dataset has at least two languages, the source language and the target language. In addition, each sentence in the source language is mapped to the according translation in the target language. Therefore, the data preprocessing for machine translation data is different to the one for language models. This section is dedicated to demonstrate how to pre-process such a dataset and then load into a set of minibatches.\n",
    "\n",
    "### 9.5.1 Reading and Preprocessing the Dataset\n",
    "We first download a dataset that contains a set of English sentences with the corresponding French translations. As can be seen that each line contains an English sentence with its French translation, which are separated by a `TAB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "Fire!\tAu feu !\n",
      "Help!\tÀ l'aide !\n"
     ]
    }
   ],
   "source": [
    "d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip', '94646ad1522d915e7b0f9296181140edcf86a4f5')\n",
    "\n",
    "#@save\n",
    "def read_data_nmt():\n",
    "    data_dir = d2l.download_extract('fra-eng')\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "raw_text = read_data_nmt()\n",
    "print(raw_text[0:106])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform several preprocessing steps on the raw text data, including ignoring cases, replacing UTF-8 non-breaking space with space, and adding space between words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who?\tqui ?\n",
      "wow !\tça alors !\n",
      "fire !\tau feu !\n"
     ]
    }
   ],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def preprocess_nmt(text):\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!') and prev_char != ' '\n",
    "\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i-1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[0:95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.2 Tokenization\n",
    "Different to using character tokens in `Section 8.3`, here a token is either a word or a punctuation mark. The following function tokenizes the text data to return source and target. Each one is a list of token list, with `source[i]` is the $i^\\mathrm{th}$ sentence in the source language and `target[i]` is the $i^\\mathrm{th}$ sentence in the target language. To make the latter training faster, we sample the first `num_examples` sentences pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['go', '.'], ['hi', '.'], ['run', '!']],\n",
       " [['va', '!'], ['salut', '!'], ['cours', '!']])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_nmt(text, num_examples=None):\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "source[0:3], target[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the histogram of the number of tokens per sentence in the following figure. As can be seen, a sentence in average contains 5 tokens, and most of the sentences have less than 10 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAEvCAYAAADSNxEkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAXk0lEQVR4nO3de4xX5b3v8fdXBouKNyg1FmxnTipaCoKKlBZTe2CrtFovae12Z1dJtRqjdqvRbqltoq01sUmz3dVYrCm02NijhtYD2bL1WKq9ehuUs0XYrdSiDt5GULwdL6Pf88fvwY4IMwPMMMM871cymbWe9az1e776m3xYaz2/9YvMRJKk2uzU3wOQJKk/GICSpCoZgJKkKhmAkqQqGYCSpCoZgJKkKjX19wC21gc/+MFsbm7u72FIkgaIpUuXPp+Zo3raf4cNwObmZlpbW/t7GJKkASIiHt+S/l4ClSRVyQCUJFXJAJQkVWmHvQcoSbV46623aGtr4/XXX+/voQwIw4YNY8yYMQwdOnSbjmMAStIA19bWxu67705zczMR0d/D6VeZydq1a2lra6OlpWWbjuUlUEka4F5//XVGjhxZffgBRAQjR47slbNhA1CSdgCG39/11n8LA1CSVCXvAUrSDqZ59m29erzVVx7Tq8frTkdHB01N/R8/ngFKkrr06quvcswxxzBx4kTGjx/PzTffzJIlSzj44IOZMGECp512Gm+88QbQeErX888/D0Brayuf/exnAbjssss45ZRTmDZtGqeccgpvv/02F110EePHj+eggw7immuuAWDp0qUcccQRHHrooRx99NE8/fTTfVZX/0ewJGlAu/322/nwhz/Mbbc1zjzXr1/P+PHjWbJkCWPHjuXUU09lzpw5nH/++V0eZ8WKFfzhD39gl112Yc6cOaxevZply5bR1NTEunXreOutt/j617/OwoULGTVqFDfffDPf+ta3mDdvXp/UZQAOFpftuQ37ru+9cUgadCZMmMCFF17IxRdfzLHHHssee+xBS0sLY8eOBWDWrFlce+213Qbgcccdxy677ALAr3/9a84666x3L4WOGDGC5cuXs3z5co488kgA3n77bfbdd98+q8sAlCR1aezYsTz44IMsXryYb3/720yfPn2zfZuamnjnnXcA3vdRhd12263L18lMPvGJT3DPPfds+6B7wHuAkqQuPfXUU+y666585Stf4Rvf+Ab33HMPq1evZtWqVQD8/Oc/54gjjgAa9wCXLl0KwC9/+cvNHvPII4/kxz/+MR0dHQCsW7eOAw44gPb29ncD8K233uKRRx7ps7oMQElSlx5++GGmTJnCpEmT+M53vsP3vvc9fvrTn3LSSScxYcIEdtppJ8466ywALr30Us477zwmT57MkCFDNnvMr33ta3zkIx/hoIMOYuLEifziF79g5513ZsGCBVx88cVMnDiRSZMm8ac//anP6orM7LOD96XJkyen3wfYifcApUFr5cqVfPzjH+/vYQwom/pvEhFLM3NyT4/hGaAkqUoGoCSpSgagJKlKBqAkqUoGoCSpSgagJKlKBqAkqUsvvvgiP/rRj/r8de6+++4+/dzfxnwUmiTtaLblc7+bPF7XnwXeEIBnn312jw6XmWQmO+20ZedYd999N8OHD+fTn/70Fu23tTwDlCR1afbs2fz1r39l0qRJXHDBBcyYMYNDDjmECRMmsHDhQgBWr17NAQccwKmnnsr48eN58sknmTt3LmPHjmXKlCmcccYZnHvuuQC0t7fzxS9+kcMOO4zDDjuMP/7xj6xevZrrrruOq666ikmTJvH73/++z+vyDFCS1KUrr7yS5cuXs2zZMjo6OnjttdfYY489eP7555k6dSrHHXccAI8++ijz589n6tSpPPXUU1x++eU8+OCD7L777kyfPp2JEycCcN5553HBBRdw+OGH88QTT3D00UezcuVKzjrrLIYPH85FF120XeoyACVJPZaZXHLJJfzud79jp512Ys2aNTz77LMAfPSjH2Xq1KkA3H///RxxxBGMGDECgJNOOom//OUvQOOrkFasWPHuMV966SVeeeWV7VyJAShJ2gI33ngj7e3tLF26lKFDh9Lc3Pzu1x5193VHG7zzzjvce++9DBs2rC+H2i3vAUqSurT77rvz8ssvA41vg//Qhz7E0KFDueuuu3j88cc3uc9hhx3Gb3/7W1544QU6Ojre89VIRx11FNdcc82768uWLXvf62wPBqAkqUsjR45k2rRpjB8/nmXLltHa2sqECRO44YYbOPDAAze5z+jRo7nkkkuYMmUK06ZNo7m5mT33bMxevfrqq2ltbeWggw5i3LhxXHfddQB84Qtf4NZbb91uk2D8OqTBwq9DkgatHfXrkF555RWGDx9OR0cHJ554Iqeddhonnnhirxzbr0OSJA1Yl112GZMmTWL8+PG0tLRwwgkn9PeQ3sNJMJKkPvGDH/ygv4fQJc8AJUlVMgAlaQewo87X6Au99d/CAJSkAW7YsGGsXbvWEKQRfmvXru2VzxD26B5gRFwAfA1I4GHgq8C+wE3ASGApcEpmvhkRHwBuAA4F1gL/mJmry3G+CZwOvA38S2beUdpnAj8EhgA/ycwrt7myHVTz7Nu2ar/V/ft5Ukl9aMyYMbS1tdHe3t7fQxkQhg0bxpgxY7b5ON0GYESMBv4FGJeZ/y8ibgFOBj4PXJWZN0XEdTSCbU75/UJmfiwiTga+D/xjRIwr+30C+DDw64gYW17mWuBIoA14ICIWZeYKJEkMHTqUlpaW/h7GoNPTS6BNwC4R0QTsCjwNTAcWlO3zgQ3zW48v65TtMyIiSvtNmflGZv4NWAVMKT+rMvOxzHyTxlnl8dtWliRJXes2ADNzDfAD4AkawbeexiXPFzOzo3RrA0aX5dHAk2XfjtJ/ZOf2jfbZXLskSX2m2wCMiL1pnJG10Lh0uRsws4/HtbmxnBkRrRHR6rVwSdK26MkkmH8A/paZ7QAR8StgGrBXRDSVs7wxwJrSfw2wH9BWLpnuSWMyzIb2DTrvs7n298jM64HrofEotB6MvUtbO+EEYPWVx2zry0uS+lFP7gE+AUyNiF3LvbwZwArgLuBLpc8sYGFZXlTWKdt/k425u4uAkyPiAxHRAuwP3A88AOwfES0RsTONiTKLtr00SZI2r9szwMy8LyIWAA8CHcBDNM7CbgNuiojvlba5ZZe5wM8jYhWwjkagkZmPlBmkK8pxzsnMtwEi4lzgDhofg5iXmY/0XomSJL1fjz4HmJmXApdu1PwYjRmcG/d9HThpM8e5ArhiE+2LgcU9GYskSb3BJ8FIkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqtTU3wPYYV2251but753xyFJ2iqeAUqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqtSjAIyIvSJiQUT8d0SsjIhPRcSIiLgzIh4tv/cufSMiro6IVRHxXxFxSKfjzCr9H42IWZ3aD42Ih8s+V0dE9H6pkiT9XU/PAH8I3J6ZBwITgZXAbGBJZu4PLCnrAJ8D9i8/ZwJzACJiBHAp8ElgCnDphtAsfc7otN/MbStLkqSudRuAEbEn8BlgLkBmvpmZLwLHA/NLt/nACWX5eOCGbLgX2Csi9gWOBu7MzHWZ+QJwJzCzbNsjM+/NzARu6HQsSZL6RE/OAFuAduCnEfFQRPwkInYD9snMp0ufZ4B9yvJo4MlO+7eVtq7a2zbRLklSn+lJADYBhwBzMvNg4FX+frkTgHLmlr0/vPeKiDMjojUiWtvb2/v65SRJg1hPArANaMvM+8r6AhqB+Gy5fEn5/VzZvgbYr9P+Y0pbV+1jNtH+Ppl5fWZOzszJo0aN6sHQJUnatG4DMDOfAZ6MiANK0wxgBbAI2DCTcxawsCwvAk4ts0GnAuvLpdI7gKMiYu8y+eUo4I6y7aWImFpmf57a6ViSJPWJph72+zpwY0TsDDwGfJVGeN4SEacDjwNfLn0XA58HVgGvlb5k5rqIuBx4oPT7bmauK8tnAz8DdgH+s/xIktRnehSAmbkMmLyJTTM20TeBczZznHnAvE20twLjezIWSZJ6g0+CkSRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVckAlCRVyQCUJFXJAJQkVanHARgRQyLioYj4j7LeEhH3RcSqiLg5InYu7R8o66vK9uZOx/hmaf9zRBzdqX1maVsVEbN7rzxJkjZtS84AzwNWdlr/PnBVZn4MeAE4vbSfDrxQ2q8q/YiIccDJwCeAmcCPSqgOAa4FPgeMA/6p9JUkqc/0KAAjYgxwDPCTsh7AdGBB6TIfOKEsH1/WKdtnlP7HAzdl5huZ+TdgFTCl/KzKzMcy803gptJXkqQ+09MzwH8H/hV4p6yPBF7MzI6y3gaMLsujgScByvb1pf+77Rvts7l2SZL6TLcBGBHHAs9l5tLtMJ7uxnJmRLRGRGt7e3t/D0eStAPryRngNOC4iFhN4/LkdOCHwF4R0VT6jAHWlOU1wH4AZfuewNrO7Rvts7n298nM6zNzcmZOHjVqVA+GLknSpnUbgJn5zcwck5nNNCax/CYz/xm4C/hS6TYLWFiWF5V1yvbfZGaW9pPLLNEWYH/gfuABYP8yq3Tn8hqLeqU6SZI2o6n7Lpt1MXBTRHwPeAiYW9rnAj+PiFXAOhqBRmY+EhG3ACuADuCczHwbICLOBe4AhgDzMvORbRiXJEnd2qIAzMy7gbvL8mM0ZnBu3Od14KTN7H8FcMUm2hcDi7dkLJIkbQufBCNJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqpIBKEmqkgEoSaqSAShJqlK3ARgR+0XEXRGxIiIeiYjzSvuIiLgzIh4tv/cu7RERV0fEqoj4r4g4pNOxZpX+j0bErE7th0bEw2WfqyMi+qJYSZI26MkZYAdwYWaOA6YC50TEOGA2sCQz9weWlHWAzwH7l58zgTnQCEzgUuCTwBTg0g2hWfqc0Wm/mdtemiRJm9dtAGbm05n5YFl+GVgJjAaOB+aXbvOBE8ry8cAN2XAvsFdE7AscDdyZmesy8wXgTmBm2bZHZt6bmQnc0OlYkiT1iS26BxgRzcDBwH3APpn5dNn0DLBPWR4NPNlpt7bS1lV72ybaN/X6Z0ZEa0S0tre3b8nQJUl6jx4HYEQMB34JnJ+ZL3XeVs7cspfH9j6ZeX1mTs7MyaNGjerrl5MkDWI9CsCIGEoj/G7MzF+V5mfL5UvK7+dK+xpgv067jyltXbWP2US7JEl9piezQAOYC6zMzH/rtGkRsGEm5yxgYaf2U8ts0KnA+nKp9A7gqIjYu0x+OQq4o2x7KSKmltc6tdOxJEnqE0096DMNOAV4OCKWlbZLgCuBWyLidOBx4Mtl22Lg88Aq4DXgqwCZuS4iLgceKP2+m5nryvLZwM+AXYD/LD+SJPWZbgMwM/8AbO5zeTM20T+BczZzrHnAvE20twLjuxuLJEm9xSfBSJKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqrU1N8D0CB12Z7bsO/63huHJG2GZ4CSpCoZgJKkKhmAkqQqGYCSpCoZgJKkKhmAkqQqGYCSpCoZgJKkKvlBeHWpefZtW7Xf6mG9PBBJ6mWeAUqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKqZABKkqo0YAIwImZGxJ8jYlVEzO7v8UiSBrcBEYARMQS4FvgcMA74p4gY17+jkiQNZgPl65CmAKsy8zGAiLgJOB5Y0a+j0uBy2Z5bud/63h2HpAFhoATgaODJTuttwCf7aSwawLb2+wlhAH5HoYEs9avIzP4eAxHxJWBmZn6trJ8CfDIzz92o35nAmWX1AODPmznkB4Hn+2i4O4Ka66+5drD+muuvuXZo1L9bZo7q6Q4D5QxwDbBfp/Uxpe09MvN64PruDhYRrZk5ufeGt2Opuf6aawfrr7n+mmuHd+tv3pJ9BsQkGOABYP+IaImInYGTgUX9PCZJ0iA2IM4AM7MjIs4F7gCGAPMy85F+HpYkaRAbEAEIkJmLgcW9dLhuL5MOcjXXX3PtYP01119z7bAV9Q+ISTCSJG1vA+UeoCRJ29WgC8DaHqkWEfMi4rmIWN6pbURE3BkRj5bfe/fnGPtKROwXEXdFxIqIeCQizivttdQ/LCLuj4j/W+r/TmlviYj7yt/AzWVi2aAUEUMi4qGI+I+yXlPtqyPi4YhYFhGtpa2W9/5eEbEgIv47IlZGxKe2pvZBFYCVPlLtZ8DMjdpmA0syc39gSVkfjDqACzNzHDAVOKf8/66l/jeA6Zk5EZgEzIyIqcD3gasy82PAC8Dp/TjGvnYesLLTek21A/zPzJzU6eMPtbz3fwjcnpkHAhNpvAe2uPZBFYB0eqRaZr4JbHik2qCVmb8D1m3UfDwwvyzPB07YroPaTjLz6cx8sCy/TOOPYDT11J+Z+UpZHVp+EpgOLCjtg7b+iBgDHAP8pKwHldTehUH/3o+IPYHPAHMBMvPNzHyRrah9sAXgph6pNrqfxtKf9snMp8vyM8A+/TmY7SEimoGDgfuoqP5yCXAZ8BxwJ/BX4MXM7ChdBvPfwL8D/wq8U9ZHUk/t0PjHzv+JiKXlKVlQx3u/BWgHflouf/8kInZjK2ofbAGojWRjmu+gnuobEcOBXwLnZ+ZLnbcN9voz8+3MnETj6UlTgAP7eUjbRUQcCzyXmUv7eyz96PDMPITGLZ9zIuIznTcO4vd+E3AIMCczDwZeZaPLnT2tfbAFYI8eqVaBZyNiX4Dy+7l+Hk+fiYihNMLvxsz8VWmupv4NyiWgu4BPAXtFxIbP+A7Wv4FpwHERsZrGrY7pNO4L1VA7AJm5pvx+DriVxj+AanjvtwFtmXlfWV9AIxC3uPbBFoA+Uq1hETCrLM8CFvbjWPpMueczF1iZmf/WaVMt9Y+KiL3K8i7AkTTug94FfKl0G5T1Z+Y3M3NMefbjycBvMvOfqaB2gIjYLSJ237AMHAUsp4L3fmY+AzwZEQeUphk0vjpvi2sfdB+Ej4jP07g3sOGRalf085D6VET8L+CzNJ6E/ixwKfC/gVuAjwCPA1/OzI0nyuzwIuJw4PfAw/z9PtAlNO4D1lD/QTRu9g+h8Y/ZWzLzuxHxP2icFY0AHgK+kplv9N9I+1ZEfBa4KDOPraX2UuetZbUJ+EVmXhERI6njvT+JxuSnnYHHgK9S/gbYgtoHXQBKktQTg+0SqCRJPWIASpKqZABKkqpkAEqSqmQASpKqZABKkqpkAEqSqmQASpKq9P8B1lDcuz+xXeUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.set_figsize()\n",
    "d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]], label=['source', 'target'])\n",
    "d2l.plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.3 Vocabulary\n",
    "Since the tokens in the source language could be different to the ones in the target language, we need to build a vocabulary for each of them. Since we are using words instead of characters as tokens, it makes the vocabulary size significantly large. Here we map every token that appears less than 3 times into the `<unk>` token `Section 8.2`. In addition, we need other special tokens such as padding and sentence beginnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9140"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab = d2l.Vocab(source, min_freq=3, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.4 Loading the Dataset\n",
    "In language models, each example is a `num_steps` length sequence from the corpus, which may be a segment of a sentence, or span over multiple sentences. In machine translation, an example should contain a pair of source sentence and target sentence. These sentences might have different lengths, while we need same length examples to form a minibatch.\n",
    "\n",
    "One way to solve this problem is that if a sentence is longer than `num_steps`, we trim its length, otherwise pad with a special `<pad>` token to meet the length. Therefore we could transform any sentence to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Trim\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
    "\n",
    "truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert a list of sentences into an `(num_example, num_steps)` index array. We also record the length of each sentence without the padding tokens, called valid length, which might be used by some models. In addition, we add the special `<bos>` and `<eos>` tokens to the target sentences so that our model will know the signals for starting and ending predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_array(lines, vocab, num_steps, is_source):\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    if not is_source:\n",
    "        lines = [[vocab['<bos>']] + l + [vocab['<eos>']] for l in lines]\n",
    "    array = np.array([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).sum(axis=1)\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can construct minibatches based on these arrays.\n",
    "\n",
    "### 9.5.5 Putting All Things Together\n",
    "Finally, we define the function `load_data_nmt` to return the data iterator with the vocabularies for source language and target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size, num_steps, num_examples=1000):\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = d2l.Vocab(source, min_freq=3, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = d2l.Vocab(target, min_freq=3, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_array(source, src_vocab, num_steps, True)\n",
    "    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, num_steps, False)\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
    "    return src_vocab, tgt_vocab, data_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[80  0  4  1  1  1  1  1]\n",
      " [32 36  4  1  1  1  1  1]]\n",
      "Valid lengths for X: [3 3]\n",
      "Y: [[ 2 15 14  0  4  3  1  1]\n",
      " [ 2 49  0  4  3  1  1  1]]\n",
      "Valid lengths for Y: [6 5]\n"
     ]
    }
   ],
   "source": [
    "src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, num_steps=8)\n",
    "for X, X_vlen, Y, Y_vlen in train_iter:\n",
    "    print('X:', X.astype('int32'))\n",
    "    print('Valid lengths for X:', X_vlen)\n",
    "    print('Y:', Y.astype('int32'))\n",
    "    print('Valid lengths for Y:', Y_vlen)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ Machine translation (MT) refers to the automatic translation of a segment of text from one language to another.\n",
    "+ We read, preprocess, and tokenize the datasets from both source language and target language.\n",
    "\n",
    "##### Exercises\n",
    "1. Find a machine translation dataset online and process it.\n",
    "\n",
    "\n",
    "## 9.6 Encoder-Decoder Architecture\n",
    "The `encoder-decoder` architecture is a neural network design pattern. As shown in `Fig. 9.6.1`, the architecture is partitioned into two parts, the `encoder` and the `decoder`. The `encoder`'s role is to encode the inputs into state, which often contains several tensors. Then the state is passed into the `decoder` to generate the outputs. In machine translation, the `encoder` transforms a source sentence, e.g., \"Hello world.\", into state, e.g., a vector, that captures its semantic information. The `decoder` then uses this state to generate the translated target sentence, e.g., \"Bonjour le monde.\".\n",
    "\n",
    "<img src=\"images/09_12.png\" style=\"width:400px;\"/>\n",
    "\n",
    "In this section, we will show an interface to implement this `encoder-decoder` architecture.\n",
    "\n",
    "### 9.6.1 Encoder\n",
    "The `encoder` is a normal neural network that takes inputs, e.g., a source sentence, to return outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Block):\n",
    "    \"\"\"The base encoder interface for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.2 Decoder\n",
    "The `decoder` has an additional method `init_state` to parse the outputs of the `encoder` with possible additional information, e.g., the valid lengths of inputs, to return the state it needs. In the `forward` method, the `decoder` takes both inputs, e.g., a target sentence and the state. It returns outputs, with potentially modified state if the `encoder` contains RNN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Block):\n",
    "    \"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6.3 Model\n",
    "The `encoder-decoder` model contains both an `encoder` and a `decoder`. We implement its `forward` method for training. It takes both `encoder` inputs and `decoder` inputs, with optional additional arguments. During computation, it first computes `encoder` outputs to initialize the `decoder` state, and then returns the `decoder` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Block):\n",
    "    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ An `encoder-decoder` architecture is a neural network design pattern mainly in natural language processing.\n",
    "+ An `encoder` is a network (FC, CNN, RNN, etc.) that takes the input, and outputs a feature map, a vector or a tensor.\n",
    "+ A `decoder` is a network (usually the same network structure as `encoder`) that takes the feature vector from the `encoder`, and gives the best closest match to the actual input or intended output.\n",
    "\n",
    "##### Exercises\n",
    "1. Besides machine translation, can you think of another application scenarios where an `encoder-decoder` architecture can fit?\n",
    "2. Can you design a deep `encoder-decoder` architecture?\n",
    "\n",
    "\n",
    "## 9.7 Sequence to Sequence\n",
    "The `sequence to sequence` (`seq2seq`) model is based on the `encoder-decoder` architecture to generate a sequence output for a sequence input, as demonstrated in `Fig. 9.7.1`. Both the `encoder` and the `decoder` use recurrent neural networks (`RNN`) to handle sequence inputs of variable length. The hidden state of the `encoder` is used directly to initialize the `decoder` hidden state to pass information from the `encoder` to the `decoder`.\n",
    "\n",
    "<img src=\"images/09_13.png\" style=\"width:500px;\"/>\n",
    "\n",
    "The layers in the `encoder` and the `decoder` are illustrated in `Fig. 9.7.2`.\n",
    "\n",
    "<img src=\"images/09_14.png\" style=\"width:350px;\"/>\n",
    "\n",
    "In this section we will explain and implement the `seq2seq` model to train on the machine translation dataset.\n",
    "\n",
    "### 9.7.1 Encoder\n",
    "Recall that the `encoder` of `seq2seq` can transform the inputs of variable length to a fixed-length context vector $\\mathbf{c}$ by encoding the sequence information into $\\mathbf{c}$. We usually use RNN layers within the `encoder`. Suppose that we have an input sequence $x_1, \\ldots, x_T$, where $x_t$ is the $t^\\mathrm{th}$ word. At timestep $t$, the RNN will have two vectors as the input: the feature vector $\\mathbf{x}_t$ of $x_t$ and the hidden state of the last timestep $\\mathbf{h}_{t-1}$. Let us denote the transformation of the RNN's hidden states by a function $f$:\n",
    "\n",
    "$$\\mathbf{h}_t = f (\\mathbf{x}_t, \\mathbf{h}_{t-1}).$$\n",
    "\n",
    "Next, the `encoder` captures information of all the hidden states and encodes it into the context vector $\\mathbf{c}$ with a function $q$:\n",
    "\n",
    "$$\\mathbf{c} = q (\\mathbf{h}_1, \\ldots, \\mathbf{h}_T).$$\n",
    "\n",
    "For example, if we choose $q$ as $q (\\mathbf{h}_1, \\ldots, \\mathbf{h}_T) = \\mathbf{h}_T$, then the context vector will be the final hidden state $\\mathbf{h}_T$.\n",
    "\n",
    "So far what we describe above is a unidirectional RNN, where each timestep's hidden state depends only on the previous timesteps'. We can also use other forms of RNNs such as `GRU`, `LSTM`, and `bidirectional RNN` to encode the sequential input.\n",
    "\n",
    "Now let us implement the `seq2seq`'s `encoder`. Here we use the word embedding layer to obtain the feature vector according to the word index of the input language. Those feature vectors will be fed to a multi-layer `LSTM`. The input for the encoder is a batch of sequences, which is 2-D tensor with shape `(batch size, sequence length)`. The encoder returns both the `LSTM` outputs, i.e., hidden states of all the timesteps, as well as the hidden state and the memory cell of the final timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        X = self.embedding(X)  # X shape: (batch_size, seq_len, embed_size)\n",
    "        # RNN needs first axes to be timestep, i.e., seq_len\n",
    "        X = X.swapaxes(0, 1)\n",
    "        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)\n",
    "        out, state = self.rnn(X, state)\n",
    "        # out shape: (seq_len, batch_size, num_hiddens)\n",
    "        # state shape: (num_layers, batch_size, num_hiddens),\n",
    "        # where \"state\" contains the hidden state and the memory cell\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a minibatch sequence input with a batch size of 4 and 7 timesteps. We assume the number of hidden layers of the `LSTM` unit is 2 and the number of hidden units is 16. The output shape returned by the encoder after performing forward calculation on the input is `(number of timesteps, batch size, number of hidden units)`. The shape of the multi-layer hidden state of the gated recurrent unit in the final timestep is `(number of hidden layers, batch size, number of hidden units)`. For the gated recurrent unit, the state list contains only one element, which is the hidden state. If long short-term memory is used, the state list will also contain another element, which is the memory cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.initialize()\n",
    "X = np.zeros((4, 7))\n",
    "output, state = encoder(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since an `LSTM` is used, the state list will contain both the hidden state and the memory cell with same shape `(number of hidden layers, batch size, number of hidden units)`. However, if a GRU is used, the state list will contain only one element---the hidden state in the final timestep with shape `(number of hidden layers, batch size, number of hidden units)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, (2, 4, 16), (2, 4, 16))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state), state[0].shape, state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.2 Decoder\n",
    "As we just introduced, the context vector $\\mathbf{c}$ encodes the information from the whole input sequence $x_1, \\ldots, x_T$. Suppose that the given outputs in the training set are $y_1, \\ldots, y_{T'}$. At each timestep $t'$, the conditional probability of output $y_{t'}$ will depend on the previous output sequence $y_1, \\ldots, y_{t'-1}$ and the context vector $\\mathbf{c}$, i.e.,\n",
    "\n",
    "$$P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c}).$$\n",
    "\n",
    "Hence, we can use another RNN as the `decoder`. At timestep $t'$, the `decoder` will update its hidden state $\\mathbf{s}_{t'}$ using three inputs: \n",
    "+ the feature vector $\\mathbf{y}_{t'-1}$ of $y_{t'-1}$\n",
    "+ the context vector $\\mathbf{c}$\n",
    "+ the hidden state of the last timestep $\\mathbf{s}_{t'-1}$\n",
    "\n",
    "Let us denote the transformation of the RNN's hidden states within the `decoder` by a function $g$:\n",
    "\n",
    "$$\\mathbf{s}_{t'} = g(\\mathbf{y}_{t'-1}, \\mathbf{c}, \\mathbf{s}_{t'-1}).$$\n",
    "\n",
    "When implementing the `decoder`, we directly use the hidden state of the `encoder` in the final timestep as the initial hidden state of the decoder. This requires that the `encoder` and `decoder` RNNs have the same numbers of layers and hidden units. The `LSTM` forward calculation of the `decoder` is similar to that of the `encoder`. The only difference is that we add a dense layer after the `LSTM` layers, where the hidden size is the vocabulary size. The dense layer will predict the confidence score for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.embedding(X).swapaxes(0, 1)\n",
    "        out, state = self.rnn(X, state)\n",
    "        # Make the batch to be the first dimension to simplify loss computation\n",
    "        out = self.dense(out).swapaxes(0, 1)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `decoder` with the same hyper-parameters as the `encoder`. As we can see, the output shape is changed to `(batch size, the sequence length, vocabulary size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 7, 10), 2, (2, 4, 16), (2, 4, 16))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "decoder.initialize()\n",
    "state = decoder.init_state(encoder(X))\n",
    "out, state = decoder(X, state)\n",
    "out.shape, len(state), state[0].shape, state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.3 The Loss Function\n",
    "For each timestep, the `decoder` outputs a vocabulary-size confidence score vector to predict words. Similar to language modeling, we can apply softmax to obtain the probabilities and then use cross-entropy loss to calculate the loss. Note that we padded the target sentences to make them have the same length, but we do not need to compute the loss on the padding symbols.\n",
    "\n",
    "To implement the loss function that filters out some entries, we will use an operator called `SequenceMask`. It can specify to mask the first dimension `(axis=0)` or the second one `(axis=1)`. If the second one is chosen, given a valid length vector `len` and 2-dim input $X$, this operator sets `X[i, len[i]:] = 0` for all $i$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [4., 5., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "npx.sequence_mask(X, np.array([1, 2]), True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to $n$-dim tensor $X$, it sets `X[i, len[i]:, :, ..., :] = 0`. In addition, we can specify the filling value such as $-1$ as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  1.,  1.,  1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.]],\n",
       "\n",
       "       [[ 1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.],\n",
       "        [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.ones((2, 3, 4))\n",
    "npx.sequence_mask(X, np.array([1, 2]), True, value=-1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the masked version of the softmax cross-entropy loss. Note that each `Gluon` loss function allows to specify per-example weights, in default they are 1s. Then we can just use a zero weight for each example we would like to remove. So our customized loss function accepts an additional `valid_len` argument to ignore some failing elements in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):\n",
    "    # pred shape: (batch_size, seq_len, vocab_size)\n",
    "    # label shape: (batch_size, seq_len)\n",
    "    # valid_len shape: (batch_size, )\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        # weights shape: (batch_size, seq_len, 1)\n",
    "        weights = np.expand_dims(np.ones_like(label), axis=-1)\n",
    "        weights = npx.sequence_mask(weights, valid_len, True, axis=1)\n",
    "        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sanity check, we create identical three sequences, keep 4 elements for the first sequence, 2 elements for the second sequence, and none for the last one. Then the first example loss should be 2 times larger than the second one, and the last loss should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.3025851, 1.1512926, 0.       ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(np.ones((3, 4, 10)), np.ones((3, 4)), np.array([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.4 Training\n",
    "During training, if the target sequence has length $n$, we feed the first $n-1$ tokens into the decoder as inputs, and the last $n-1$ tokens are used as ground truth label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_s2s_ch9(model, data_iter, lr, num_epochs, ctx):\n",
    "    model.initialize(init.Xavier(), force_reinit=True, ctx=ctx)\n",
    "    trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, num_epochs], ylim=[0, 0.25])\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # loss_sum, num_tokens\n",
    "        for batch in data_iter:\n",
    "            X, X_vlen, Y, Y_vlen = [x.as_in_ctx(ctx) for x in batch]\n",
    "            Y_input, Y_label, Y_vlen = Y[:, :-1], Y[:, 1:], Y_vlen-1\n",
    "            with autograd.record():\n",
    "                Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)\n",
    "                l = loss(Y_hat, Y_label, Y_vlen)\n",
    "            l.backward()\n",
    "            d2l.grad_clipping(model, 1)\n",
    "            num_tokens = Y_vlen.sum()\n",
    "            trainer.step(num_tokens)\n",
    "            metric.add(l.sum(), num_tokens)\n",
    "        if epoch % 10 == 0:\n",
    "            animator.add(epoch, (metric[0]/metric[1],))\n",
    "    print('loss %.3f, %d tokens/sec on %s ' % (metric[0]/metric[1], metric[1]/timer.stop(), ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a model instance and set hyper-parameters. Then, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()\n",
    "\n",
    "src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7.5 Predicting\n",
    "Here we implement the simplest method, greedy search, to generate an output sequence. As illustrated in `Fig. 9.7.3`, during predicting, we feed the same `<bos>` token to the decoder as training at timestep 0. But the input token for a later timestep is the predicted token from the previous timestep.\n",
    "\n",
    "<img src=\"images/09_15.png\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_s2s_ch9(model, src_sentence, src_vocab, tgt_vocab, num_steps, ctx):\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')]\n",
    "    enc_valid_len = np.array([len(src_tokens)], ctx=ctx)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    enc_X = np.array(src_tokens, ctx=ctx)\n",
    "    # Add the batch_size dimension\n",
    "    enc_outputs = model.encoder(np.expand_dims(enc_X, axis=0), enc_valid_len)\n",
    "    dec_state = model.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=ctx), axis=0)\n",
    "    predict_tokens = []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "        # The token with highest score is used as the next timestep input\n",
    "        dec_X = Y.argmax(axis=2)\n",
    "        py = dec_X.squeeze(axis=0).astype('int32').item()\n",
    "        if py == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        predict_tokens.append(py)\n",
    "    return ' '.join(tgt_vocab.to_tokens(predict_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try several examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in ['Go .', 'Wow !', \"I'm OK .\", 'I won !']:\n",
    "    print(sentence + ' => ' + predict_s2s_ch9(model, sentence, src_vocab, tgt_vocab, num_steps, ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ The sequence to sequence (seq2seq) model is based on the encoder-decoder architecture to generate a sequence output from a sequence input.\n",
    "+ We use multiple LSTM layers for both the encoder and the decoder.\n",
    "\n",
    "##### Exercises\n",
    "1. Can you think of other use cases of seq2seq besides neural machine translation?\n",
    "2. What if the input sequence in the example of this section is longer?\n",
    "3. If we do not use the SequenceMask in the loss function, what may happen?\n",
    "\n",
    "\n",
    "## 9.8 Beam Search\n",
    "In `Section 9.7`, we discussed how to train an `encoder-decoder` with input and output sequences that are both of variable length. In this section, we are going to introduce how to use the `encoder-decoder` to predict sequences of variable length.\n",
    "\n",
    "As in `Section 9.5`, when preparing to train the dataset, we normally attach a special symbol `<eos>` after each sentence to indicate the termination of the sequence. We will continue to use this mathematical symbol in the discussion below. For ease of discussion, we assume that the output of the `decoder` is a sequence of text. Let the size of output text dictionary $\\mathcal{Y}$ (contains special symbol `<eos>`) be $\\left|\\mathcal{Y}\\right|$, and the maximum length of the output sequence be $T'$. There are a total $\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T'})$ types of possible output sequences. All the subsequences after the special symbol `<eos>` in these output sequences will be discarded. Besides, we still denote the context vector as $\\mathbf{c}$, which encodes information of all the hidden states from the input.\n",
    "\n",
    "### 9.8.1 Greedy Search\n",
    "First, we will take a look at a simple solution: `greedy search`. For any timestep $t'$ of the output sequence, we are going to search for the word with the highest conditional probability from $|\\mathcal{Y}|$ numbers of words, with\n",
    "\n",
    "$$y_{t'} = \\operatorname*{argmax}{y \\in \\mathcal{Y}} P(y \\mid y_1, \\ldots, y{t'-1}, \\mathbf{c})$$\n",
    "\n",
    "as the output. Once the `<eos>` symbol is detected, or the output sequence has reached its maximum length $T'$, the output is completed.\n",
    "\n",
    "As we mentioned in our discussion of the decoder, the conditional probability of generating an output sequence based on the input sequence is $\\prod_{t'=1}^{T'} P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$. We will take the output sequence with the highest conditional probability as the optimal sequence. The main problem with greedy search is that there is no guarantee that the optimal sequence will be obtained.\n",
    "\n",
    "Take a look at the example below. We assume that there are four words `A`, `B`, `C`, and `<eos>` in the output dictionary. The four numbers under each timestep in `Fig. 9.8.1` represent the conditional probabilities of generating `A`, `B`, `C`, and `<eos>` at that timestep, respectively. At each timestep, greedy search selects the word with the highest conditional probability. Therefore, the output sequence `A`, `B`, `C`, and `<eos>` will be generated in `Fig. 9.8.1`. The conditional probability of this output sequence is $0.5\\times0.4\\times0.4\\times0.6 = 0.048$.\n",
    "\n",
    "<img src=\"images/09_16.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Now, we will look at another example shown in `Fig. 9.8.2`. Unlike in `Fig. 9.8.1`, the following figure `Fig. 9.8.2` selects the word `C`, which has the second highest conditional probability at timestep 2. Since the output subsequences of timesteps 1 and 2, on which timestep 3 is based, are changed from `A` and `B` in `Fig. 9.8.1` to `A` and `C` in `Fig. 9.8.2`, the conditional probability of each word generated at timestep 3 has also changed in `Fig. 9.8.2`. We choose the word `B`, which has the highest conditional probability. Now, the output subsequences of timestep 4 based on the first three timesteps are `A`, `C`, and `B`, which are different from `A`, `B`, and `C` in `Fig. 9.8.1`. Therefore, the conditional probability of generating each word in timestep 4 in `Fig. 9.8.2` is also different from that in `Fig. 9.8.1`. We find that the conditional probability of the output sequence `A`, `C`, `B`, and `<eos>` at the current timestep is $0.5\\times0.3 \\times0.6\\times0.6=0.054$, which is higher than the conditional probability of the output sequence obtained by greedy search. Therefore, the output sequence `A`, `B`, `C`, and `<eos>` obtained by the greedy search is not an optimal sequence.\n",
    "\n",
    "<img src=\"images/09_17.png\" style=\"width:600px;\"/>\n",
    "\n",
    "### 9.8.2 Exhaustive Search\n",
    "If the goal is to obtain the optimal sequence, we may consider using exhaustive search: an exhaustive examination of all possible output sequences, which outputs the sequence with the highest conditional probability.\n",
    "\n",
    "Although we can use an exhaustive search to obtain the optimal sequence, its computational overhead $\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T'})$ is likely to be excessively high. For example, when $|\\mathcal{Y}|=10000$ and $T'=10$, we will need to evaluate $10000^{10} = 10^{40}$ sequences. This is next to impossible to complete. The computational overhead of greedy search is $\\mathcal{O}(\\left|\\mathcal{Y}\\right|T')$, which is usually significantly less than the computational overhead of an exhaustive search. For example, when $|\\mathcal{Y}|=10000$ and $T'=10$, we only need to evaluate $10000\\times10=1\\times10^5$ sequences.\n",
    "\n",
    "### 9.8.3 Beam Search\n",
    "`Beam search` is an improved algorithm based on greedy search. It has a hyper-parameter named `beam size`, $k$. At timestep 1, we select $k$ words with the highest conditional probability to be the first word of the $k$ candidate output sequences. For each subsequent timestep, we are going to select the $k$ output sequences with the highest conditional probability from the total of $k\\left|\\mathcal{Y}\\right|$ possible output sequences based on the $k$ candidate output sequences from the previous timestep. These will be the candidate output sequences for that timestep. Finally, we will filter out the sequences containing the special symbol `<eos>` from the candidate output sequences of each timestep and discard all the subsequences after it to obtain a set of final candidate output sequences.\n",
    "\n",
    "<img src=\"images/09_18.png\" style=\"width:600px;\"/>\n",
    "    \n",
    "`Fig. 9.8.3` demonstrates the process of beam search with an example. Suppose that the vocabulary of the output sequence contains only five elements: $\\mathcal{Y} = {A, B, C, D, E}$ where one of them is a special symbol `<eos>`. Set beam size to 2, the maximum length of the output sequence to 3. At timestep 1 of the output sequence, suppose the words with the highest conditional probability $P(y_1 \\mid \\mathbf{c})$ are $A$ and $C$. At timestep 2, for all $y_2 \\in \\mathcal{Y},$ we compute\n",
    "\n",
    "$$P(A, y_2 \\mid \\mathbf{c}) = P(A \\mid \\mathbf{c})P(y_2 \\mid A, \\mathbf{c})$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P(C, y_2 \\mid \\mathbf{c}) = P(C \\mid \\mathbf{c})P(y_2 \\mid C, \\mathbf{c}),$$\n",
    "\n",
    "and pick the largest two among these 10 values, say\n",
    "\n",
    "$$P(A, B \\mid \\mathbf{c}) \\text{ and } P(C, E \\mid \\mathbf{c}).$$\n",
    "\n",
    "Then at timestep 3, for all $y_3 \\in \\mathcal{Y}$, we compute\n",
    "\n",
    "$$P(A, B, y_3 \\mid \\mathbf{c}) = P(A, B \\mid \\mathbf{c})P(y_3 \\mid A, B, \\mathbf{c})$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P(C, E, y_3 \\mid \\mathbf{c}) = P(C, E \\mid \\mathbf{c})P(y_3 \\mid C, E, \\mathbf{c}),$$\n",
    "\n",
    "and pick the largest two among these 10 values, say\n",
    "\n",
    "$$P(A, B, D \\mid \\mathbf{c}) \\text{ and } P(C, E, D \\mid \\mathbf{c}).$$\n",
    "\n",
    "As a result, we obtain 6 candidates output sequences:\n",
    "1. $A$\n",
    "2. $C$\n",
    "3. $A$, $B$\n",
    "4. $C$, $E$\n",
    "5. $A$, $B$, $D$\n",
    "6. $C$, $E$, $D$\n",
    "\n",
    "In the end, we will get the set of final candidate output sequences based on these 6 sequences.\n",
    "\n",
    "In the set of final candidate output sequences, we will take the sequence with the highest score as the output sequence from those below:\n",
    "\n",
    "$$ \\frac{1}{L^\\alpha} \\log P(y_1, \\ldots, y_{L}) = \\frac{1}{L^\\alpha} \\sum_{t'=1}^L \\log P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c}),$$\n",
    "\n",
    "Here, $L$ is the length of the final candidate sequence and the selection for $\\alpha$ is generally 0.75. The $L^\\alpha$ on the denominator is a penalty on the logarithmic addition scores for the longer sequences above. The computational overhead $\\mathcal{O}(k\\left|\\mathcal{Y}\\right|T')$ of the beam search can be obtained through analysis. The result is between the computational overhead of greedy search and exhaustive search. In addition, greedy search can be treated as a beam search with a beam size of 1. Beam search strikes a balance between computational overhead and search quality using a flexible beam size of $k$.\n",
    "\n",
    "##### Summary\n",
    "+ Methods for predicting variable-length sequences include greedy search, exhaustive search, and beam search.\n",
    "+ Beam search strikes a balance between computational overhead and search quality using a flexible beam size.\n",
    "\n",
    "##### Exercises\n",
    "1. Can we treat an exhaustive search as a beam search with a special beam size? Why?\n",
    "2. We used language models to generate sentences in :numref:sec_rnn_scratch. Which kind of search does this output use? Can you improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
