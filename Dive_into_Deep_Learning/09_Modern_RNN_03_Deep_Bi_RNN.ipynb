{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import d2l\n",
    "import os\n",
    "\n",
    "from mxnet import autograd, np, npx, gluon, init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet.gluon import rnn\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  09. Modern Recurrent Neural Networks\n",
    "Although we have learned the basics of recurrent neural networks, they are not sufficient for a practitioner to solve today's sequence learning problems. For instance, given the numerical unstability during gradient calculation, gated recurrent neural networks are much more common in practice. We will begin by introducing two of such widely-used networks, namely `gated recurrent units` (`GRU`) and `long short term memory` (`LSTM`), with illustrations using the same language modeling problem as introduced in `Chapter 8`.\n",
    "\n",
    "Furthermore, we will modify recurrent neural networks with a single undirectional hidden layer. We will describe deep architectures, and discuss the bidirectional design with both forward and backward recursion. They are frequently adopted in modern recurrent networks.\n",
    "\n",
    "In fact, a large portion of sequence learning problems such as automatic speech recognition, text to speech, and machine translation, consider both inputs and outputs to be sequences of arbitrary length. Finally, we will take machine translation as an example, and introduce the encoder-decoder architecture based on recurrent neural networks and modern practices for such sequence to sequence learning problems.\n",
    "\n",
    "\n",
    "## 9.3 Deep Recurrent Neural Networks\n",
    "Up to now, we only discussed recurrent neural networks with a single unidirectional hidden layer. In it the specific functional form of how latent variables and observations interact was rather arbitrary. This is not a big problem as long as we have enough flexibility to model different types of interactions. With a single layer, however, this can be quite challenging. In the case of the perceptron, we fixed this problem by adding more layers. Within RNNs this is a bit trickier, since we first need to decide how and where to add extra nonlinearity. Our discussion below focuses primarily on `LSTM`, but it applies to other sequence models, too.\n",
    "\n",
    "+ We could add extra nonlinearity to the gating mechanisms. That is, instead of using a single perceptron we could use multiple layers. This leaves the mechanism of the `LSTM` unchanged. Instead it makes it more sophisticated. This would make sense if we were led to believe that the `LSTM` mechanism describes some form of universal truth of how latent variable autoregressive models work.\n",
    "\n",
    "+ We could stack multiple layers of `LSTM` on top of each other. This results in a mechanism that is more flexible, due to the combination of several simple layers. In particular, data might be relevant at different levels of the stack. For instance, we might want to keep high-level data about financial market conditions (bear or bull market) available, whereas at a lower level we only record shorter-term temporal dynamics.\n",
    "\n",
    "Beyond all this abstract discussion it is probably easiest to understand the family of models we are interested in by reviewing `Fig. 9.3.1`. It describes a deep recurrent neural network with $L$ hidden layers. Each hidden state is continuously passed to both the next timestep of the current layer and the current timestep of the next layer.\n",
    "\n",
    "<img src=\"images/09_09.png\" style=\"width:350px;\"/>\n",
    "\n",
    "### 9.3.1 Functional Dependencies\n",
    "At timestep $t$ we assume that we have a minibatch $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ (number of examples: $n$, number of inputs: $d$). The hidden state of hidden layer $\\ell$ ($\\ell=1,\\ldots, L$) is $\\mathbf{H}_t^{(\\ell)} \\in \\mathbb{R}^{n \\times h}$ (number of hidden units: $h$), the output layer variable is $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$ (number of outputs: $q$) and a hidden layer activation function $f_l$ for layer $l$. We compute the hidden state of layer $1$ as before, using $\\mathbf{X}_t$ as input. For all subsequent layers, the hidden state of the previous layer is used in its place.\n",
    "\n",
    "$$\\begin{aligned} \\mathbf{H}_t^{(1)} & = f_1\\left(\\mathbf{X}_t, \\mathbf{H}_{t-1}^{(1)}\\right), \\\\ \\mathbf{H}_t^{(l)} & = f_l\\left(\\mathbf{H}_t^{(l-1)}, \\mathbf{H}_{t-1}^{(l)}\\right). \\end{aligned}$$\n",
    "\n",
    "Finally, the output layer is only based on the hidden state of hidden layer $L$. We use the output function $g$ to address this:\n",
    "\n",
    "$$\\mathbf{O}_t = g \\left(\\mathbf{H}_t^{(L)}\\right).$$\n",
    "\n",
    "Just as with multilayer perceptrons, the number of hidden layers $L$ and number of hidden units $h$ are hyper parameters. In particular, we can pick a regular `RNN`, a `GRU`, or an `LSTM` to implement the model.\n",
    "\n",
    "### 9.3.2 Concise Implementation\n",
    "Fortunately many of the logistical details required to implement multiple layers of an RNN are readily available in `Gluon`. To keep things simple we only illustrate the implementation using such built-in functionality. The code is very similar to the one we used previously for `LSTM`. In fact, the only difference is that we specify the number of layers explicitly rather than picking the default of a single layer. Let us begin by importing the appropriate modules and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architectural decisions (such as choosing parameters) are very similar to those of previous sections. We pick the same number of inputs and outputs as we have distinct tokens, i.e., `vocab_size`. The number of hidden units is still 256. The only difference is that we now select a nontrivial number of layers `num_layers = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, num_layers, ctx = len(vocab), 256, 3, d2l.try_gpu()\n",
    "lstm_layer = rnn.LSTM(num_hiddens, num_layers=num_layers)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3 Training\n",
    "The actual invocation logic is identical to before. The only difference is that we now instantiate two layers with `LSTM`. This rather more complex architecture and the large number of epochs slow down training considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 2\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ In deep recurrent neural networks, hidden state information is passed to the next timestep of the current layer and the current timestep of the next layer.\n",
    "+ There exist many different flavors of deep RNNs, such as LSTMs, GRUs, or regular RNNs. Conveniently these models are all available as parts of the rnn module in Gluon.\n",
    "+ Initialization of the models requires care. Overall, deep RNNs require considerable amount of work (such as learning rate and clipping) to ensure proper convergence.\n",
    "\n",
    "##### Exercises\n",
    "1. Try to implement a two-layer RNN from scratch using the single layer implementation we discussed in `Section 8.5`.\n",
    "2. Replace the LSTM by a GRU and compare the accuracy.\n",
    "3. Increase the training data to include multiple books. How low can you go on the perplexity scale?\n",
    "4. Would you want to combine sources of different authors when modeling text? Why is this a good idea? What could go wrong?\n",
    "\n",
    "\n",
    "## 9.4 Bidirectional Recurrent Neural Networks\n",
    "So far we assumed that our goal is to model the next word given what we have seen so far, e.g., in the context of a time series or in the context of a language model. While this is a typical scenario, it is not the only one we might encounter. To illustrate the issue, consider the following three tasks of filling in the blanks in a text:\n",
    "1. `I am _____`\n",
    "2. `I am _____ very hungry.`\n",
    "3. `I am _____ very hungry, I could eat half a pig.`\n",
    "\n",
    "Depending on the amount of information available, we might fill the blanks with very different words such as \"happy\", \"not\", and \"very\". Clearly the end of the phrase (if available) conveys significant information about which word to pick. A sequence model that is incapable of taking advantage of this will perform poorly on related tasks. For instance, to do well in named entity recognition (e.g., to recognize whether \"Green\" refers to \"Mr. Green\" or to the color) longer-range context is equally vital. To get some inspiration for addressing the problem let us take a detour to graphical models.\n",
    "\n",
    "### 9.4.1 Dynamic Programming\n",
    "This section serves to illustrate the dynamic programming problem. The specific technical details do not matter for understanding the deep learning counterpart but they help in motivating why one might use deep learning and why one might pick specific architectures.\n",
    "\n",
    "If we want to solve the problem using graphical models we could for instance design a latent variable model as follows. We assume that there exists some latent variable $h_t$ which governs the emissions $x_t$ that we observe via $p(x_t \\mid h_t)$. Moreover, the transitions $h_t \\to h_{t+1}$ are given by some state transition probability $p(h_t+1 \\mid h_{t})$. The graphical model is then a `Hidden Markov Model` (`HMM`) as in `Fig. 9.4.1`.\n",
    "\n",
    "<img src=\"images/09_10.png\" style=\"width:300px;\"/>\n",
    "\n",
    "Thus, for a sequence of $T$ observations we have the following joint probability distribution over observed and hidden states:\n",
    "\n",
    "$$p(x, h) = p(h_1) p(x_1 \\mid h_1) \\prod_{t=2}^T p(h_t \\mid h_{t-1}) p(x_t \\mid h_t).$$\n",
    "\n",
    "Now assume that we observe all $x_i$ with the exception of some $x_j$ and it is our goal to compute $p(x_j \\mid x^{-j})$, where $x^{-j} = (x_1, x_2, \\ldots, x_{j-1})$. To accomplish this we need to sum over all possible choices of $h = (h_1, \\ldots, h_T)$. In case $h_i$ can take on $k$ distinct values, this means that we need to sum over $k^T$ terms---mission impossible! Fortunately there is an elegant solution for this: dynamic programming. To see how it works, consider summing over the first two hidden variable $h_1$ and $h_2$. This yields:\n",
    "\n",
    "$$\\begin{aligned} p(x) & = \\sum_{h_1, \\ldots, h_T} p(x_1, \\ldots, x_T; h_1, \\ldots, h_T) \\\\ & = \\sum_{h_1, \\ldots, h_T} p(h_1) p(x_1 \\mid h_1) \\prod_{t=2}^T p(h_t \\mid h_{t-1}) p(x_t \\mid h_t) \\\\ & = \\sum_{h_2, \\ldots, h_T} \\underbrace{\\left[\\sum_{h_1} p(h_1) p(x_1 \\mid h_1) p(h_2 \\mid h_1)\\right]}_{=: \\pi_2(h_2)} p(x_2 \\mid h_2) \\prod_{t=3}^T p(h_t \\mid h_{t-1}) p(x_t \\mid h_t) \\\\ & = \\sum_{h_3, \\ldots, h_T} \\underbrace{\\left[\\sum_{h_2} \\pi_2(h_2) p(x_2 \\mid h_2) p(h_3 \\mid h_2)\\right]}_{=: \\pi_3(h_3)} p(x_3 \\mid h_3) \\prod_{t=4}^T p(h_t \\mid h_{t-1}) p(x_t \\mid h_t)\\\\ & = \\dots \\\\ & = \\sum_{h_T} \\pi_T(h_T) p(x_T \\mid h_T). \\end{aligned}$$\n",
    "\n",
    "In general we have the forward recursion as\n",
    "\n",
    "$$\\pi_{t+1}(h_{t+1}) = \\sum_{h_t} \\pi_t(h_t) p(x_t \\mid h_t) p(h_{t+1} \\mid h_t).$$\n",
    "\n",
    "The recursion is initialized as $\\pi_1(h_1) = p(h_1)$. In abstract terms this can be written as $\\pi_{t+1} = f(\\pi_t, x_t)$, where $f$ is some learnable function. This looks very much like the update equation in the hidden variable models we discussed so far in the context of RNNs. Entirely analogously to the forward recursion, we can also start a backward recursion. This yields:\n",
    "\n",
    "$$\\begin{aligned} p(x) & = \\sum_{h_1, \\ldots, h_T} p(x_1, \\ldots, x_T; h_1, \\ldots, h_T) \\\\ & = \\sum_{h_1, \\ldots, h_T} \\prod_{t=1}^{T-1} p(h_t \\mid h_{t-1}) p(x_t \\mid h_t) \\cdot p(h_T \\mid h_{T-1}) p(x_T \\mid h_T) \\\\ & = \\sum_{h_1, \\ldots, h_{T-1}} \\prod_{t=1}^{T-1} p(h_t \\mid h_{t-1}) p(x_t \\mid h_t) \\cdot \\underbrace{\\left[\\sum_{h_T} p(h_T \\mid h_{T-1}) p(x_T \\mid h_T)\\right]}_{=: \\rho{T-1}(h_{T-1})} \\\\ & = \\sum_{h_1, \\ldots, h_{T-2}} \\prod_{t=1}^{T-2} p(h_t \\mid h_{t-1}) p(x_t \\mid h_t) \\cdot \\underbrace{\\left[\\sum_{h_{T-1}} p(h_{T-1} \\mid h_{T-2}) p(x_{T-1} \\mid h_{T-1}) \\rho_{T-1}(h_{T-1}) \\right]}_{=: \\rho{T-2}(h_{T-2})} \\\\ & = \\ldots, \\\\ & = \\sum_{h_1} p(h_1) p(x_1 \\mid h_1)\\rho_{1}(h_{1}). \\end{aligned}$$\n",
    "\n",
    "We can thus write the backward recursion as\n",
    "\n",
    "$$\\rho_{t-1}(h_{t-1})= \\sum_{h_{t}} p(h_{t} \\mid h_{t-1}) p(x_{t} \\mid h_{t}) \\rho_{t}(h_{t}),$$\n",
    "\n",
    "with initialization $\\rho_T(h_T) = 1$. These two recursions allow us to sum over $T$ variables in $\\mathcal{O}(kT)$ (linear) time over all values of $(h_1, \\ldots, h_T)$ rather than in exponential time. This is one of the great benefits of the probabilistic inference with graphical models. It is a very special instance of the (`Aji & McEliece, 2000`) proposed in 2000 by Aji and McEliece. Combining both forward and backward pass, we are able to compute\n",
    "\n",
    "$$p(x_j \\mid x_{-j}) \\propto \\sum_{h_j} \\pi_j(h_j) \\rho_j(h_j) p(x_j \\mid h_j).$$\n",
    "\n",
    "Note that in abstract terms the backward recursion can be written as $\\rho_{t-1} = g(\\rho_t, x_t)$, where $g$ is a learnable function. Again, this looks very much like an update equation, just running backwards unlike what we have seen so far in RNNs. Indeed, HMMs benefit from knowing future data when it is available. Signal processing scientists distinguish between the two cases of knowing and not knowing future observations as interpolation v.s. extrapolation. See the introductory chapter of the book by (`Doucet et al., 2001`) on sequential Monte Carlo algorithms for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2 Bidirectional Model\n",
    "If we want to have a mechanism in RNNs that offers comparable look-ahead ability as in `HMM`, we need to modify the recurrent net design that we have seen so far. Fortunately, this is easy conceptually. Instead of running an RNN only in the forward mode starting from the first symbol, we start another one from the last symbol running from back to front. `Bidirectional recurrent neural networks` add a hidden layer that passes information in a backward direction to more flexibly process such information. `Fig. 9.4.2` illustrates the architecture of a bidirectional recurrent neural network with a single hidden layer.\n",
    "\n",
    "<img src=\"images/09_11.png\" style=\"width:450px;\"/>\n",
    "\n",
    "In fact, this is not too dissimilar to the forward and backward recursion we encountered above. The main distinction is that in the previous case these equations had a specific statistical meaning. Now they are devoid of such easily accessible interpretation and we can just treat them as generic functions. This transition epitomizes many of the principles guiding the design of modern deep networks: first, use the type of functional dependencies of classical statistical models, and then use the models in a generic form.\n",
    "\n",
    "##### Definition\n",
    "Bidirectional RNNs were introduced by (`Schuster & Paliwal, 1997`). For a detailed discussion of the various architectures see also the paper by (`Graves & Schmidhuber, 2005`). Let us look at the specifics of such a network.\n",
    "\n",
    "For a given timestep $t$, the minibatch input is $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ (number of examples: $n$, number of inputs: $d$) and the hidden layer activation function is $\\phi$. In the bidirectional architecture, we assume that the forward and backward hidden states for this timestep are $\\overrightarrow{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$ and $\\overleftarrow{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$ respectively. Here $h$ indicates the number of hidden units. We compute the forward and backward hidden state updates as follows:\n",
    "\n",
    "$$ \\begin{aligned} \\overrightarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)} + \\mathbf{b}_h^{(f)}),\\\\ \\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)} + \\mathbf{b}_h^{(b)}). \\end{aligned} $$\n",
    "\n",
    "Here, the weight parameters $\\mathbf{W}_{xh}^{(f)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh}^{(f)} \\in \\mathbb{R}^{h \\times h}, \\mathbf{W}_{xh}^{(b)} \\in \\mathbb{R}^{d \\times h}, \\text{ and } \\mathbf{W}_{hh}^{(b)} \\in \\mathbb{R}^{h \\times h}$, and bias parameters $\\mathbf{b}_h^{(f)} \\in \\mathbb{R}^{1 \\times h} \\text{ and } \\mathbf{b}_h^{(b)} \\in \\mathbb{R}^{1 \\times h}$ are all model parameters.\n",
    "\n",
    "Then we concatenate the forward and backward hidden states $\\overrightarrow{\\mathbf{H}}_t$ and $\\overleftarrow{\\mathbf{H}}_t$ to obtain the hidden state $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$ and feed it to the output layer. In deep bidirectional RNNs, the information is passed on as input to the next bidirectional layer. Last, the output layer computes the output $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$ (number of outputs: $q$):\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "Here, the weight parameter $\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}$ and the bias parameter $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ are the model parameters of the output layer. The two directions can have different numbers of hidden units.\n",
    "\n",
    "##### Computational Cost and Applications\n",
    "One of the key features of a bidirectional RNN is that information from both ends of the sequence is used to estimate the output. That is, we use information from both future and past observations to predict the current one (a smoothing scenario). In the case of language models this is not quite what we want. After all, we do not have the luxury of knowing the next to next symbol when predicting the next one. Hence, if we were to use a bidirectional RNN naively we would not get a very good accuracy: during training we have past and future data to estimate the present. During test time we only have past data and thus poor accuracy (we will illustrate this in an experiment below).\n",
    "\n",
    "To add insult to injury, bidirectional RNNs are also exceedingly slow. The main reasons for this are that they require both a forward and a backward pass and that the backward pass is dependent on the outcomes of the forward pass. Hence, gradients will have a very long dependency chain.\n",
    "\n",
    "In practice bidirectional layers are used very sparingly and only for a narrow set of applications, such as filling in missing words, annotating tokens (e.g., for named entity recognition), or encoding sequences wholesale as a step in a sequence processing pipeline (e.g., for machine translation). In short, handle with care!\n",
    "\n",
    "##### Training a Bidirectional RNN for the Wrong Application\n",
    "If we were to ignore all advice regarding the fact that bidirectional LSTMs use past and future data and simply apply it to language models, we will get estimates with acceptable perplexity. Nonetheless, the ability of the model to predict future symbols is severely compromised as the example below illustrates. Despite reasonable perplexity, it only generates gibberish even after many iterations. We include the code below as a cautionary example against using them in the wrong context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "vocab_size, num_hiddens, num_layers, ctx = len(vocab), 256, 2, d2l.try_gpu()\n",
    "lstm_layer = rnn.LSTM(num_hiddens, num_layers, bidirectional=True)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
    "# Train the model\n",
    "num_epochs, lr = 500, 1\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is clearly unsatisfactory for the reasons described above. For a discussion of more effective uses of bidirectional models, please see the sentiment classification in `Section 15.2`.\n",
    "\n",
    "##### Summary\n",
    "+ In bidirectional recurrent neural networks, the hidden state for each timestep is simultaneously determined by the data prior to and after the current timestep.\n",
    "+ Bidirectional RNNs bear a striking resemblance with the forward-backward algorithm in graphical models.\n",
    "+ Bidirectional RNNs are mostly useful for sequence embedding and the estimation of observations given bidirectional context.\n",
    "+ Bidirectional RNNs are very costly to train due to long gradient chains.\n",
    "\n",
    "##### Exercises\n",
    "1. If the different directions use a different number of hidden units, how will the shape of $\\mathbf{H}_t$ change?\n",
    "2. Design a bidirectional recurrent neural network with multiple hidden layers.\n",
    "3. Implement a sequence classification algorithm using bidirectional RNNs. Hint: use the RNN to embed each word and then aggregate (average) all embedded outputs before sending the output into an MLP for classification. For instance, if we have $(\\mathbf{o}_1, \\mathbf{o}_2, \\mathbf{o}_3)$, we compute $\\bar{\\mathbf{o}} = \\frac{1}{3} \\sum_i \\mathbf{o}_i$ first and then use the latter for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
