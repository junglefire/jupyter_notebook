{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import d2l\n",
    "import os\n",
    "\n",
    "from mxnet import autograd, np, npx, gluon, init, image\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Natural Language Processing: Pretraining\n",
    "Humans need to communicate. Out of this basic need of the human condition, a vast amount of written text has been generated on an everyday basis. Given rich text in social media, chat apps, emails, product reviews, news articles, research papers, and books, it becomes vital to enable computers to understand them to offer assistance or make decisions based on human languages.\n",
    "\n",
    "Natural language processing studies interactions between computers and humans using natural languages. In practice, it is very common to use natural language processing techniques to process and analyze text (human natural language) data, such as language models in `Section 8.3` and machine translation models in `Section 9.5`.\n",
    "\n",
    "To understand text, we can begin with its representation, such as treating each word or subword as an individual text token. As we will see in this chapter, the representation of each token can be pretrained on a large corpus, using `word2vec`, `GloVe`, or `subword embedding` models. After pretraining, representation of each token can be a vector, however, it remains the same no matter what the context is. For instance, the vector representation of \"bank\" is the same in both \"go to the bank to deposit some money\" and \"go to the bank to sit down\". Thus, many more recent pretraining models adapt representation of the same token to different contexts. Among them is `BERT`, a much deeper model based on the `Transformer` encoder. In this chapter, we will focus on how to pretrain such representations for text, as highlighted in `Fig. 14.1`.\n",
    "\n",
    "<img src=\"images/14_01.png\" style=\"width:600px;\"/>\n",
    "\n",
    "As shown in `Fig. 14.1`, the pretrained text representations can be fed to a variety of deep learning architectures for different downstream natural language processing applications. We will cover them in `Chapter 15`.\n",
    "\n",
    "## 14.1 Word Embedding (word2vec)\n",
    "A natural language is a complex system that we use to express meanings. In this system, words are the basic unit of linguistic meaning. As its name implies, a word vector is a vector used to represent a word. It can also be thought of as the feature vector of a word. The technique of mapping words to vectors of real numbers is also known as `word embedding`. Over the last few years, word embedding has gradually become basic knowledge in natural language processing.\n",
    "\n",
    "### 14.1.1 Why Not Use One-hot Vectors?\n",
    "We used one-hot vectors to represent words (characters are words) in `Section 8.5`. Recall that when we assume the number of different words in a dictionary (the dictionary size) is $N$, each word can correspond one-to-one with consecutive integers from 0 to $N-1$. These integers that correspond to words are called the indices of the words. We assume that the index of a word is $i$. In order to get the one-hot vector representation of the word, we create a vector of all `0`s with a length of $N$ and set element $i$ to 1. In this way, each word is represented as a vector of length $N$ that can be used directly by the neural network.\n",
    "\n",
    "Although one-hot word vectors are easy to construct, they are usually not a good choice. One of the major reasons is that the one-hot word vectors cannot accurately express the similarity between different words, such as the cosine similarity that we commonly use. For the vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their cosine similarities are the cosines of the angles between them:\n",
    "\n",
    "$$\\displaystyle\\frac{\\mathbf{x}^\\top \\mathbf{y}}{\\lVert\\mathbf{x}\\rVert \\lVert\\mathbf{y}\\rVert} \\in [-1, 1].$$\n",
    "\n",
    "Since the cosine similarity between the one-hot vectors of any two different words is 0, it is difficult to use the one-hot vector to accurately represent the similarity between multiple different words.\n",
    "\n",
    "`Word2vec` is a tool that we came up with to solve the problem above. It represents each word with a fixed-length vector and uses these vectors to better indicate the similarity and analogy relationships between different words. The Word2vec tool contains two models:\n",
    "+ **skip-gram** (`Mikolov et al., 2013b`) \n",
    "+ **continuous bag of words** (CBOW) (`Mikolov et al., 2013a`)\n",
    "\n",
    "Next, we will take a look at the two models and their training methods.\n",
    "\n",
    "### 14.1.2 The Skip-Gram Model\n",
    "The skip-gram model assumes that a word can be used to generate the words that surround it in a text sequence. For example, we assume that the text sequence is `the`, `man`, `loves`, `his`, and `son`. We use `loves` as the central target word and set the context window size to 2. As shown in `Fig. 14.1.1`, given the central target word `loves`, the skip-gram model is concerned with the conditional probability for generating the context words, `the`, `man`, `his` and `son`, that are within a distance of no more than 2 words, which is\n",
    "\n",
    "$$P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).$$\n",
    "\n",
    "We assume that, given the central target word, the context words are generated independently of each other. In this case, the formula above can be rewritten as\n",
    "\n",
    "$$P(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).$$\n",
    "\n",
    "<img src=\"images/14_02.png\" style=\"width:600px;\"/>\n",
    "\n",
    "In the skip-gram model, each word is represented as two $d$-dimension vectors, which are used to compute the conditional probability. We assume that the word is indexed as $i$ in the dictionary, its vector is represented as $\\mathbf{v}_i\\in\\mathbb{R}^d$ when it is the central target word, and $\\mathbf{u}_i\\in\\mathbb{R}^d$ when it is a context word. Let the central target word $w_c$ and context word $w_o$ be indexed as $c$ and $o$ respectively in the dictionary. The conditional probability of generating the context word for the given central target word can be obtained by performing a softmax operation on the vector inner product:\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\displaystyle\\frac{\\text{exp}(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\displaystyle\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)},$$\n",
    "\n",
    "where vocabulary index set $\\mathcal{V} = {0, 1, \\ldots, |\\mathcal{V}|-1}$. Assume that a text sequence of length $T$ is given, where the word at timestep $t$ is denoted as $w^{(t)}$. Assume that context words are independently generated given center words. When context window size is $m$, the likelihood function of the skip-gram model is the joint probability of generating all the context words given any center word\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),$$\n",
    "\n",
    "Here, any timestep that is less than 1 or greater than $T$ can be ignored.\n",
    "\n",
    "##### Skip-Gram Model Training\n",
    "The skip-gram model parameters are the central target word vector and context word vector for each individual word. In the training process, we are going to learn the model parameters by maximizing the likelihood function, which is also known as maximum likelihood estimation. This is equivalent to minimizing the following loss function:\n",
    "\n",
    "$$ - \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\log{P(w^{(t+j)} \\mid w^{(t)}}).$$\n",
    "\n",
    "If we use the SGD, in each iteration we are going to pick a shorter subsequence through random sampling to compute the loss for that subsequence, and then compute the gradient to update the model parameters. The key of gradient computation is to compute the gradient of the logarithmic conditional probability for the central word vector and the context word vector. By definition, we first have\n",
    "\n",
    "$$\\log P(w_o \\mid w_c) = \\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).$$\n",
    "\n",
    "Through differentiation, we can get the gradient $\\mathbf{v}_c$ from the formula above.\n",
    "\n",
    "$$ \\begin{aligned} \\frac{\\partial \\text{log} P(w_o \\mid w_c)}{\\partial \\mathbf{v}_c} &= \\mathbf{u}_o - \\frac{\\displaystyle\\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)\\mathbf{u}_j}{\\displaystyle\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)} \\\\ &= \\mathbf{u}_o - \\displaystyle\\sum_{j \\in \\mathcal{V}} \\left(\\frac{\\text{exp}(\\mathbf{u}_j^\\top \\mathbf{v}_c)}{\\displaystyle\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\right) \\mathbf{u}_j \\\\ &= \\mathbf{u}o - \\displaystyle\\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j. \\end{aligned} $$\n",
    "\n",
    "Its computation obtains the conditional probability for all the words in the dictionary given the central target word $w_c$. We then use the same method to obtain the gradients for other word vectors.\n",
    "\n",
    "After the training, for any word in the dictionary with index $i$, we are going to get its two word vector sets $\\mathbf{v}_i$ and $\\mathbf{u}_i$. In applications of natural language processing, the central target word vector in the skip-gram model is generally used as the representation vector of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.3 The Continuous Bag of Words (CBOW) Model\n",
    "The continuous bag of words (CBOW) model is similar to the skip-gram model. The biggest difference is that the CBOW model assumes that the central target word is generated based on the context words before and after it in the text sequence. With the same text sequence `the`, `man`, `loves`, `his` and `son`, in which `loves` is the central target word, given a context window size of 2, the CBOW model is concerned with the conditional probability of generating the target word `loves` based on the context words `the`, `man`, `his` and `son` (as shown in `Fig. 14.1.2`), such as\n",
    "\n",
    "$$P(\\textrm{\"loves\"}\\mid\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}).$$\n",
    "\n",
    "<img src=\"images/14_03.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Since there are multiple context words in the CBOW model, we will average their word vectors and then use the same method as the skip-gram model to compute the conditional probability. We assume that $\\mathbf{v_i}\\in\\mathbb{R}^d$ and $\\mathbf{u_i}\\in\\mathbb{R}^d$ are the context word vector and central target word vector of the word with index $i$ in the dictionary (notice that the symbols are opposite to the ones in the skip-gram model). Let central target word $w_c$ be indexed as $c$, and context words $w_{o_1}, \\ldots, w_{o_{2m}}$ be indexed as $o_1, \\ldots, o_{2m}$ in the dictionary. Thus, the conditional probability of generating a central target word from the given context word is\n",
    "\n",
    "$$P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_c^\\top (\\mathbf{v}_{o_1} + \\ldots, + \\mathbf{v}_{o_{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_i^\\top (\\mathbf{v}_{o_1} + \\ldots, + \\mathbf{v}_{o_{2m}}) \\right)}.$$\n",
    "\n",
    "For brevity, denote $\\mathcal{W}_o= {w_{o_1}, \\ldots, w_{o_{2m}}}$, and $\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots, + \\mathbf{v}_{o_{2m}} \\right)/(2m)$. The equation above can be simplified as\n",
    "\n",
    "$$P(w_c \\mid \\mathcal{W}_o) = \\frac{\\exp\\left(\\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)}.$$\n",
    "\n",
    "Given a text sequence of length $T$, we assume that the word at timestep $t$ is $w^{(t)}$, and the context window size is $m$. The likelihood function of the CBOW model is the probability of generating any central target word from the context words.\n",
    "\n",
    "$$ \\prod_{t=1}^{T} P(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$\n",
    "\n",
    "##### CBOW Model Training\n",
    "CBOW model training is quite similar to skip-gram model training. The maximum likelihood estimation of the CBOW model is equivalent to minimizing the loss function.\n",
    "\n",
    "$$ -\\sum_{t=1}^T \\text{log} P(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$\n",
    "\n",
    "Notice that\n",
    "\n",
    "$$\\log P(w_c \\mid \\mathcal{W}_o) = \\mathbf{u}_c^\\top \\bar{\\mathbf{v}}o - \\log,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)\\right).$$\n",
    "\n",
    "Through differentiation, we can compute the logarithm of the conditional probability of the gradient of any context word vector $\\mathbf{v}_{o_i}$($i = 1, \\ldots, 2m$) in the formula above.\n",
    "\n",
    "$$\\frac{\\partial \\log P(w_c \\mid \\mathcal{W}o)}{\\partial \\mathbf{v}{o_i}} = \\frac{1}{2m} \\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\mathbf{u}_j^\\top \\bar{\\mathbf{v}}_o)\\mathbf{u}_j}{\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o)} \\right) = \\frac{1}{2m}\\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j \\right).$$\n",
    "\n",
    "We then use the same method to obtain the gradients for other word vectors. Unlike the skip-gram model, we usually use the context word vector as the representation vector for a word in the CBOW model.\n",
    "\n",
    "##### Summary\n",
    "+ A word vector is a vector used to represent a word. The technique of mapping words to vectors of real numbers is also known as word embedding.\n",
    "+ Word2vec includes both the continuous bag of words (CBOW) and skip-gram models. The skip-gram model assumes that context words are generated based on the central target word. The CBOW model assumes that the central target word is generated based on the context words.\n",
    "\n",
    "##### Exercises\n",
    "1. What is the computational complexity of each gradient? If the dictionary contains a large volume of words, what problems will this cause?\n",
    "2. There are some fixed phrases in the English language which consist of multiple words, such as \"new york\". How can you train their word vectors? Hint: See section 4 in the Word2vec paper[2].\n",
    "3. Use the skip-gram model as an example to think about the design of a word2vec model. What is the relationship between the inner product of two word vectors and the cosine similarity in the skip-gram model? For a pair of words with close semantical meaning, why it is likely for their word vector cosine similarity to be high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 Approximate Training\n",
    "Recall content of the last section. The core feature of the skip-gram model is the use of softmax operations to compute the conditional probability of generating context word $w_o$ based on the given central target word $w_c$.\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\frac{\\text{exp}(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\displaystyle\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)}.$$\n",
    "\n",
    "The logarithmic loss corresponding to the conditional probability is given as\n",
    "\n",
    "$$-\\log P(w_o \\mid w_c) =\n",
    "-\\mathbf{u}_o^\\top \\mathbf{v}_c + \\log\\left(\\displaystyle\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).$$\n",
    "\n",
    "Because the softmax operation has considered that the context word could be any word in the dictionary $\\mathcal{V}$, the loss mentioned above actually includes the sum of the number of items in the dictionary size. From the last section, we know that for both the skip-gram model and CBOW model, because they both get the conditional probability using a softmax operation, the gradient computation for each step contains the sum of the number of items in the dictionary size. For larger dictionaries with hundreds of thousands or even millions of words, the overhead for computing each gradient may be too high.  In order to reduce such computational complexity, we will introduce two approximate training methods in this section: negative sampling and hierarchical softmax. Since there is no major difference between the skip-gram model and the CBOW model, we will only use the skip-gram model as an example to introduce these two training methods in this section.\n",
    "\n",
    "### 14.2.1 Negative Sampling\n",
    "Negative sampling modifies the original objective function. Given a context window for the central target word $w_c$, we will treat it as an event for context word $w_o$ to appear in the context window and compute the probability of this event from\n",
    "\n",
    "$$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c),$$\n",
    "\n",
    "Here, the $\\sigma$ function has the same definition as the sigmoid activation function:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+\\exp(-x)}.$$\n",
    "\n",
    "We will first consider training the word vector by maximizing the joint probability of all events in the text sequence. Given a text sequence of length $T$, we assume that the word at timestep $t$ is $w^{(t)}$ and the context window size is $m$. Now we consider maximizing the joint probability\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}).$$\n",
    "\n",
    "However, the events included in the model only consider positive examples. In this case, only when all the word vectors are equal and their values approach infinity can the joint probability above be maximized to 1. Obviously, such word vectors are meaningless. Negative sampling makes the objective function more meaningful by sampling with an addition of negative examples. Assume that event $P$ occurs when context word $w_o$ appears in the context window of central target word $w_c$, and we sample $K$ words that do not appear in the context window according to the distribution $P(w)$ to act as noise words. We assume the event for noise word $w_k$($k=1, \\ldots, K$) to not appear in the context window of central target word $w_c$ is $N_k$. Suppose that events $P$ and $N_1, \\ldots, N_K$ for both positive and negative examples are independent of each other. By considering negative sampling, we can rewrite the joint probability above, which only considers the positive examples, as\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),$$\n",
    "\n",
    "Here, the conditional probability is approximated to be\n",
    "$$ P(w^{(t+j)} \\mid w^{(t)}) =P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).$$\n",
    "\n",
    "\n",
    "Let the text sequence index of word $w^{(t)}$ at timestep $t$ be $i_t$ and $h_k$ for noise word $w_k$ in the dictionary. The logarithmic loss for the conditional probability above is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\log P(w^{(t+j)} \\mid w^{(t)})\n",
    "=& -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here, the gradient computation in each step of the training is no longer related to the dictionary size, but linearly related to $K$. When $K$ takes a smaller constant, the negative sampling has a lower computational overhead for each step.\n",
    "\n",
    "### 14.2.1 Hierarchical Softmax\n",
    "Hierarchical softmax is another type of approximate training method. It uses a binary tree for data structure as illustrated in `Fig. 14.2.1`, with the leaf nodes of the tree representing every word in the dictionary $\\mathcal{V}$.\n",
    "\n",
    "<img src=\"images/14_04.png\" style=\"width:500px;\"/>\n",
    "\n",
    "We assume that $L(w)$ is the number of nodes on the path (including the root and leaf nodes) from the root node of the binary tree to the leaf node of word $w$. Let $n(w, j)$ be the $j^\\mathrm{th}$ node on this path, with the context word vector $\\mathbf{u}_{n(w, j)}$. We use Figure 10.3 as an example, so $L(w_3) = 4$. Hierarchical softmax will approximate the conditional probability in the skip-gram model as\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\text{leftChild}(n(w_o, j)) ]\\!] \\cdot \\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_c\\right),$$\n",
    "\n",
    "Here the $\\sigma$ function has the same definition as the sigmoid activation function, and $\\text{leftChild}(n)$ is the left child node of node $n$. If $x$ is true, $[\\![x]\\!] = 1$; otherwise $[\\![x]\\!] = -1$.\n",
    "Now, we will compute the conditional probability of generating word $w_3$ based on the given word $w_c$ in Figure 10.3. We need to find the inner product of word vector $\\mathbf{v}_c$ (for word $w_c$) and each non-leaf node vector on the path from the root node to $w_3$. Because, in the binary tree, the path from the root node to leaf node $w_3$ needs to be traversed left, right, and left again (the path with the bold line in Figure 10.3), we get\n",
    "\n",
    "$$P(w_3 \\mid w_c) = \\sigma(\\mathbf{u}_{n(w_3, 1)}^\\top \\mathbf{v}_c) \\cdot \\sigma(-\\mathbf{u}_{n(w_3, 2)}^\\top \\mathbf{v}_c) \\cdot \\sigma(\\mathbf{u}_{n(w_3, 3)}^\\top \\mathbf{v}_c).$$\n",
    "\n",
    "Because $\\sigma(x)+\\sigma(-x) = 1$, the condition that the sum of the conditional probability of any word generated based on the given central target word $w_c$ in dictionary $\\mathcal{V}$ be 1 will also suffice:\n",
    "\n",
    "$$\\sum_{w \\in \\mathcal{V}} P(w \\mid w_c) = 1.$$\n",
    "\n",
    "In addition, because the order of magnitude for $L(w_o)-1$ is $\\mathcal{O}(\\text{log}_2|\\mathcal{V}|)$, when the size of dictionary $\\mathcal{V}$ is large, the computational overhead for each step in the hierarchical softmax training is greatly reduced compared to situations where we do not use approximate training.\n",
    "\n",
    "##### Summary\n",
    "* Negative sampling constructs the loss function by considering independent events that contain both positive and negative examples. The gradient computational overhead for each step in the training process is linearly related to the number of noise words we sample.\n",
    "* Hierarchical softmax uses a binary tree and constructs the loss function based on the path from the root node to the leaf node. The gradient computational overhead for each step in the training process is related to the logarithm of the dictionary size.\n",
    "\n",
    "##### Exercises\n",
    "1. Before reading the next section, think about how we should sample noise words in negative sampling.\n",
    "2. What makes the last formula in this section hold?\n",
    "3. How can we apply negative sampling and hierarchical softmax in the skip-gram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 The Dataset for Pretraining Word Embedding\n",
    "In this section, we will introduce how to preprocess a dataset with negative sampling `Section 14.2` and load into minibatches for word2vec training. The dataset we use is [Penn Tree Bank (PTB)](https://catalog.ldc.upenn.edu/LDC99T42), which is a small but commonly-used corpus. It takes samples from Wall Street Journal articles and includes training sets, validation sets, and test sets.\n",
    "\n",
    "### 14.3.1 Reading and Preprocessing the Dataset\n",
    "This dataset has already been preprocessed. Each line of the dataset acts as a sentence. All the words in a sentence are separated by spaces. In the word embedding task, each word is a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# sentences: 42069'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip', '319d85e578af0cdc590547f26231e4e31cdf1e42')\n",
    "\n",
    "#@save\n",
    "def read_ptb():\n",
    "    data_dir = d2l.download_extract('ptb')\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "    return [line.split() for line in raw_text.split('\\n')]\n",
    "\n",
    "sentences = read_ptb()\n",
    "f'# sentences: {len(sentences)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build a vocabulary with words appeared not greater than 10 times mapped into a `<unk>` token. Note that the preprocessed PTB data also contains `<unk>` tokens presenting rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vocab size: 6719'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "f'vocab size: {len(vocab)}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.2 Subsampling\n",
    "In text data, there are generally some words that appear at high frequencies, such `the`, `a`, and `in` in English. Generally speaking, in a context window, it is better to train the word embedding model when a word (such as \"chip\") and a lower-frequency word (such as \"microprocessor\") appear at the same time, rather than when a word appears with a higher-frequency word (such as \"the\"). Therefore, when training the word embedding model, we can perform subsampling[2] on the words. Specifically, each indexed word $w_i$ in the dataset will drop out at a certain probability. The dropout probability is given as:\n",
    "\n",
    "$$ P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right),$$\n",
    "\n",
    "Here, $f(w_i)$ is the ratio of the instances of word $w_i$ to the total number of words in the dataset, and the constant $t$ is a hyperparameter (set to $10^{-4}$ in this experiment). As we can see, it is only possible to drop out the word $w_i$ in subsampling when $f(w_i) > t$. The higher the word's frequency, the higher its dropout probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subsampled size: 42069'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "def subsampling(sentences, vocab):\n",
    "    # Map low frequency words into <unk>\n",
    "    sentences = [[vocab.idx_to_token[vocab[tk]] for tk in line] for line in sentences]\n",
    "    # Count the frequency for each word\n",
    "    counter = d2l.count_corpus(sentences)\n",
    "    num_tokens = sum(counter.values())\n",
    "\n",
    "    # Return True if to keep this token during subsampling\n",
    "    def keep(token):\n",
    "        return(random.uniform(0, 1) < math.sqrt(1e-4/counter[token]*num_tokens))\n",
    "\n",
    "    # Now do the subsampling\n",
    "    return [[tk for tk in line if keep(tk)] for line in sentences]\n",
    "\n",
    "subsampled = subsampling(sentences, vocab)\n",
    "f'subsampled size: {len(subsampled)}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the sequence lengths before and after sampling, we can see subsampling significantly reduced the sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.set_figsize()\n",
    "d2l.plt.hist([[len(line) for line in sentences],\n",
    "              [len(line) for line in subsampled]])\n",
    "d2l.plt.xlabel('# tokens per sentence')\n",
    "d2l.plt.ylabel('count')\n",
    "d2l.plt.legend(['origin', 'subsampled']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For individual tokens, the sampling rate of the high-frequency word `the` is less than 1/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# of \"the\": before=50770, after=2170'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return (f'# of \"{token}\": '\n",
    "            f'before={sum([line.count(token) for line in sentences])}, '\n",
    "            f'after={sum([line.count(token) for line in subsampled])}')\n",
    "\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the low-frequency word `join` is completely preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# of \"join\": before=45, after=45'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we map each token into an index to construct the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [2132, 145, 275], [5464, 3080, 1595]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [vocab[line] for line in subsampled]\n",
    "corpus[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.3 Loading the Dataset\n",
    "Next we read the corpus with token indicies into data batches for training.\n",
    "\n",
    "##### Extracting Central Target Words and Context Words\n",
    "We use words with a distance from the central target word not exceeding the context window size as the context words of the given center target word. The following definition function extracts all the central target words and their context words. It uniformly and randomly samples an integer to be used as the context window size between integer 1 and the `max_window_size` (maximum context window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        # Each sentence needs at least 2 words to form a \"central target word - context word\" pair\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        for i in range(len(line)):  # Context window centered at i\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i - window_size),\n",
    "                                 min(len(line), i + 1 + window_size)))\n",
    "            # Exclude the central target word from the context words\n",
    "            indices.remove(i)\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an artificial dataset containing two sentences of 7 and 3 words, respectively. Assume the maximum context window is 2 and print all the central target words and their context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1, 2]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [2, 3, 5, 6]\n",
      "center 5 has contexts [4, 6]\n",
      "center 6 has contexts [4, 5]\n",
      "center 7 has contexts [8, 9]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the maximum context window size to 5. The following extracts all the central target words and their context words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# center-context pairs: 353590'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
    "f'# center-context pairs: {len(all_centers)}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Negative Sampling\n",
    "We use negative sampling for approximate training. For a central and context word pair, we randomly sample $K$ noise words ($K=5$ in the experiment). According to the suggestion in the Word2vec paper, the noise word sampling probability $P(w)$ is the ratio of the word frequency of $w$ to the total word frequency raised to the power of 0.75.\n",
    "\n",
    "We first define a class to draw a candidate according to the sampling weights. It caches a 10000 size random number bank instead of calling `random.choices` every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 2, 0, 2, 0, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@save\n",
    "class RandomGenerator:\n",
    "    \"\"\"Draw a random int in [0, n] according to n sampling weights.\"\"\"\n",
    "    def __init__(self, sampling_weights):\n",
    "        self.population = list(range(len(sampling_weights)))\n",
    "        # print(self.population)\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            # 从population中进行K次随机选取，每次选取一个元素，同一个元素可能多次被选中\n",
    "            # weights是相对权重值，population中有几个元素就要有相对应的weights值，cum_weights是累加权重值\n",
    "            # 一次取k个，返回一个k长的列表\n",
    "            self.candidates = random.choices(self.population, self.sampling_weights, k=10000)\n",
    "            self.i = 0\n",
    "        self.i += 1\n",
    "        return self.candidates[self.i-1]\n",
    "\n",
    "generator = RandomGenerator([2, 3, 4])\n",
    "[generator.draw() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_negatives(all_contexts, corpus, K):\n",
    "    counter = d2l.count_corpus(corpus)\n",
    "    sampling_weights = [counter[i]**0.75 for i in range(len(counter))]\n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            # Noise words cannot be context words\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "all_negatives = get_negatives(all_contexts, corpus, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading into Batches\n",
    "We extract all central target words `all_centers`, and the context words `all_contexts` and noise words `all_negatives` of each central target word from the dataset. We will read them in random minibatches.\n",
    "\n",
    "In a minibatch of data, the $i^\\mathrm{th}$ example includes a central word and its corresponding $n_i$ context words and $m_i$ noise words. Since the context window size of each example may be different, the sum of context words and noise words, $n_i+m_i$, will be different. When constructing a minibatch, we concatenate the context words and noise words of each example, and add `0` for padding until the length of the concatenations are the same, that is, the length of all concatenations is $\\max_i n_i+m_i$(max_len). In order to avoid the effect of padding on the loss function calculation, we construct the mask variable masks, each element of which corresponds to an element in the concatenation of context and noise words, `contexts_negatives`. When an element in the variable `contexts_negatives` is a padding, the element in the mask variable masks at the same position will be 0. Otherwise, it takes the value 1. In order to distinguish between positive and negative examples, we also need to distinguish the context words from the noise words in the `contexts_negatives` variable. Based on the construction of the mask variable, we only need to create a label variable labels with the same shape as the `contexts_negatives` variable and set the elements corresponding to context words (positive examples) to 1, and the rest to 0.\n",
    "\n",
    "Next, we will implement the minibatch reading function batchify. Its minibatch input data is a list whose length is the batch size, each element of which contains central target words center, context words context, and noise words negative. The minibatch data returned by this function conforms to the format we need, for example, it includes the mask variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def batchify(data):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (np.array(centers).reshape(-1, 1), \n",
    "            np.array(contexts_negatives),\n",
    "            np.array(masks), np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct two simple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = [[1.]\n",
      " [1.]]\n",
      "contexts_negatives = [[2. 2. 3. 3. 3. 3.]\n",
      " [2. 2. 2. 3. 3. 0.]]\n",
      "masks = [[1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0.]]\n",
      "labels = [[1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x_1 = (1, [2, 2], [3, 3, 3, 3])\n",
    "x_2 = (1, [2, 2, 2], [3, 3])\n",
    "batch = batchify((x_1, x_2))\n",
    "\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `batchify` function just defined to specify the minibatch reading method in the `DataLoader` instance.\n",
    "\n",
    "### 14.3.4 Putting All Things Together\n",
    "Last, we define the `load_data_ptb` function that read the `PTB` dataset and return the data iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    sentences = read_ptb()\n",
    "    vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "    subsampled = subsampling(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "    all_negatives = get_negatives(all_contexts, corpus, num_noise_words)\n",
    "    dataset = gluon.data.ArrayDataset(all_centers, all_contexts, all_negatives)\n",
    "    data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                                      batchify_fn=batchify,\n",
    "                                      num_workers=num_workers)\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the first minibatch of the data iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: (512, 1)\n",
      "contexts_negatives shape: (512, 60)\n",
      "masks shape: (512, 60)\n",
      "labels shape: (512, 60)\n"
     ]
    }
   ],
   "source": [
    "data_iter, vocab = load_data_ptb(512, 5, 5)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ Subsampling attempts to minimize the impact of high-frequency words on the training of a word embedding model.\n",
    "+ We can pad examples of different lengths to create minibatches with examples of all the same length and use mask variables to distinguish between padding and non-padding elements, so that only non-padding elements participate in the calculation of the loss function.\n",
    "\n",
    "##### Exercises\n",
    "1. We use the batchify function to specify the minibatch reading method in the DataLoader instance and print the shape of each variable in the first batch read. How should these shapes be calculated?\n",
    "\n",
    "\n",
    "## 14.4 Pretraining word2vec\n",
    "In this section, we will train a skip-gram model defined in `Section 14.1`.\n",
    "\n",
    "First, import the packages and modules required for the experiment, and load the `PTB` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_window_size, num_noise_words = 512, 5, 5\n",
    "data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size, num_noise_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4.1 The Skip-Gram Model\n",
    "We will implement the skip-gram model by using embedding layers and minibatch multiplication. These methods are also often used to implement other natural language processing applications.\n",
    "\n",
    "##### Embedding Layer\n",
    "The layer in which the obtained word is embedded is called the embedding layer, which can be obtained by creating an `nn.Embedding` instance in `Gluon`. The weight of the embedding layer is a matrix whose number of rows is the dictionary size (`input_dim`) and whose number of columns is the dimension of each word vector (`output_dim`). We set the dictionary size to $20$ and the word vector dimension to $4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter embedding0_weight (shape=(20, 4), dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(input_dim=20, output_dim=4)\n",
    "embed.initialize()\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the embedding layer is the index of the word. When we enter the index $i$ of a word, the embedding layer returns the $i^\\mathrm{th}$ row of the weight matrix as its word vector. Below we enter an index of shape ($2$, $3$) into the embedding layer. Because the dimension of the word vector is 4, we obtain a word vector of shape ($2$, $3$, $4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01438687,  0.05011239,  0.00628365,  0.04861524],\n",
       "        [-0.01068833,  0.01729892,  0.02042518, -0.01618656],\n",
       "        [-0.00873779, -0.02834515,  0.05484822, -0.06206018]],\n",
       "\n",
       "       [[ 0.06491279, -0.03182812, -0.01631819, -0.00312688],\n",
       "        [ 0.0408415 ,  0.04370362,  0.00404529, -0.0028032 ],\n",
       "        [ 0.00952624, -0.01501013,  0.05958354,  0.04705103]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minibatch Multiplication\n",
    "We can multiply the matrices in two minibatches one by one, by the minibatch multiplication operation batch_dot. Suppose the first batch contains $n$ matrices $\\mathbf{X}_1, \\ldots, \\mathbf{X}_n$ with a shape of $a\\times b$, and the second batch contains $n$ matrices $\\mathbf{Y}_1, \\ldots, \\mathbf{Y}_n$ with a shape of $b\\times c$. The output of matrix multiplication on these two batches are $n$ matrices $\\mathbf{X}_1\\mathbf{Y}_1, \\ldots, \\mathbf{X}_n\\mathbf{Y}_n$ with a shape of $a\\times c$. Therefore, given two tensors of shape ($n$, $a$, $b$) and ($n$, $b$, $c$), the shape of the minibatch multiplication output is ($n$, $a$, $c$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.ones((2, 1, 4))\n",
    "Y = np.ones((2, 4, 6))\n",
    "npx.batch_dot(X, Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skip-gram Model Forward Calculation\n",
    "In forward calculation, the input of the skip-gram model contains the central target word index center and the concatenated context and noise word index `contexts_and_negatives`. In which, the center variable has the shape `(batch size, 1)`, while the `contexts_and_negatives` variable has the shape `(batch size, max_len)`. These two variables are first transformed from word indexes to word vectors by the word embedding layer, and then the output of shape `(batch size, 1, max_len)` is obtained by minibatch multiplication. Each element in the output is the inner product of the central target word vector and the context word vector or noise word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = npx.batch_dot(v, u.swapaxes(1, 2))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the output shape should be `(batch size, 1, max_len)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram(np.ones((2, 1)), np.ones((2, 4)), embed, embed).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4.2 Training\n",
    "Before training the word embedding model, we need to define the loss function of the model.\n",
    "\n",
    "##### Binary Cross Entropy Loss Function\n",
    "According to the definition of the loss function in negative sampling, we can directly use `Gluon`'s binary cross-entropy loss function `SigmoidBinaryCrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning that we can use the mask variable to specify the partial predicted value and label that participate in loss function calculation in the minibatch: when the mask is 1, the predicted value and label of the corresponding position will participate in the calculation of the loss function; When the mask is 0, the predicted value and label of the corresponding position do not participate in the calculation of the loss function. As we mentioned earlier, mask variables can be used to avoid the effect of padding on loss function calculations.\n",
    "\n",
    "Given two identical examples, different masks lead to different loss values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.724077 , 0.3620385])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.array([[.5]*4]*2)\n",
    "label = np.array([[1, 0, 1, 0]]*2)\n",
    "mask = np.array([[1, 1, 1, 1], [1, 1, 0, 0]])\n",
    "loss(pred, label, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize the loss in each example due to various lengths in each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.724077, 0.724077])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(pred, label, mask) / mask.sum(axis=1) * mask.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing Model Parameters\n",
    "We construct the embedding layers of the central and context words, respectively, and set the hyperparameter word vector dimension `embed_size` to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Embedding(input_dim=len(vocab), output_dim=embed_size),\n",
    "        nn.Embedding(input_dim=len(vocab), output_dim=embed_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training\n",
    "The training function is defined below. Because of the existence of padding, the calculation of the loss function is slightly different compared to the previous training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n",
    "    net.initialize(ctx=device, force_reinit=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[0, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # Sum of losses, no. of tokens\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            center, context_negative, mask, label = [\n",
    "                data.as_in_ctx(device) for data in batch]\n",
    "            with autograd.record():\n",
    "                pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "                l = (loss(pred.reshape(label.shape), label, mask)\n",
    "                     / mask.sum(axis=1) * mask.shape[1])\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            metric.add(l.sum(), l.size)\n",
    "            if (i+1) % 50 == 0:\n",
    "                animator.add(epoch+(i+1)/len(data_iter),\n",
    "                             (metric[0]/metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, '\n",
    "          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train a skip-gram model using negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.331, 2383.0 tokens/sec on cpu(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAAE9CAYAAAB3Hgm3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de3xeZZ3v/c8vSZM05+bUpEl6TtO0pQcKpeUYRKCM2OKAyjgyOKPDOANbZ9jbUfa40QfHGZ1nv/TRR0ZlOzg6iqggWKVQOQXkUOiZnts0PSVNmjRNm6Rtzr/9R+6WEFpI09xZSdb3/XrdL3Kve637/uUizTfXWtd1LXN3REREwiwm6AJERESCpjAUEZHQUxiKiEjoKQxFRCT0FIYiIhJ6CkMREQm9uKALGCwZGRk+ffr0oMsYkU6cOEFycnLQZYw4areBUbsNjNptYNatW3fE3XPeb79RE4bjx49n7dq1QZcxIpWXl1NWVhZ0GSOO2m1g1G4Do3YbGDPb35/9dJpURERCT2EoIiKhpzAUEZHQUxiKiEjoKQxFRCT0FIYiIhJ6CkMREQk9haGIiISewlBEREJvVIfh63sa+O3G6qDLEBGRYW5Uh+Gv1x7k357ZGXQZIiIyzI3qMMxJS6C+uQ13D7oUEREZxkZ1GOamJtLe1c3xUx1BlyIiIsPYqA7DnNQEAOqb2wKuREREhrNRHYa5kTCsUxiKiMh7GNVhqJ6hiIj0R1TD0MyWmtlOM6swsy+dY5+Pmdk2M9tqZo/02t5lZhsjjxUD+fy3e4atA6pfRETCIWp3ujezWOBB4HqgClhjZivcfVuvfYqB+4Ar3L3RzHJ7vcUpd59/ITWkJMSROCaGuib1DEVE5Nyi2TNcBFS4e6W7twOPAsv77PPXwIPu3gjg7nWDWYCZkZuaSH2LwlBERM4tmmFYABzs9bwqsq23GcAMM3vVzFab2dJeryWa2drI9lsGWkRuaoJ6hiIi8p6idpr0PD6/GCgDCoGXzewidz8GTHL3ajObCrxgZpvdfU/vg83sLuAugJycHMrLy9/9Ca2t7G/pPvtrAkBLS4vaZwDUbgOjdhsYtVt0RTMMq4GiXs8LI9t6qwLecPcOYK+Z7aInHNe4ezWAu1eaWTmwAHhHGLr7Q8BDACUlJV5WVvauIl48voVdG6o522vSo7y8XO0zAGq3gVG7DYzaLbqieZp0DVBsZlPMLB64Heg7KvRJenqFmFk2PadNK81snJkl9Np+BbCNAchJTaCptZPWjq6BfRciIjLqRS0M3b0TuAdYBWwHfuXuW83sATNbFtltFdBgZtuAF4EvuHsDUAqsNbNNke3f6D0K9XzkpiYCmmsoIiLnFtVrhu6+EljZZ9v9vb524N7Io/c+rwEXDUYNOb1WoSnKTBqMtxQRkVFmVK9AA1qFRkRE3t+oD8PctNNhqFVoRETk7EZ9GGYlJxBjWqxbRETObdSHYWyMkZWSoNOkIiJyTqM+DAFyUhLUMxQRkXMKRRjmpqlnKCIi5xaOMExN0G2cRETknEIRhjmpCRxpaaer24MuRUREhqFQhGFuaiJd3U7jyfagSxERkWEoFGF4ZhUa3cpJRETOIhRhmHt6FRrd5FdERM4iFGH4ds9Qg2hEROTdQhGGp+9cobmGIiJyNqEIw7HxsaQmxGmuoYiInFUowhB6TpUqDEVE5GwUhiIiEnqhCkOtQiMiImcTmjDMTU3UABoRETmr0IRhTmoCJ9u7ONHWGXQpIiIyzIQmDE9PvFfvUERE+gpPGKZFVqFRGIqISB+hCcMzq9BoEI2IiPQRmjA8swqNFusWEZE+QhOGGWPHEBdjWqxbRETeJTRhGBNjPXMN1TMUEZE+ohqGZrbUzHaaWYWZfekc+3zMzLaZ2VYze6TX9jvNbHfkcedg1JOTmqCeoYiIvEtctN7YzGKBB4HrgSpgjZmtcPdtvfYpBu4DrnD3RjPLjWzPBL4CXAI4sC5ybOOF1JSbmkBV46kLeQsRERmFotkzXARUuHulu7cDjwLL++zz18CDp0PO3esi228EnnX3o5HXngWWXmhBOamJHFHPUERE+ohazxAoAA72el4FXNZnnxkAZvYqEAt81d2fOcexBX0/wMzuAu4CyMnJoby8/D0LOtnQTkNLB8+/8CKxMXZe38xo1tLS8r5tJ++mdhsYtdvAqN2iK5ph2N/PLwbKgELgZTO7qL8Hu/tDwEMAJSUlXlZW9p77VyXu57d7tjB74RLy0hMHWvOoU15ezvu1nbyb2m1g1G4Do3aLrmieJq0Gino9L4xs660KWOHuHe6+F9hFTzj259jzdnrivVahERGR3qIZhmuAYjObYmbxwO3Aij77PElPrxAzy6bntGklsAq4wczGmdk44IbItguSq1VoRETkLKJ2mtTdO83sHnpCLBZ42N23mtkDwFp3X8HbobcN6AK+4O4NAGb2NXoCFeABdz96oTXlpvWcGlXPUEREeovqNUN3Xwms7LPt/l5fO3Bv5NH32IeBhweznuyUeEB3rhARkXcKzQo0AAlxsWQkjdFpUhEReYdQhSFATkqCTpOKiMg7hC4Mc9MSdJpURETeIXRhqJ6hiIj0FbowzE1LpK65jZ6xOyIiImEMw9QE2ju7aTrVGXQpIiIyTIQuDM+sQtOiEaUiItIjtGGom/yKiMhpoQvD3DM9Q4WhiIj0CF0Y5qT2LMmmnqGIiJwWujBMS4wjJSGO/UdPBF2KiIgME6ELQzNjTkEam6uOB12KiIgME6ELQ4B5RRlsq2mirbMr6FJERGQYCGcYFmbQ0eXsqGkOuhQRERkGwhmGRRkAbKo6FnAlIiIyHIQyDCekJ5KdEs+mg7puKCIiIQ1DM2NeYYZ6hiIiAoQ0DAHmFmawp76F5taOoEsREZGAhTYM5xWl4w6bq3WqVEQk7EIbhnMLewbRvKX5hiIioRfaMMxMjmdiZhKbDuq6oYhI2IU2DAHmFqarZygiIuEOw/lFGVQfO0V9sxbtFhEJs1CH4dvXDXWqVEQkzKIahma21Mx2mlmFmX3pLK9/yszqzWxj5PGZXq919dq+Ihr1zSlII8bQdUMRkZCLi9Ybm1ks8CBwPVAFrDGzFe6+rc+uv3T3e87yFqfcfX606gNIio9jxvhUNum6oYhIqEWzZ7gIqHD3SndvBx4Flkfx8wbk9Eo07h50KSIiEpBohmEBcLDX86rItr5uNbO3zOwxMyvqtT3RzNaa2WozuyVaRc4tSufYyQ4OHj0VrY8QEZFhLmqnSfvpd8Av3L3NzP4G+Anwgchrk9y92symAi+Y2WZ339P7YDO7C7gLICcnh/Ly8vMuoLOp556Gj/zhNRbnB90cwWhpaRlQ24Wd2m1g1G4Do3aLrmj+9q8Gevf0CiPbznD3hl5PfwT8W6/XqiP/rTSzcmABsKfP8Q8BDwGUlJR4WVnZeRfZ0dXNv7y5is7UCZSVzTrv40eD8vJyBtJ2Yad2Gxi128Co3aIrmqdJ1wDFZjbFzOKB24F3jAo1s/xeT5cB2yPbx5lZQuTrbOAKoO/Am0ExJjaG2RPSdAcLEZEQi1rP0N07zeweYBUQCzzs7lvN7AFgrbuvAD5nZsuATuAo8KnI4aXAD82sm57A/sZZRqEOmrmFGfxyzUE6u7qJiw311EsRkVCK6kUyd18JrOyz7f5eX98H3HeW414DLopmbb3NL8rgP1/bx+66Fkrz04bqY0VEZJhQN4ieNUpBK9GIiISVwhCYnJVMWmIcGw9q8r2ISBgpDIGYGGNuYYZ6hiIiIaUwjJhXlM6O2mZaO7qCLkVERIaYwjDi4onj6Op2Vlc2vP/OIiIyqigMI64sziYjaQyPr69+/51FRGRUURhGJMTFsnzeBFZtreX4qY6gyxERkSGkMOzltoVFtHd287tNh4IuRUREhpDCsJc5BWmUjE/lsXVVQZciIiJDSGHYi5lx28JCNh48RkVdc9DliIjIEFEY9nHLggJiY4zH1mkgjYhIWCgM+8hJTeDakhye2FBFV7cHXY6IiAwBheFZ3LawkMNNbfxxd33QpYiIyBBQGJ7FB2aOZ1zSGA2kEREJCYXhWcTHxbB8fgF/2HaY4yc151BEZLRTGJ7DbQsLae/sZsVbmnMoIjLaKQzPYfaENGbmac6hiEgYKAzP4fScw00Hj7H7sOYcioiMZgrD93DLggLiYozH1qt3KCIymikM30N2SgLXzszll2sOaiCNiMgopjB8H//wwRkcP9XBd1/YHXQpIiISJQrD9zFrQhq3X1rET17bR2V9S9DliIhIFCgM++He60tIHBPLv6zcHnQpIiISBQrDfshJTeDua6fz3PY6LdEmIjIKKQz76S+vmExR5lj++ffb6ezqDrocEREZRFENQzNbamY7zazCzL50ltc/ZWb1ZrYx8vhMr9fuNLPdkced0ayzPxLHxPI/bypl5+FmHl1zMOhyRERkEEUtDM0sFngQuAmYBfyZmc06y66/dPf5kcePIsdmAl8BLgMWAV8xs3HRqrW/ls7JY9GUTL717C6On9JUCxGR0SKaPcNFQIW7V7p7O/AosLyfx94IPOvuR929EXgWWBqlOvvNzLj/5lk0nmzne5pqISIyakQzDAuA3ucTqyLb+rrVzN4ys8fMrOg8jx1ycwrS+ejCQv7ztX3sPXIi6HJERGQQxAX8+b8DfuHubWb2N8BPgA/092Azuwu4CyAnJ4fy8vKoFNnX5andrMD59P95mfsuS2RMjA3J50ZLS0vLkLXdaKJ2Gxi128Co3aIrmmFYDRT1el4Y2XaGuzf0evoj4N96HVvW59jyvh/g7g8BDwGUlJR4WVlZ312iJrGghs/+bD3PN2bxjVvnDtnnRkN5eTlD2XajhdptYNRuA6N2i65oniZdAxSb2RQziwduB1b03sHM8ns9XQacntW+CrjBzMZFBs7cENk2bCydk8/d107j0TUHeeSNA0GXIyIiFyBqPUN37zSze+gJsVjgYXffamYPAGvdfQXwOTNbBnQCR4FPRY49amZfoydQAR5w96PRqnWg7r2+hC3VTXxlxRZK8lJZOCnwAa8iIjIAUZ1n6O4r3X2Gu09z969Htt0fCULc/T53n+3u89z9Wnff0evYh919euTx42jWOVCxMcZ3b19AfvpY/u7n66hrbg26JBERGQCtQHOB0pPG8MM7FtJ0qpO7f76e9k6tTiMiMtL0KwzN7PNmlmY9/sPM1pvZDdEubqQozU/jm7fNZc2+Rv75qW1BlyMiIuepvz3Dv3L3JnoGsowD7gC+EbWqRqBl8yZw19VT+enr+/n2s7tw96BLEhGRfurvAJrTE+n+BPivyECYkT25Lgq+uHQmx062853nd3Oqo4v7bpqJmklEZPjrbxiuM7M/AFOA+8wsFdDFsT5iY4xv/Olcxo6J5aGXKznZ3skDy+YQM8In5YuIjHb9DcNPA/OBSnc/GVlI+y+jV9bIFRNjfHXZbMbGx/GDl/Zwsr2Lf7t1LnGxGqskIjJc9TcMlwAb3f2EmX0SuBj4TvTKGtnMjC8uLSEpPpZvPbuLto5uvv3x+cTHKRBFRIaj/v52/j5w0szmAf8d2AP8NGpVjQJmxueuK+bLHyrlqc01fOanazl2sj3oskRE5Cz6G4ad3jM8cjnwPXd/EEiNXlmjx2eumso3b72I1/cc4eb//xU2Vx0PuiQREemjv2HYbGb30TOl4ikziwHGRK+s0eXjl07kV3+zhO5u59YfvMYv3jygqRciIsNIf8Pw40AbPfMNa+m5i8T/G7WqRqEFE8fx+89dxWVTMrnvN5v5H79+i1PtXUGXJSIi9DMMIwH4cyDdzG4GWt1d1wzPU2ZyPP/5l4v4/HXF/GZDFR/591epqGsOuiwRkdDr73JsHwPeBD4KfAx4w8xui2Zho1VsjPEP18/gx5+6lMNNrdz0nT/yv1ftpLVDvUQRkaD09zTpPwGXuvud7v4XwCLgf0WvrNGvrCSXZ++9hg/Pm8D3Xqzghm+/TPnOuqDLEhEJpf6GYYy79/5N3XAex8o5ZKck8K2PzeeRv76MuFjjUz9ew92PrOdwk24FJSIylPo76f4ZM1sF/CLy/OPAyuiUFD6XT8vm6c9fxUMvVfK9Fyso31HHp6+cwqevmkr6WA3aFRGJtv4OoPkC8BAwN/J4yN2/GM3CwiYhLpb/dl0xf/iHq7l6Rg7ffaGCq775At97YTctbZ1BlyciMqr1t2eIuz8OPB7FWgSYlJXM9z+5kK2HjvPtZ3fxv/+wi/94ZS+fvWYadyyZRFJ8v/+XiYhIP73nb1YzawbONjvcAHf3tKhUJcyekM6P7ryUjQeP8e1nd/GvT+/ghy9XcsfiSfzFkklkpSQEXaKIyKjxnmHo7lpyLWDzizL4yV8tYt3+o3y/fA/feX43P3hpD7ctLOTTV05hak5K0CWKiIx4Ouc2QiyclMmP7sykoq6F/3ilkl+vq+KRNw9wfel47lgyicunZROr+yaKiAyIwnCEmZ6bwr/+6Vzuvb6E/3p9Hz9dvZ8/bDvMhPRE/vTiQm5bWMjk7OSgyxQRGVEUhiNUTmoC995Qwt9dO53nth/msXVV/Ht5Bd97sYJLJ4/jtoWF3Dg7j4yk+KBLFREZ9hSGI1zimFhunjuBm+dOoPZ4K09sqObX6w7yxcc3809PbOHy6dn8yZw8bpidR2ayglFE5GwUhqNIXnoif1s2jc9eM5XN1cdZubmWlZtr+NJvNvNPT25h8dRM/tsHilk8NSvoUkVEhpWoLqlmZkvNbKeZVZjZl95jv1vNzM3sksjzyWZ2ysw2Rh4/iGado42ZMbcwgy/dNJOXvlDGU5+7kr+9Zhq7DrfwP3+zOejyRESGnaj1DM0sFngQuB6oAtaY2Qp339Znv1Tg88Abfd5ij7vPj1Z9YWFmzJ6QzuwJ6cTFGt95fjcn2zs1eV9EpJdo9gwXARXuXunu7cCjwPKz7Pc14JuAVqeOstL8NNxhR63uoSgi0ls0uwcFwMFez6uAy3rvYGYXA0Xu/pSZfaHP8VPMbAPQBHzZ3f/Y9wPM7C7gLoCcnBzKy8sHsfzRp+lkNwC/LV9L08S3FwBvaWlR2w2A2m1g1G4Do3aLrsDOlZlZDPAt4FNnebkGmOjuDWa2EHjSzGa7e1Pvndz9IXoWEKekpMTLysqiW/QI5+488MYf6ErLo6zsojPby8vLUdudP7XbwKjdBkbtFl3RPE1aDRT1el4Y2XZaKjAHKDezfcBiYIWZXeLube7eAODu64A9wIwo1hoKZsbM/FS21+g0qYhIb9EMwzVAsZlNMbN44HZgxekX3f24u2e7+2R3nwysBpa5+1ozy4kMwMHMpgLFQGUUaw2N0vw0dtY20919tvXXRUTCKWph6O6dwD3AKmA78Ct332pmD5jZsvc5/GrgLTPbCDwGfNbdj0ar1jApzU+jpa2TqsZTQZciIjJsRPWaobuvBFb22Xb/OfYt6/W17p0YJaX5PXfd2lbTxMSspICrEREZHqI66V6Gn5LxqcQYbK9pev+dRURCQmEYMmPjY5mcnawwFBHpRWEYQqX5aWyvVRiKiJymMAyhWflpHDx6iubWjqBLEREZFhSGIVSanwpoWTYRkdMUhiF0ekSprhuKiPRQGIZQXloiGUljFIYiIhEKwxAyM0rz0timZdlERACFYWj1LMvWRJeWZRMRURiGVWl+Kq0d3exrOBF0KSIigVMYhpQG0YiIvE1hGFLTc1OIjTGFoYgICsPQShwTy7ScZN3bUEQEhWGoleanqWcoIoLCMNRK89OoOd5KS7tGlIpIuCkMQ+z0IJqDzd0BVyIiEiyFYYidXqP0gMJQREJOYRhiuamJZKfEq2coIqGnMAy50vw0haGIhJ7CMORK89Oobu6mo0uBKCLhpTAMudL8VDodKuu1LJuIhJfCMOS0LJuIiMIw9KblpBBnCkMRCTeFYciNiY1hQkoM22u1LJuIhFdUw9DMlprZTjOrMLMvvcd+t5qZm9klvbbdFzlup5ndGM06w64oNUY9QxEJtaiFoZnFAg8CNwGzgD8zs1ln2S8V+DzwRq9ts4DbgdnAUuDfI+8nUTAxLYb65jaOtLQFXYqISCCi2TNcBFS4e6W7twOPAsvPst/XgG8Crb22LQcedfc2d98LVETeT6KgKLXnx0C9QxEJq2iGYQFwsNfzqsi2M8zsYqDI3Z8632Nl8CgMRSTs4oL6YDOLAb4FfOoC3uMu4C6AnJwcysvLB6W2sLH2E2QkxPDixt3M6D74/gcIAC0tLfqZGwC128Co3aIrmmFYDRT1el4Y2XZaKjAHKDczgDxghZkt68exALj7Q8BDACUlJV5WVjaI5YdHeXk58ycnUXu8lbKyq4MuZ8QoLy9HP3PnT+02MGq36IrmadI1QLGZTTGzeHoGxKw4/aK7H3f3bHef7O6TgdXAMndfG9nvdjNLMLMpQDHwZhRrDb3S/DQq6lpo6+wKuhQRkSEXtTB0907gHmAVsB34lbtvNbMHIr2/9zp2K/ArYBvwDHC3u+u3dBSV5qfR2e1U1LUEXYqIyJCL6jVDd18JrOyz7f5z7FvW5/nXga9HrTh5h1mRextur2lm9oT0gKsRERlaWoFGAJiclUxCnCbfi0g4KQwFgLjYGEryUhWGIhJKCkM5ozQvje01Tbh70KWIiAypwOYZyvBTmp/KL9ce5HBTG3npiUGXM2S6up365jYOHT9F7fFWDje1UtfcRl1TG3XNrdQ3tzE9N4XvfeLioEsVkShRGMoZve9tOJrCsK2zi5pjrVQ1nqKq8eSZ/1YfO8WhYz3h19n9zt5wXIyRk5pAblpPO/z+rRruv7n1zHMRGV0UhnLGzEgYbqtp4tqZuQFX03+n2ruoajzJwcaTVDeeoioSctWRwKtrbqP3md/YGCM/PZGCjLEsmpJJfnoi+RljKchIJC9tLOPTEhiXFE9MjAHwVtUxln3vVV6vbGD5fK0KKDIaKQzljPSxYyjIGDssB9GcbO+ksv4Ee+pb2FN/ggMNJzjYeIoDR09S3/zOu23Ex8aQn9ETdlcV51CQMZaizCQKx42lcNxY8tISiYvt/+Xy2RPSSU2MY7XCUGTUUhjKO5TmpwUWhq0dXRw8epJ9DSfZ33CC/Q0n2ddwgj11LRw6/vZNTWIM8tPHMjEziWtLcpiYmRQJuySKxo0lOyXhTK9uMMTGGJdNyeT1PQ2D9p4iMrwoDOUdZuWn8sKOw7R2dJE4Jjq3kGzt6KKiroWdtc3sPNzMztpmdh9upqap9R2nM1MT45iSncxlU7OYlpPM1JwUpuWkMCkrKWq1ncviqVk8t72OmuOnyE8fO6SfLSLRpzCUdyjNT6PbYWdtM/OKMi7ovdydqsZT7KhtZkdNEztqm9le28S+Iyc4PV4lPjaGabkpLJqSyZTsnqCblJXE5KxkMpLGEFnEPXBLpmUBsLqygY8sKAy4GhEZbApDeYfeI0rPJww7u7rZU3+CLdXH2XLoOFurm9he00RzW+eZfSZlJTEzL5Wb505gZl4qM8anMjkr6byu3wWlNC+NjKQxvL5HYSgyGikM5R0mZiaRHB97zuuGLW2d7DtygsojJ9gXeeypb2FHbTNtnd0AjB0Ty6wJadyyoIDS/DRm5qdSMj6V5ISR++MWc/q6YaWuG4qMRiP3t5NERUyMUZKXypZDTWw9dJydtc09pzlrm9lZ28ThpneO3MxPT2RKdjJ3LJ7EnIJ05hSkMSU7hdhBHMAyXCyZmsWqrYepajxJ4bikoMsRkUGkMJR3Kc1P4+dvHOBD330F6LmuNz03hSumZzM9N4Wp2clMzk5mUmYyY+OHdiBLkJZMywbg9T0NfPQShaHIaKIwlHf5zFVTyU1NZGpOMjPzUpmSnTwirutF24zxKWQlx/N6ZQMfvaQo6HJEZBApDOVdpmQn8/kPFgddxrBjZiyemsXqPQ1azFxklNGf+yLnYfG0LA4db+XA0ZNBlyIig0hhKHIelkzNBNBqNCKjjMJQ5DxMy0khJzVBUyxERhmFoch5OH3d8HVdNxQZVRSGIudpydQs6prbqD2hMBQZLRSGIufp9Dql2492BVyJiAwWhaHIeZqclUReWiI7FIYio4bCUOQ8mRlLpmWx42iXrhuKjBKadC8yAEumZvHEhmoq6looHp8adDnDQne3s//oSbYdamJbzXG2HWoixowf3rFQKxjJsBfVMDSzpcB3gFjgR+7+jT6vfxa4G+gCWoC73H2bmU0GtgM7I7uudvfPRrNWkfNx+rrh65UNoQzD0/eq3HDwGBsPHOOtqmNsq2niZHvPqeO4GCMvPZGqxlO8ufcol0/PDrhikfcWtTA0s1jgQeB6oApYY2Yr3H1br90ecfcfRPZfBnwLWBp5bY+7z49WfSIXoigziaxE4/U9DfzFkslBlxN1Ta0dbDp4jA0HjrHx4DE2HTxGw4l2ABLHxDBnQjofu6SIWRPSmJWfRvH4FLq6nYu/9izPbK1VGMqwF82e4SKgwt0rAczsUWA5cCYM3b33TfOSAV2AkRGjNCuW1ZUNdHc7MaPollXd3U5FfQvr9jey4UAjGw4co6K+BXcwg+k5KXxgZi7zJ2YwrzCDkrxUxpzjNGjZjFye2VLLVz88e1S1kYw+0QzDAuBgr+dVwGV9dzKzu4F7gXjgA71emmJmG4Am4Mvu/sco1ipy3kozY3ilup2dh5spzU8LupwBa+3oYtPBY6zd38jafUdZt7+RptZOAMYljWHBxHEsmzeBBRPHMbconbTEMf1+76Vz8nhmay0bDh5j4aRx0foWRC5Y4ANo3P1B4EEz+wTwZeBOoAaY6O4NZrYQeNLMZvfpSWJmdwF3AeTk5FBeXj60xY8SLS0tarsBmJjYBhj/teoNbpjc/4AIWke3s+dYN9sbuth+tIs9x7rpipyTmZBszM+OpTgjnuJxsYxPMsxOACforK5mffX5fVZ8hxNr8H+efpPmmQmAft4GSu0WXdEMw2qg903fCiPbzuVR4PsA7t4GtEW+Xmdme4AZwNreB7j7Q8BDACUlJV5WVjZYtYdKeXk5arvzV15ezqQspz4mlbKyS4Iu55zcnR21zby4s45XK46wdl8jbZ3dxBjMKUjnM9Cvxi4AABONSURBVBdlsWhKJhdPHMe45PhB//xfVb3J1voWrrnmGsxMP28DpHaLrmiG4Rqg2Mym0BOCtwOf6L2DmRW7++7I0w8BuyPbc4Cj7t5lZlOBYqAyirWKDMjiKVk8vaWGrm4ndhhdE2tp6+TViiOU76zjxR311Da1AjAzL5U/v2wSS6b1BGD62Oj3aG+ak8cXH9/M1kNNzClIj/rniQxE1MLQ3TvN7B5gFT1TKx52961m9gCw1t1XAPeY2QeBDqCRnlOkAFcDD5hZB9ANfNbdj0arVpGBWjIti1+uPcj2muB/0e89coIXdtTx4o463tjbQEeXk5oQx1UzsikryaVsRg65aYlDXtcHS8cTY5tZtbU28DYSOZeoXjN095XAyj7b7u/19efPcdzjwOPRrE1kMJyZb7inYch/0Xd0dbNm71GejwRg5ZETABTnpvBXV0zh2pm5LJw07pwjPYdKVkoCl03J4pkttfz3G0oCrUXkXAIfQCMyko1PS2RqdjKvVzbw11dPjfrnnWrv4uXd9azaWsvz2+s4fqqD+LgYlkzN4s7LJ/OBmbkUZSZFvY7ztXROHl9ZsZWKupagSxE5K4WhyAVaPC2LFRsP0dnVHZVlx+qaWnlpVz3PbT/MS7vqae3oJn3sGK4rzeXG2XlcVZxNUvzw/qd84+yeMFy1tZbZw+fS6rDS0NLG+gPHWH+gkaMt7Xx12WzGxscGXVZoDO9/QSIjwJKpWTzyxgG2HGpiflHGBb9fe2c36w808tKuel7aWc+2mp4ZRXlpiXzskiJunJ3HoimZgZ/+PB956YksmJjB01tqmH1R0NUEr6vb2XW4mXX7G1m/v5H1BxrZ13ASgBiDbofF0zL5yILCgCsND4WhyAVaPPXt64YDDcNT7V28uLOOlZtrKN9ZT0tbJ3ExxiWTx/HFpTO5ZkYOpfmpmI3cbtXS2Xn869M7qJ82NuhShlxLWycbDjSydl9P8G04cIyWtp6FDbJT4lkwcRy3L5rIxRPHMacgjRu+/TJPbjikMBxCCkORC5STmkBxbgqrKxv427Jp/T7uRFvnmQB8cUc9pzq6yEqO58Pz8rm2JJcl07JIPY/VXoa7pXN6wnDd4S4+GnQxUdba0cX6A428VtHAa3uOsKnqOF3dToxBSV4atyyYwMJJ47h44jgmZia964+c5fMn8P3yPdQ3t5GTmhDQdxEuCkORQbBkWhaPrauio6v7PU9fdnU7r1Yc4Tfrq3hmay2tHd3kpCZw28JC/uSifBZNyRxW8xUH06SsZErz01h3ePQNounudrbVNPHy7npe2X2Etfsbae/sJjbGmFuYzmevmcriqVnML8ro1x84t8wv4MEX9/C7TYf4qyunDMF3IApDkUGwZGoWP319P29VHT/rGpy7Djfz+PoqntxQzeGmNtLHjuHWiwtZNm8Cl0wevQHY101z8vj2s7uoa2oNZM7jYKprauWPu4+cCcDTd/GYmZfKHYsncXlkYYOB9O6Lx6cypyCNJzdWKwyHiMJQZBBcFrluuLqy4UwYNrd28LtNNTy65gBvVR0nLsYoK8nhqx8u5AOluSTEhW+k4NI5eXzr2V2s2naYOxZPCrqc89La0cWafUd7AnBXPTtqmwHISo7nquJsrirO4ari7EEL+VvmF/DPT22noq6F6bkpg/Keo1lLWyebDh5jbmH6gP4AURiKDILM5Hhm5qXy+p4GlkzL4tE3D/D7t2o42d7FzLxU7r95FsvmTyA7JdzXf4pzU8hLMlZtqR0RYbjvyIkzU1re3HuUts5u4mNjuGTyOP5xaQlXF+cwKz8tKrenWjZvAv+ycju/3VitxQr6cHcqj5yIjMQ9xoYDjew83Iw7/PhTl3LtzNzzfk+FocggWTItix+/uo9XKo6QFB/Lh+dO4PZFRcwvyhjRo0AHk5lxSV4cT1c20HiiPSoLg1+Izq5u1h84xvPbD/Pc9sPsqe9Z1Wd6bgp/ftkkrpqRzWVTModkXmduWiJXTM/miQ3V3Hv9jKh/3nDW1e1sr2lidWUDb+49ypp9R2k82QFAamIc84syuHF2HhdPGjfgW4UpDEUGyUcXFrHvyAmun5XHsvkTSEnQP6+zWTg+lt9XdvDc9sN89JKi9z8gylraOnl5Vz3PbTvMizvraDzZwZhY47IpWXxy8SQ+WDo+sFV9PrKggHt/tYl1+xsD+fygdHU7W6qP83plA6srG1i7r/HMVJSJmUlcVzqeSyf3jMadlpMyKD1z/WsVGSSzJqTx479cFHQZw97ktBgKMsbyzJbawMKw5vgpntt2mGe317F6TwPtXd1kJI3h2pJcrivN5eoZOed1E+NouXF2HmPHbOHJjdV88MLXcxi23J2dh5sjU1EaeGNvA82RG0xPz01h+fwJLJqSyaIpmeSnR2eeqsJQRIaUmXHj7PH8bPV+Wto6h6wHXVnfwqqth3lmay2bDh4DYEp2Mnde3tP7WzhpXFSW07sQyQlxXD9rPL9/q4ayK4MP58FU39zGKxX1/HHXEf5YcYT65jYAJmclcfPcfJZMy2bx1ExyU4dm1LHCUESG3E0X5fHwq3t5YUcdy+ZNiMpnnL6p8dNbalm1pZadh3tGf84rTOcfl5Zww6y8ETFK8yMLClix6RCbj8TwwaCLuQCtHV2s29/Iy7t7AvD0MoOZyfFcOT2bK4uzuWJ6NgUZwaxQpDAUkSF38cRxZKcksGpL7aCGobuz9VATKzfX8PSWWvYeOYEZXDo5k698eBY3zM4L7JftQF1ZnE1WcjyvHerkH4Iu5jy4O7sOt/DH3fW8vPsIb+5toLWjmzGxxsUTx/GFG0u4Zkb0RuOeL4WhiAy52BjjhtnjeXJDNa0dXSSOubA5l5X1Lfx6XRUrN9ewv+EksTHGkqlZfOaqKdwwK29EL2k2JjaGD8+bwM9W76OptWNYXMs8l7bOLlZXHu0ZjbvtMIeOtwI91/3+bNFEri7OYdGUTJKH4eCy4VeRiITCTXPyeOSNA7y8q54bZued9/GdXd08t/0wP1t9gFcqjhAXY1w+PZu/vWYaN8zOI3OYTdu4ELcsKOA/X9vHM5tr+dilwY/A7e34yQ6e39EzFeWlnfWcaO9i7JhYrizO5nPXFXP1jBwmjIDeuMJQRAKxeGoW6WPH8MzW2vMKw8NNrfzizQM8+uZBaptamZCeyP+4YQYfu7RoyAZbDLV5hemMTzKe2FA9LMKwrrmVZ7cd5pkttby+p4HObicnNYFl8wu4flYul0/LvuDe/lBTGIpIIMbExvDB0vE8u62W9s5u4uPOPZLzRFsnq7bW8sSGal6tOEK3w9UzcvjaLXO4tiRn2I0CHWxmxuUT4nhyTwM1x09FbXrBe6lrbuX3m2p4eksNa/c34t4z8vMzV03lxtnjmVeYMSyu/Q2UwlBEArN0Th6Pr69idWUDV8/IecdrnV3dvLqngSfWV7Fq62FOdXRRkDGWvyubzm0LC5mcnRxQ1cFYMiGOJyo6WLHxEH9zTf9vFXYhmls7eGZLLb/deIjX9vT8ETIzL5W/v24GS+fkMWN8yqhZXUlhKCKBuao4m6T4WJ7eUsvVM3Lo6nbe2NvAU2/V8MyWWhpOtJOWGMctCwr404sLWDhx3IjufVyI3KQYFkzM4IkN1VENw9aOLsp31rNiUzXPba+jvbObiZlJ3H3tdJbPLxgR01EGQmEoIoFJHBPLtTNz+cPWWuJijKe31HKkpY2xY2K5rjSXm+fmc+3McN7h42w+sqCA+3+7le01TZTmpw3a+3Z2dfPangZWbDrEqi21NLd1kpkcz+2XFrF8fgEXTxz96+sqDEUkUB+6KJ+n3qrh1+sOct3M8Xxobj7XluQyNl4B2NeHLsrngd9t48mN1YMShttrmnj0zQM8tbmGIy3tpCbEccPsnrV1r5iWNeqvxfamMBSRQN00J4/HPruE0vy0YTn/bDjJSkngmhk5/HbDIb5448wBnTJu7eji6S01/Gz1AdbtbyQhrmcg04fnTaCsJGfEjQIdLPrJE5FAmRmXTM4MuowR45YFBTy/o47Vexu4fFp2v4870HCSn7+xn1+tPUjjyQ6mZCfz5Q+VctvCQjKSRs+czIGKahia2VLgO0As8CN3/0af1z8L3A10AS3AXe6+LfLafcCnI699zt1XRbNWEZGR4IOl40lJiOPJDdXvG4buzurKozz86l6e236YGDOuLx3PJxdP4vJpWaEdjHQ2UQtDM4sFHgSuB6qANWa24nTYRTzi7j+I7L8M+Baw1MxmAbcDs4EJwHNmNsPdu6JVr4jISDA2Ppalc/J4enMtDyyfc9bTmq0dXfxu0yEefnUf22uayEyO5+6y6Xxy8STy0kfnwgQXKpo9w0VAhbtXApjZo8By4EwYuntTr/2TAY98vRx41N3bgL1mVhF5v9ejWK+IyIjwkQUFPLauiue31/Ghuflnttc1t/Kz1Qd45I39HGlpp2R8Kt+89SKWzy8I7bXA/opmGBYAB3s9rwIu67uTmd0N3AvEAx/odezqPscWnOXYu4C7AHJycigvLx+MukOnpaVFbTcAareBUbsNTO9263YnI8H40XObSD66k73Hu/jD/g7erOmi22FuTix/VZpIaWYXdqKS1a9WBlv8CBD4ABp3fxB40Mw+AXwZuPM8jn0IeAigpKTEy8rKolLjaFdeXo7a7vyp3QZG7TYwfdvtoye38Z+v7eO728aw/sAJkuNjuWPJZD51+eTQrc4zGKIZhtVA7xVlCyPbzuVR4PsDPFZEJFRuXVjIf7yyl4YT7dx/8yw+ekkhqcP49k7DXTTDcA1QbGZT6Amy24FP9N7BzIrdfXfk6YeA01+vAB4xs2/RM4CmGHgzirWKiIwoM/PSeP2+68hOSSBWo0IvWNTC0N07zeweYBU9UysedvetZvYAsNbdVwD3mNkHgQ6gkcgp0sh+v6JnsE0ncLdGkoqIvNP4NI0MHSxRvWbo7iuBlX223d/r68+/x7FfB74evepERER6hGfhORERkXNQGIqISOgpDEVEJPQUhiIiEnoKQxERCT2FoYiIhJ7CUEREQk9hKCIioacwFBGR0DN3f/+9RgAzawZ2Bl3HCJUNHAm6iBFI7TYwareBUbsNTIm7p77fToHfwmkQ7XT3S4IuYiQys7Vqu/OndhsYtdvAqN0GxszW9mc/nSYVEZHQUxiKiEjojaYwfCjoAkYwtd3AqN0GRu02MGq3gelXu42aATQiIiIDNZp6hiIiIgMyKsLQzJaa2U4zqzCzLwVdz0hhZg+bWZ2ZbQm6lpHCzIrM7EUz22ZmW83snDeolreZWaKZvWlmmyLt9v8EXdNIYmaxZrbBzH4fdC0jiZntM7PNZrbx/UaVjvjTpGYWC+wCrgeqgDXAn7n7tkALGwHM7GqgBfipu88Jup6RwMzygXx3X29mqcA64Bb9vL03MzMg2d1bzGwM8ArweXdfHXBpI4KZ3QtcAqS5+81B1zNSmNk+4BJ3f9/5maOhZ7gIqHD3SndvBx4Flgdc04jg7i8DR4OuYyRx9xp3Xx/5uhnYDhQEW9Xw5z1aIk/HRB4j+y/xIWJmhcCHgB8FXctoNhrCsAA42Ot5FfrlJEPAzCYDC4A3gq1kZIic6tsI1AHPurvarX/+P+Afge6gCxmBHPiDma0zs7vea8fREIYiQ87MUoDHgb9396ag6xkJ3L3L3ecDhcAiM9Op+fdhZjcDde6+LuhaRqgr3f1i4Cbg7silobMaDWFYDRT1el4Y2SYSFZFrXo8DP3f33wRdz0jj7seAF4GlQdcyAlwBLItc+3oU+ICZ/SzYkkYOd6+O/LcOeIKey2pnNRrCcA1QbGZTzCweuB1YEXBNMkpFBoL8B7Dd3b8VdD0jhZnlmFlG5Oux9Ax42xFsVcOfu9/n7oXuPpme320vuPsnAy5rRDCz5MggN8wsGbgBOOfI+REfhu7eCdwDrKJnMMOv3H1rsFWNDGb2C+B1oMTMqszs00HXNAJcAdxBz1/oGyOPPwm6qBEgH3jRzN6i5w/YZ91d0wQkmsYDr5jZJuBN4Cl3f+ZcO4/4qRUiIiIXasT3DEVERC6UwlBEREJPYSgiIqGnMBQRkdBTGIqISOgpDEVCyMzKdAcEkbcpDEVEJPQUhiLDmJl9MnIfwI1m9sPIYtctZvbtyH0BnzeznMi+881stZm9ZWZPmNm4yPbpZvZc5F6C681sWuTtU8zsMTPbYWY/j6yuIxJKCkORYcrMSoGPA1dEFrjuAv4cSAbWuvts4CXgK5FDfgp80d3nApt7bf858KC7zwMuB2oi2xcAfw/MAqbSs7qOSCjFBV2AiJzTdcBCYE2k0zaWntsfdQO/jOzzM+A3ZpYOZLj7S5HtPwF+HVmbscDdnwBw91aAyPu96e5Vkecbgcn03HRXJHQUhiLDlwE/cff73rHR7H/12W+gayq29fq6C/0+kBDTaVKR4et54DYzywUws0wzm0TPv9vbIvt8AnjF3Y8DjWZ2VWT7HcBL7t4MVJnZLZH3SDCzpCH9LkRGAP0lKDJMufs2M/syPXfqjgE6gLuBE/TcHPfL9Jw2/XjkkDuBH0TCrhL4y8j2O4AfmtkDkff46BB+GyIjgu5aITLCmFmLu6cEXYfIaKLTpCIiEnrqGYqISOipZygiIqGnMBQRkdBTGIqISOgpDEVEJPQUhiIiEnoKQxERCb3/C34KsX+UJ8qxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "train(net, data_iter, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4.3 Applying the Word Embedding Model\n",
    "After training the word embedding model, we can represent similarity in meaning between words based on the cosine similarity of two word vectors. As we can see, when using the trained word embedding model, the words closest in meaning to the word `chip` are mostly related to chips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.495: intel\n",
      "cosine sim=0.453: mips\n",
      "cosine sim=0.448: computerized\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data()\n",
    "    x = W[vocab[query_token]]\n",
    "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)\n",
    "    topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print(f'cosine sim={float(cos[i]):.3f}: {vocab.idx_to_token[i]}')\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ We can pretrain a skip-gram model through negative sampling.\n",
    "\n",
    "##### Exercises\n",
    "1. Set `sparse_grad=True` when creating an instance of `nn.Embedding`. Does it accelerate training? Look up `MXNet` documentation to learn the meaning of this argument.\n",
    "2. Try to find synonyms for other words.\n",
    "3. Tune the hyperparameters and observe and analyze the experimental results.\n",
    "4. When the dataset is large, we usually sample the context words and the noise words for the central target word in the current minibatch only when updating the model parameters. In other words, the same central target word may have different context words or noise words in different epochs. What are the benefits of this sort of training? Try to implement this training method.\n",
    "\n",
    "\n",
    "## 14.5 Word Embedding with Global Vectors (GloVe)\n",
    "First, we should review the skip-gram model in word2vec. The conditional probability $P(w_j\\mid w_i)$ expressed in the skip-gram model using the softmax operation will be recorded as $q_{ij}$, that is:\n",
    "\n",
    "$$q_{ij}=\\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)}{\\displaystyle\\sum_{k \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_k^\\top \\mathbf{v}_i)},$$\n",
    "\n",
    "where $\\mathbf{v}_i$ and $\\mathbf{u}_i$ are the vector representations of word $w_i$ of index $i$ as the center word and context word respectively, and $\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$ is the vocabulary index set.\n",
    "\n",
    "For word $w_i$, it may appear in the dataset for multiple times. We collect all the context words every time when $w_i$ is a center word and keep duplicates, denoted as multiset $\\mathcal{C}_i$. The number of an element in a multiset is called the multiplicity of the element. For instance, suppose that word $w_i$ appears twice in the dataset: the context windows when these two $w_i$ become center words in the text sequence contain context word indices $2, 1, 5, 2$ and $2, 3, 2, 1$. Then, multiset $\\mathcal{C}_i = \\{1, 1, 2, 2, 2, 2, 3, 5\\}$, where multiplicity of element 1 is 2, multiplicity of element 2 is 4, and multiplicities of elements 3 and 5 are both 1. Denote multiplicity of element $j$ in multiset $\\mathcal{C}_i$ as $x_{ij}$: it is the number of word $w_j$ in all the context windows for center word $w_i$ in the entire dataset. As a result, the loss function of the skip-gram model can be expressed in a different way:\n",
    "\n",
    "$$-\\sum_{i\\in\\mathcal{V}}\\sum_{j\\in\\mathcal{V}} x_{ij} \\log\\,q_{ij}.$$\n",
    "\n",
    "We add up the number of all the context words for the central target word $w_i$ to get $x_i$, and record the conditional probability $x_{ij}/x_i$ for generating context word $w_j$ based on central target word $w_i$ as $p_{ij}$. We can rewrite the loss function of the skip-gram model as\n",
    "\n",
    "$$-\\sum_{i\\in\\mathcal{V}} x_i \\sum_{j\\in\\mathcal{V}} p_{ij} \\log\\,q_{ij}.$$\n",
    "\n",
    "In the formula above, $\\displaystyle\\sum_{j\\in\\mathcal{V}} p_{ij} \\log\\,q_{ij}$ computes the conditional probability distribution $p_{ij}$ for context word generation based on the central target word $w_i$ and the cross-entropy of conditional probability distribution $q_{ij}$ predicted by the model. The loss function is weighted using the sum of the number of context words with the central target word $w_i$. If we minimize the loss function from the formula above, we will be able to allow the predicted conditional probability distribution to approach as close as possible to the true conditional probability distribution.\n",
    "\n",
    "However, although the most common type of loss function, the cross-entropy loss function is sometimes not a good choice. On the one hand, as we mentioned in `Section 14.2` the cost of letting the model prediction $q_{ij}$ become the legal probability distribution has the sum of all items in the entire dictionary in its denominator. This can easily lead to excessive computational overhead. On the other hand, there are often a lot of uncommon words in the dictionary, and they appear rarely in the dataset. In the cross-entropy loss function, the final prediction of the conditional probability distribution on a large number of uncommon words is likely to be inaccurate.\n",
    "\n",
    "\n",
    "\n",
    "### 14.5.1 The GloVe Model\n",
    "To address this, GloVe (`Pennington et al., 2014`), a word embedding model that came after word2vec, adopts square loss and makes three changes to the skip-gram model based on this loss.\n",
    "\n",
    "1. Here, we use the non-probability distribution variables $p'_{ij}=x_{ij}$ and $q'_{ij}=\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)$ and take their logs. Therefore, we get the square loss $\\left(\\log\\,p'_{ij} - \\log\\,q'_{ij}\\right)^2 = \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i - \\log\\,x_{ij}\\right)^2$.\n",
    "2. We add two scalar model parameters for each word $w_i$: the bias terms $b_i$ (for central target words) and $c_i$( for context words).\n",
    "3. Replace the weight of each loss with the function $h(x_{ij})$. The weight function $h(x)$ is a monotone increasing function with the range $[0, 1]$.\n",
    "\n",
    "Therefore, the goal of GloVe is to minimize the loss function.\n",
    "\n",
    "$$\\sum_{i\\in\\mathcal{V}} \\sum_{j\\in\\mathcal{V}} h(x_{ij}) \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log\\,x_{ij}\\right)^2.$$\n",
    "\n",
    "Here, we have a suggestion for the choice of weight function $h(x)$: when $x < c$ (e.g $c = 100$), make $h(x) = (x/c) ^\\alpha$ (e.g $\\alpha = 0.75$), otherwise make $h(x) = 1$. Because $h(0)=0$, the squared loss term for $x_{ij}=0$ can be simply ignored. When we use minibatch SGD for training, we conduct random sampling to get a non-zero minibatch $x_{ij}$ from each timestep and compute the gradient to update the model parameters. These non-zero $x_{ij}$ are computed in advance based on the entire dataset and they contain global statistics for the dataset. Therefore, the name GloVe is taken from \"Global Vectors\".\n",
    "\n",
    "Notice that if word $w_i$ appears in the context window of word $w_j$, then word $w_j$ will also appear in the context window of word $w_i$. Therefore, $x_{ij}=x_{ji}$. Unlike word2vec, GloVe fits the symmetric $\\log\\, x_{ij}$ in lieu of the asymmetric conditional probability $p_{ij}$. Therefore, the central target word vector and context word vector of any word are equivalent in GloVe. However, the two sets of word vectors that are learned by the same word may be different in the end due to different initialization values. After learning all the word vectors, GloVe will use the sum of the central target word vector and the context word vector as the final word vector for the word.\n",
    "\n",
    "\n",
    "### 14.5.2 Understanding GloVe from Conditional Probability Ratios\n",
    "We can also try to understand GloVe word embedding from another perspective. We will continue the use of symbols from earlier in this section, $P(w_j \\mid w_i)$ represents the conditional probability of generating context word $w_j$ with central target word $w_i$ in the dataset, and it will be recorded as $p_{ij}$. From a real example from a large corpus, here we have the following two sets of conditional probabilities with \"ice\" and \"steam\" as the central target words and the ratio between them:\n",
    "\n",
    "<img src=\"images/t_14_01.png\" style=\"width:500px;\"/>\n",
    "\n",
    "We will be able to observe phenomena such as:\n",
    "* For a word $w_k$ that is related to \"ice\" but not to \"steam\", such as $w_k=$\"solid\", we would expect a larger conditional probability ratio, like the value 8.9 in the last row of the table above.\n",
    "* For a word $w_k$ that is related to \"steam\" but not to \"ice\", such as $w_k=$\"gas\", we would expect a smaller conditional probability ratio, like the value 0.085 in the last row of the table above.\n",
    "* For a word $w_k$ that is related to both \"ice\" and \"steam\", such as $w_k=$\"water\", we would expect a conditional probability ratio close to 1, like the value 1.36 in the last row of the table above.\n",
    "* For a word $w_k$ that is related to neither \"ice\" or \"steam\", such as $w_k=$\"fashion\", we would expect a conditional probability ratio close to 1, like the value 0.96 in the last row of the table above.\n",
    "\n",
    "We can see that the conditional probability ratio can represent the relationship between different words more intuitively. We can construct a word vector function to fit the conditional probability ratio more effectively. As we know, to obtain any ratio of this type requires three words $w_i$, $w_j$, and $w_k$. The conditional probability ratio with $w_i$ as the central target word is ${p_{ij}}/{p_{ik}}$. We can find a function that uses word vectors to fit this conditional probability ratio.\n",
    "\n",
    "$$f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) \\approx \\frac{p_{ij}}{p_{ik}}.$$\n",
    "\n",
    "The possible design of function $f$ here will not be unique. We only need to consider a more reasonable possibility. Notice that the conditional probability ratio is a scalar, we can limit $f$ to be a scalar function: $f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) = f\\left((\\mathbf{u}_j - \\mathbf{u}_k)^\\top {\\mathbf{v}}_i\\right)$. After exchanging index $j$ with $k$, we will be able to see that function $f$ satisfies the condition $f(x)f(-x)=1$, so one possibility could be $f(x)=\\exp(x)$. Thus:\n",
    "\n",
    "$$f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) = \\frac{\\exp\\left(\\mathbf{u}_j^\\top {\\mathbf{v}}_i\\right)}{\\exp\\left(\\mathbf{u}_k^\\top {\\mathbf{v}}_i\\right)} \\approx \\frac{p_{ij}}{p_{ik}}.$$\n",
    "\n",
    "One possibility that satisfies the right side of the approximation sign is $\\exp\\left(\\mathbf{u}_j^\\top {\\mathbf{v}}_i\\right) \\approx \\alpha p_{ij}$, where $\\alpha$ is a constant. Considering that $p_{ij}=x_{ij}/x_i$, after taking the logarithm we get $\\mathbf{u}_j^\\top {\\mathbf{v}}_i \\approx \\log\\,\\alpha + \\log\\,x_{ij} - \\log\\,x_i$. We use additional bias terms to fit $- \\log\\, \\alpha + \\log\\, x_i$, such as the central target word bias term $b_i$ and context word bias term $c_j$:\n",
    "\n",
    "$$\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j \\approx \\log(x_{ij}).$$\n",
    "\n",
    "By taking the square error and weighting the left and right sides of the formula above, we can get the loss function of GloVe.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* In some cases, the cross-entropy loss function may have a disadvantage. GloVe uses squared loss and the word vector to fit global statistics computed in advance based on the entire dataset.\n",
    "* The central target word vector and context word vector of any word are equivalent in GloVe.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. If a word appears in the context window of another word, how can we use the\n",
    "  distance between them in the text sequence to redesign the method for\n",
    "  computing the conditional probability $p_{ij}$? Hint: See section 4.2 from the\n",
    "  paper GloVe :cite:`Pennington.Socher.Manning.2014`.\n",
    "1. For any word, will its central target word bias term and context word bias term be equivalent to each other in GloVe? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6 Subword Embedding\n",
    "English words usually have internal structures and formation methods. For example, we can deduce the relationship between `dog`, `dogs`, and `dogcatcher` by their spelling. All these words have the same root, `dog`, but they use different suffixes to change the meaning of the word. Moreover, this association can be extended to other words. For example, the relationship between `dog` and `dogs` is just like the relationship between `cat` and `cats`. The relationship between `boy` and `boyfriend` is just like the relationship between `girl` and `girlfriend`. This characteristic is not unique to English. In French and Spanish, a lot of verbs can have more than 40 different forms depending on the context. In Finnish, a noun may have more than 15 forms. In fact, morphology, which is an important branch of linguistics, studies the internal structure and formation of words.\n",
    "\n",
    "### 14.6.1 fastText\n",
    "In word2vec, we did not directly use morphology information. In both the skip-gram model and continuous bag-of-words model, we use different vectors to represent words with different forms. For example, `dog` and `dogs` are represented by two different vectors, while the relationship between these two vectors is not directly represented in the model. In view of this, fastText (`Bojanowski et al., 2017`) proposes the method of subword embedding, thereby attempting to introduce morphological information in the skip-gram model in word2vec.\n",
    "\n",
    "In fastText, each central word is represented as a collection of subwords. Below we use the word `where` as an example to understand how subwords are formed. First, we add the special characters `<` and `>` at the beginning and end of the word to distinguish the subwords used as prefixes and suffixes. Then, we treat the word as a sequence of characters to extract the $n$-grams. For example, when $n=3$, we can get all subwords with a length of $3$:\n",
    "\n",
    "$$\\textrm{\"<wh\"}, \\ \\textrm{\"whe\"}, \\ \\textrm{\"her\"}, \\ \\textrm{\"ere\"}, \\ \\textrm{\"re>\"},$$\n",
    "\n",
    "and the special subword $\\textrm{\"\"}$.\n",
    "\n",
    "In fastText, for a word $w$, we record the union of all its subwords with length of $3$ to $6$ and special subwords as $\\mathcal{G}_w$. Thus, the dictionary is the union of the collection of subwords of all words. Assume the vector of the subword $g$ in the dictionary is $\\mathbf{z}_g$. Then, the central word vector $\\mathbf{u}_w$ for the word $w$ in the skip-gram model can be expressed as\n",
    "\n",
    "$$\\mathbf{u}_w = \\sum_{g\\in\\mathcal{G}_w} \\mathbf{z}_g.$$\n",
    "\n",
    "The rest of the fastText process is consistent with the skip-gram model, so it is not repeated here. As we can see, compared with the skip-gram model, the dictionary in fastText is larger, resulting in more model parameters. Also, the vector of one word requires the summation of all subword vectors, which results in higher computation complexity. However, we can obtain better vectors for more uncommon complex words, even words not existing in the dictionary, by looking at other words with similar structures.\n",
    "\n",
    "### 14.6.2 Byte Pair Encoding\n",
    "In fastText, all the extracted subwords have to be of the specified lengths, such as $3$ to $6$, thus the vocabulary size cannot be predefined. To allow for variable-length subwords in a fixed-size vocabulary, we can apply a compression algorithm called `byte pair encoding` (`BPE`) to extract subwords (`Sennrich et al., 2015`).\n",
    "\n",
    "Byte pair encoding performs a statistical analysis of the training dataset to discover common symbols within a word, such as consecutive characters of arbitrary length. Starting from symbols of length $1$, byte pair encoding iteratively merges the most frequent pair of consecutive symbols to produce new longer symbols. Note that for efficiency, pairs crossing word boundaries are not considered. In the end, we can use such symbols as subwords to segment words. Byte pair encoding and its variants has been used for input representations in popular natural language processing pretraining models such as `GPT-2` (`Radford et al., 2019`) and `RoBERTa` (`Liu et al., 2019`). In the following, we will illustrate how byte pair encoding works.\n",
    "\n",
    "First, we initialize the vocabulary of symbols as all the English lowercase characters, a special end-of-word symbol `_`, and a special unknown symbol `[UNK]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not consider symbol pairs that cross boundaries of words, we only need a dictionary `raw_token_freqs` that maps words to their frequencies (`number of occurrences`) in a dataset. Note that the special symbol `_` is appended to each word so that we can easily recover a word sequence (e.g., `a taller man`) from a sequence of output symbols ( e.g., `a_ tall er_ man`). Since we start the merging process from a vocabulary of only single characters and special symbols, space is inserted between every pair of consecutive characters within each word (keys of the dictionary `token_freqs`). In other words, space is the delimiter between symbols within a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "token_freqs = {}\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following `get_max_freq_pair` function that returns the most frequent pair of consecutive symbols within a word, where words come from keys of the input dictionary `token_freqs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_freq_pair(token_freqs):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Key of `pairs` is a tuple of two consecutive symbols\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return max(pairs, key=pairs.get)  # Key of `pairs` with the max value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a greedy approach based on frequency of consecutive symbols, byte pair encoding will use the following `merge_symbols` function to merge the most frequent pair of consecutive symbols to produce new symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair), ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iteratively perform the byte pair encoding algorithm over the keys of the dictionary `token_freqs`. In the first iteration, the most frequent pair of consecutive symbols are `t` and `a`, thus byte pair encoding merges them to produce a new symbol `ta`. In the second iteration, byte pair encoding continues to merge `ta` and `l` to result in another new symbol `tal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge #1: ('t', 'a')\n",
      "merge #2: ('ta', 'l')\n",
      "merge #3: ('tal', 'l')\n",
      "merge #4: ('f', 'a')\n",
      "merge #5: ('fa', 's')\n",
      "merge #6: ('fas', 't')\n",
      "merge #7: ('e', 'r')\n",
      "merge #8: ('er', '_')\n",
      "merge #9: ('tall', '_')\n",
      "merge #10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f'merge #{i + 1}:', max_freq_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10 iterations of byte pair encoding, we can see that list symbols now contains 10 more symbols that are iteratively merged from other symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n"
     ]
    }
   ],
   "source": [
    "print(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same dataset specified in the keys of the dictionary `raw_token_freqs`, each word in the dataset is now segmented by subwords `fast_`, `fast`, `er_`, `tall_`, and `tall` as a result of the byte pair encoding algorithm. For instance, words `faster_` and `taller_` are segmented as `fast er_` and `tall er_`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fast_', 'fast er_', 'tall_', 'tall er_']\n"
     ]
    }
   ],
   "source": [
    "print(list(token_freqs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result of byte pair encoding depends on the dataset being used. We can also use the subwords learned from one dataset to segment words of another dataset. As a greedy approach, the following `segment_BPE` function tries to break words into the longest possible subwords from the input argument symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_BPE(tokens, symbols):\n",
    "    outputs = []\n",
    "    for token in tokens:\n",
    "        start, end = 0, len(token)\n",
    "        cur_output = []\n",
    "        # Segment token with the longest possible subwords from symbols\n",
    "        while start < len(token) and start < end:\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we use the subwords in list symbols, which is learned from the aforementioned dataset, to segment tokens that represent another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tall e s t _', 'fa t t er_']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['tallest_', 'fatter_']\n",
    "print(segment_BPE(tokens, symbols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ FastText proposes a subword embedding method. Based on the skip-gram model in word2vec, it represents the central word vector as the sum of the subword vectors of the word.\n",
    "+ Subword embedding utilizes the principles of morphology, which usually improves the quality of representations of uncommon words.\n",
    "+ Byte pair encoding performs a statistical analysis of the training dataset to discover common symbols within a word. As a greedy approach, byte pair encoding iteratively merges the most frequent pair of consecutive symbols.\n",
    "\n",
    "##### Exercises\n",
    "1. When there are too many subwords (for example, 6 words in English result in about $3\\times 10^8$ combinations), what problems arise? Can you think of any methods to solve them? Hint: Refer to the end of section 3.2 of the fastText paper[1].\n",
    "2. How can you design a subword embedding model based on the continuous bag-of-words model?\n",
    "3. To get a vocabulary of size $m$, how many merging operations are needed when the initial symbol vocabulary size is $n$?\n",
    "4. How can we extend the idea of byte pair encoding to extract phrases?\n",
    "\n",
    "\n",
    "## 14.7 Finding Synonyms and Analogies\n",
    "In `Section 14.4` we trained a word2vec word embedding model on a small-scale dataset and searched for synonyms using the cosine similarity of word vectors. In practice, word vectors pretrained on a large-scale corpus can often be applied to downstream natural language processing tasks. This section will demonstrate how to use these pretrained word vectors to find synonyms and analogies. We will continue to apply pretrained word vectors in subsequent sections.\n",
    "\n",
    "### 14.7.1 Using Pretrained Word Vectors\n",
    "Below lists pretrained GloVe embeddings of dimensions 50, 100, and 300, which can be downloaded from the [GloVe website](https://nlp.stanford.edu/projects/glove/). The pretrained fastText embeddings are available in multiple languages. Here we consider one English version (300-dimensional `wiki.en`) that can be downloaded from the [fastText website](https://fasttext.cc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL+'glove.6B.50d.zip', '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL+'glove.6B.100d.zip', 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL+'glove.42B.300d.zip', 'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL+'wiki.en.zip', 'c1816da3821ae9f43899be655002f6c723e91b88')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following `TokenEmbedding` class to load the above pretrained Glove and fastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = d2l.download_extract(embedding_name)\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        print(os.path.join(data_dir, 'vec.txt'))\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, np.array(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx) for token in tokens]\n",
    "        vecs = self.idx_to_vec[np.array(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use 50-dimensional GloVe embeddings pretrained on a subset of the Wikipedia. The corresponding word embedding is automatically downloaded the first time we create a pretrained word embedding instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/glove.6B.50d/vec.txt\n"
     ]
    }
   ],
   "source": [
    "glove_6b50d = TokenEmbedding('glove.6b.50d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the dictionary size. The dictionary contains $400,000$ words and a special unknown token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a word to get its index in the dictionary, or we can get the word from its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3367, 'beautiful')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.7.2 Applying Pretrained Word Vectors\n",
    "Below, we demonstrate the application of pretrained word vectors, using GloVe as an example.\n",
    "\n",
    "##### Finding Synonyms\n",
    "Here, we re-implement the algorithm used to search for synonyms by cosine similarity introduced in `Section 14.1`.\n",
    "\n",
    "In order to reuse the logic for seeking the $k$ nearest neighbors when seeking analogies, we encapsulate this part of the logic separately in the knn ($k$-nearest neighbors) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(W, x, k):\n",
    "    # The added 1e-9 is for numerical stability\n",
    "    cos = np.dot(W, x.reshape(-1,))/(np.sqrt(np.sum(W * W, axis=1) + 1e-9) * np.sqrt((x * x).sum()))\n",
    "    topk = npx.topk(cos, k=k, ret_typ='indices')\n",
    "    return topk, [cos[int(i)] for i in topk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we search for synonyms by pre-training the word vector instance embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):  # Remove input words\n",
    "        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary of pretrained word vector instance `glove_6b50d` already created contains 400,000 words and a special unknown token. Excluding input words and unknown words, we search for the three words that are the most similar in meaning to `chip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.856: chips\n",
      "cosine sim=0.749: intel\n",
      "cosine sim=0.749: electronics\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('chip', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we search for the synonyms of `baby` and `beautiful`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.839: babies\n",
      "cosine sim=0.800: boy\n",
      "cosine sim=0.792: girl\n",
      "cosine sim=0.921: lovely\n",
      "cosine sim=0.893: gorgeous\n",
      "cosine sim=0.830: wonderful\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('baby', 3, glove_6b50d)\n",
    "get_similar_tokens('beautiful', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding Analogies\n",
    "In addition to seeking synonyms, we can also use the pretrained word vector to seek the analogies between words. For example, `man`:`woman`::`son`:`daughter` is an example of analogy, `man` is to `woman` as `son` is to `daughter`. The problem of seeking analogies can be defined as follows: for four words in the analogical relationship $a : b :: c : d$, given the first three words, $a$, $b$ and $c$, we want to find $d$. Assume the word vector for the word $w$ is $\\text{vec}(w)$. To solve the analogy problem, we need to find the word vector that is most similar to the result vector of $\\text{vec}(c)+\\text{vec}(b)-\\text{vec}(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    vecs = embed[[token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = knn(embed.idx_to_vec, x, 1)\n",
    "    return embed.idx_to_token[int(topk[0])]  # Remove unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the `male-female` analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('man', 'woman', 'son', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Capital-country` analogy: `beijing` is to `china` as `tokyo` is to what? The answer should be `japan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japan'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('beijing', 'china', 'tokyo', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Adjective-superlative adjective` analogy: `bad` is to `worst` as `big` is to what? The answer should be `biggest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biggest'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('bad', 'worst', 'big', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Present tense verb-past tense verb` analogy: `do` is to `did` as `go` is to what? The answer should be `went`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('do', 'did', 'go', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ Word vectors pre-trained on a large-scale corpus can often be applied to downstream natural language processing tasks.\n",
    "+ We can use pre-trained word vectors to seek synonyms and analogies.\n",
    "\n",
    "##### Exercises\n",
    "1. Test the fastText results using `TokenEmbedding('wiki.en')`.\n",
    "2. If the dictionary is extremely large, how can we accelerate finding synonyms and analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
