{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import d2l\n",
    "import os\n",
    "\n",
    "from mxnet import autograd, np, npx, gluon, init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  07. Modern Convolutional Neural Networks\n",
    "Now that we understand the basics of wiring together convolutional neural networks, we will take you through a tour of modern deep learning. In this chapter, each section will correspond to a significant neural network architecture that was at some point (or currently) the base model upon which an enormous amount of research and projects were built. Each of these networks was at briefly a dominant architecture and many were at one point winners or runners-up in the famous `ImageNet` competition, which has served as a barometer of progress on supervised learning in computer vision since 2010.\n",
    "\n",
    "These models include \n",
    "+ `AlexNet`: the first large-scale network deployed to beat conventional computer vision methods on a large-scale vision challenge; \n",
    "+ `VGG`: makes use of a number of repeating blocks of elements\n",
    "+ `NiN`: network in network, which convolves whole neural networks patch-wise over inputs\n",
    "+ `GoogLeNet`: makes use of networks with parallel concatenations\n",
    "+ `ResNet`: residual networks, which are the most popular go-to architecture today\n",
    "+ `DenseNet`: densely connected networks, which are expensive to compute but have set some recent benchmarks\n",
    "\n",
    "\n",
    "## 7.4 Networks with Parallel Concatenations (GoogLeNet)\n",
    "In 2014, (`Szegedy et al., 2015`) won the ImageNet Challenge, proposing a structure that combined the strengths of the `NiN` and repeated blocks paradigms. One focus of the paper was to address the question of which sized convolutional kernels are best. After all, previous popular networks employed choices as small as $1 \\times 1$ and as large as $11 \\times 11$. One insight in this paper was that sometimes it can be advantageous to employ a combination of variously-sized kernels. In this section, we will introduce `GoogLeNet`, presenting a slightly simplified version of the original model---we omit a few ad hoc features that were added to stabilize training but are unnecessary now with better training algorithms available.\n",
    "\n",
    "### 7.4.1 Inception Blocks\n",
    "The basic convolutional block in `GoogLeNet` is called an `Inception` block, likely named due to a quote from the movie `Inception` (\"We Need To Go Deeper\"), which launched a viral meme.\n",
    "\n",
    "<img src=\"images/07_05.png\" style=\"width:500px;\"/>\n",
    "\n",
    "As depicted in the figure above, the inception block consists of four parallel paths. The first three paths use convolutional layers with window sizes of $1\\times 1$, $3\\times 3$, and $5\\times 5$ to extract information from different spatial sizes. The middle two paths perform a $1\\times 1$ convolution on the input to reduce the number of input channels, reducing the model's complexity. The fourth path uses a $3\\times 3$ maximum pooling layer, followed by a $1\\times 1$ convolutional layer to change the number of channels. The four paths all use appropriate padding to give the input and output the same height and width. Finally, the outputs along each path are concatenated along the channel dimension and comprise the block's output. The commonly-tuned parameters of the Inception block are the number of output channels per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Block):\n",
    "    # c1 - c4 are the number of output channels for each layer in the path\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # Path 1 is a single 1 x 1 convolutional layer\n",
    "        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n",
    "        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3 convolutional layer\n",
    "        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
    "        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1, activation='relu')\n",
    "        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5 convolutional layer\n",
    "        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n",
    "        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2, activation='relu')\n",
    "        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1 convolutional layer\n",
    "        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_2(self.p2_1(x))\n",
    "        p3 = self.p3_2(self.p3_1(x))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "        # Concatenate the outputs on the channel dimension\n",
    "        return np.concatenate((p1, p2, p3, p4), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain some intuition for why this network works so well, consider the combination of the filters. They explore the image in varying ranges. This means that details at different extents can be recognized efficiently by different filters. At the same time, we can allocate different amounts of parameters for different ranges (e.g., more for short range but not ignore the long range entirely).\n",
    "\n",
    "### 7.4.2 GoogLeNet Model\n",
    "As shown in `Fig. 7.4.2`, `GoogLeNet` uses a stack of a total of 9 inception blocks and global average pooling to generate its estimates. Maximum pooling between inception blocks reduced the dimensionality. The first part is identical to `AlexNet` and `LeNet`, the stack of blocks is inherited from `VGG` and the global average pooling avoids a stack of fully-connected layers at the end. The architecture is depicted below.\n",
    "\n",
    "<img src=\"images/07_06.png\" style=\"width:250px;\"/>\n",
    "\n",
    "We can now implement `GoogLeNet` piece by piece. The first component uses a 64-channel 7 Ã— 7 convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential()\n",
    "b1.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second component uses two convolutional layers: first, a 64-channel $1\\times 1$ convolutional layer, then a $3\\times 3$ convolutional layer that triples the number of channels. This corresponds to the second path in the Inception block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential()\n",
    "b2.add(nn.Conv2D(64, kernel_size=1, activation='relu'),\n",
    "       nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third component connects two complete `Inception` blocks in series. The number of output channels of the first Inception block is $64+128+32+32=256$, and the ratio to the output channels of the four paths is $64:128:32:32=2:4:1:1$. The second and third paths first reduce the number of input channels to $96/192=1/2$ and $16/192=1/12$, respectively, and then connect the second convolutional layer. The number of output channels of the second Inception block is increased to $128+192+96+64=480$, and the ratio to the number of output channels per path is $128:192:96:64 = 4:6:3:2$. The second and third paths first reduce the number of input channels to $128/256=1/2$ and $32/256=1/8$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = nn.Sequential()\n",
    "b3.add(Inception(64, (96, 128), (16, 32), 32),\n",
    "       Inception(128, (128, 192), (32, 96), 64),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth block is more complicated. It connects five Inception blocks in series, and they have $192+208+48+64=512$, $160+224+64+64=512$, $128+256+64+64=512$, $112+288+64+64=528$, and $256+320+128+128=832$ output channels, respectively. The number of channels assigned to these paths is similar to that in the third module: the second path with the $3\\times 3$ convolutional layer outputs the largest number of channels, followed by the first path with only the $1\\times 1$ convolutional layer, the third path with the $5\\times 5$ convolutional layer, and the fourth path with the $3\\times 3$ maximum pooling layer. The second and third paths will first reduce the number of channels according the ratio. These ratios are slightly different in different Inception blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential()\n",
    "b4.add(Inception(192, (96, 208), (16, 48), 64),\n",
    "       Inception(160, (112, 224), (24, 64), 64),\n",
    "       Inception(128, (128, 256), (24, 64), 64),\n",
    "       Inception(112, (144, 288), (32, 64), 64),\n",
    "       Inception(256, (160, 320), (32, 128), 128),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth block has two Inception blocks with $256+320+128+128=832$ and $384+384+128+128=1024$ output channels. The number of channels assigned to each path is the same as that in the third and fourth modules, but differs in specific values. It should be noted that the fifth block is followed by the output layer. This block uses the global average pooling layer to change the height and width of each channel to 1, just as in NiN. Finally, we turn the output into a two-dimensional array followed by a fully-connected layer whose number of outputs is the number of label classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b5 = nn.Sequential()\n",
    "b5.add(Inception(256, (160, 320), (32, 128), 128),\n",
    "       Inception(384, (192, 384), (48, 128), 128),\n",
    "       nn.GlobalAvgPool2D())\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(b1, b2, b3, b4, b5, nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GoogLeNet model is computationally complex, so it is not as easy to modify the number of channels as in VGG. To have a reasonable training time on Fashion-MNIST, we reduce the input height and width from 224 to 96. This simplifies the computation. The changes in the shape of the output between the various modules is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential0 output shape:\t (1, 64, 24, 24)\n",
      "sequential1 output shape:\t (1, 192, 12, 12)\n",
      "sequential2 output shape:\t (1, 480, 6, 6)\n",
      "sequential3 output shape:\t (1, 832, 3, 3)\n",
      "sequential4 output shape:\t (1, 1024, 1, 1)\n",
      "dense0 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 96, 96))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 Data Acquisition and Training\n",
    "As before, we train our model using the Fashion-MNIST dataset. We transform it to $96 \\times 96$ pixel resolution before invoking the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-86232926a7c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_fashion_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ch6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/d2l/d2l.py\u001b[0m in \u001b[0;36mtrain_ch6\u001b[0;34m(net, train_iter, test_iter, num_epochs, lr, ctx)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/d2l/d2l.py\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(y_hat, y)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         return float((y_hat.argmax(axis=1).astype('float32') == y.astype(\n\u001b[0;32m--> 206\u001b[0;31m             'float32')).sum())\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/mxnet/numpy/multiarray.py\u001b[0m in \u001b[0;36m__float__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_elements\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'only size-1 arrays can be converted to Python scalars'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__int__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/mxnet/numpy/multiarray.py\u001b[0m in \u001b[0;36mitem\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \"\"\"\n\u001b[1;32m    832\u001b[0m         \u001b[0;31m# TODO(junwu): no need to call asnumpy() on the whole array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/3rd/py-venv/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2536\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ The `Inception` block is equivalent to a subnetwork with four paths. It extracts information in parallel through convolutional layers of different window shapes and maximum pooling layers. $1 \\times 1$ convolutions reduce channel dimensionality on a per-pixel level. Max-pooling reduces the resolution.\n",
    "+ `GoogLeNet` connects multiple well-designed Inception blocks with other layers in series. The ratio of the number of channels assigned in the Inception block is obtained through a large number of experiments on the ImageNet dataset.\n",
    "+ `GoogLeNet`, as well as its succeeding versions, was one of the most efficient models on ImageNet, providing similar test accuracy with lower computational complexity.\n",
    "\n",
    "##### Exercises\n",
    "1. There are several iterations of GoogLeNet. Try to implement and run them. Some of them include the following:\n",
    "    + Add a batch normalization layer (`Ioffe & Szegedy, 2015`), as described later in `Section 7.5`.\n",
    "    + Make adjustments to the Inception block (`Szegedy et al., 2016`).\n",
    "    + Use \"label smoothing\" for model regularization (`Szegedy et al., 2016`).\n",
    "    + Include it in the residual connection (`Szegedy et al., 2017`), as described later in `Section 7.6`.\n",
    "2. What is the minimum image size for `GoogLeNet` to work?\n",
    "3. Compare the model parameter sizes of `AlexNet`, `VGG`, and `NiN` with `GoogLeNet`. How do the latter two network architectures significantly reduce the model parameter size?\n",
    "4. Why do we need a large range convolution initially?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
