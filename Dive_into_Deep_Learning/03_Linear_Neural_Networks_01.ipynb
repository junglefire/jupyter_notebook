{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "from mxnet import autograd, np, npx, gluon, init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 定义绘图相关函数\n",
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\" \n",
    "    # display.set_matplotlib_formats('svg')\n",
    "    \n",
    "def set_figsize(figsize=(7, 5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, \n",
    "         xscale='linear', yscale='linear',fmts=('-', 'm--', 'g-.', 'r:'), figsize=(7, 5), axes=None\n",
    "    ):\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else plt.gca()\n",
    "\n",
    "    # Return True if X (ndarray or list) has 1 axis\n",
    "    def has_one_axis(X):\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "    plt.show()\n",
    "\n",
    "# Saved in the d2l package for later use \n",
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\" \n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        # Start the timer \n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        # Stop the timer and record the time in a list \n",
    "        self.times.append(time.time() - self.tik) \n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        # Return the average time \n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        # Return the sum of time \n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        # Return the accumulated times \n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  03. Linear Neural Networks\n",
    "Before we get into the details of deep neural networks, we need to cover the basics of neural network training. In this chapter, we will cover the entire training process, including defining simple neural network architectures, handling data, specifying a loss function, and training the model. In order to make things easier to grasp, we begin with the simplest concepts. Fortunately, classic statistical learning techniques such as linear and logistic regression can be cast as shallow neural networks. Starting from these classic algorithms, we will introduce you to the basics, providing the basis for more complex techniques such as softmax regression (introduced at the end of this chapter) and multilayer perceptrons (introduced in the next chapter).\n",
    "\n",
    "## 3.1 Linear Regression\n",
    "Regression refers to a set of methods for modeling the relationship between data points $\\mathbf{x}$ and corresponding real-valued targets $y$. In the natural sciences and social sciences, the purpose of regression is most often to characterize the relationship between the inputs and outputs. Machine learning, on the other hand, is most often concerned with prediction.\n",
    "\n",
    "Regression problems pop up whenever we want to predict a numerical value. Common examples include predicting prices (of homes, stocks, etc.), predicting length of stay (for patients in the hospital), demand forecasting (for retail sales), among countless others. Not every prediction problem is a classic regression problem. In subsequent sections, we will introduce classification problems, where the goal is to predict membership among a set of categories.\n",
    "\n",
    "### 3.1.1 Basic Elements of Linear Regression\n",
    "Linear regression may be both the simplest and most popular among the standard tools to regression. Dating back to the dawn of the 19th century, linear regression flows from a few simple assumptions:\n",
    "+ First, we assume that the relationship between the features $\\mathbf{x}$ and targets $y$ is linear, i.e., that $y$ can be expressed as a weighted sum of the inputs $\\textbf{x}$, give or take some noise on the observations. \n",
    "+ Second, we assume that any noise is well-behaved (following a Gaussian distribution). \n",
    "\n",
    "To motivate the approach, let us start with a running example. Suppose that we wish to estimate the prices of houses (in dollars) based on their area (in square feet) and age (in years).\n",
    "\n",
    "To actually fit a model for predicting house prices, we would need to get our hands on a dataset consisting of sales for which we know the sale price, area and age for each home. In the terminology of machine learning, the dataset is called a `training data set` or `training set`, and each row (here the data corresponding to one sale) is called an `example` (or `data instance`, `data point`, `sample`). The thing we are trying to predict (here, the price) is called a `label` (or `target`). The variables (here age and area) upon which the predictions are based are called `features` or `covariates`.\n",
    "\n",
    "Typically, we will use $n$ to denote the number of examples in our dataset. We index the data instances by $i$, denoting each input as $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]$ and the corresponding label as $y^{(i)}$.\n",
    "\n",
    "##### Linear Model\n",
    "The linearity assumption just says that the target (price) can be expressed as a weighted sum of the features (area and age):\n",
    "$$\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b.$$\n",
    "\n",
    "Here, $w_{\\mathrm{area}}$ and $w_{\\mathrm{age}}$ are called `weights`, and $b$ is called a `bias` (also called an `offset` or `intercept`). The weights determine the influence of each feature on our prediction and the bias just says what value the predicted price should take when all of the features take value $0$. Even if we will never see any homes with zero area, or that are precisely zero years old, we still need the intercept or else we will limit the expressivity of our linear model.\n",
    "\n",
    "Given a dataset, our goal is to choose the weights $w$ and bias $b$ such that on average, the predictions made according to our model best fit the true prices observed in the data.\n",
    "\n",
    "In disciplines where it is common to focus on datasets with just a few features, explicitly expressing models long-form like this is common. In ML, we usually work with high-dimensional datasets, so it is more convenient to employ linear algebra notation. When our inputs consist of $d$ features, we express our prediction $\\hat{y}$ as\n",
    "$$\\hat{y} = w_1 \\cdot x_1 + ... + w_d \\cdot x_d + b.$$\n",
    "\n",
    "Collecting all features into a vector $\\mathbf{x}$ and all weights into a vector $\\mathbf{w}$, we can express our model compactly using a dot product:\n",
    "$$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b.$$\n",
    "\n",
    "Here, the vector $\\mathbf{x}$ corresponds to a single data point. We will often find it convenient to refer to our entire dataset via the design matrix $\\mathbf{X}$. Here, $\\mathbf{X}$ contains one row for every example and one column for every feature.\n",
    "\n",
    "For a collection of data points $\\mathbf{X}$, the predictions $\\hat{\\mathbf{y}}$ can be expressed via the matrix-vector product:\n",
    "\n",
    "$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b.$$\n",
    "\n",
    "Given a training dataset $\\mathbf{X}$ and corresponding (known) targets $\\mathbf{y}$, the goal of linear regression is to find the weight vector $w$ and bias term $b$ that given a new data point $\\mathbf{x}_i$, sampled from the same distribution as the training data will (in expectation) predict the target $y_i$ with the lowest error.\n",
    "\n",
    "Even if we believe that the best model for predicting $y$ given $\\mathbf{x}$ is linear, we would not expect to find real-world data where $y_i$ exactly equals $\\mathbf{w}^\\top \\mathbf{x}+b$ for all points ($\\mathbf{x}, y)$. For example, whatever instruments we use to observe the features $\\mathbf{X}$ and labels $\\mathbf{y}$ might suffer small amount of measurement error. Thus, even when we are confident that the underlying relationship is linear, we will incorporate a noise term to account for such errors.\n",
    "\n",
    "Before we can go about searching for the best parameters $\\mathbf{w}$ and $b$, we will need two more things:\n",
    "+ a quality measure for some given model,\n",
    "+ a procedure for updating the model to improve its quality.\n",
    "\n",
    "##### Loss Function\n",
    "Before we start thinking about how to fit our model, we need to determine a measure of fitness. The `loss function` quantifies the distance between the real and predicted value of the target. The loss will usually be a non-negative number where smaller values are better and perfect predictions incur a loss of $0$. The most popular loss function in regression problems is the sum of squared errors. When our prediction for an example $i$ is $\\hat{y}^{(i)}$ and the corresponding true label is $y^{(i)}$, the squared error is given by:\n",
    "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$\n",
    "\n",
    "The constant $\\frac{1}{2}$ makes no real difference but will prove notationally convenient, cancelling out when we take the derivative of the loss. Since the training dataset is given to us, and thus out of our control, the empirical error is only a function of the model parameters. To make things more concrete, consider the example below where we plot a regression problem for a one-dimensional case as shown in `Fig. 3.1.1`.\n",
    "\n",
    "<img src=\"images/03_01.png\" style=\"width:300px;\"/>\n",
    "\n",
    "Note that large differences between estimates $\\hat{y}^{(i)}$ and observations $y^{(i)}$ lead to even larger contributions to the loss, due to the quadratic dependence. To measure the quality of a model on the entire dataset, we simply average (or equivalently, sum) the losses on the training set.\n",
    "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
    "\n",
    "When training the model, we want to find parameters ($\\mathbf{w}^*, b^*$) that minimize the total loss across all training examples:\n",
    "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\ L(\\mathbf{w}, b).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analytic Solution\n",
    "Linear regression happens to be an unusually simple optimization problem. Unlike most other models that we will encounter in this book, linear regression can be solved analytically by applying a simple formula, yielding a global optimum. To start, we can subsume the bias $b$ into the parameter $\\mathbf{w}$ by appending a column to the design matrix consisting of all $1s$. Then our prediction problem is to minimize $||\\mathbf{y} - \\mathbf{X}\\mathbf{w}||$. Because this expression has a quadratic form, it is convex, and so long as the problem is not degenerate (our features are linearly independent), it is strictly convex.\n",
    "\n",
    "Thus there is just one critical point on the loss surface and it corresponds to the global minimum. Taking the derivative of the loss with respect to $\\mathbf{w}$ and setting it equal to $0$ yields the analytic solution:\n",
    "\n",
    "$$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}.$$\n",
    "\n",
    "While simple problems like linear regression may admit analytic solutions, you should not get used to such good fortune. Although analytic solutions allow for nice mathematical analysis, the requirement of an analytic solution is so restrictive that it would exclude all of deep learning.\n",
    "\n",
    "##### Gradient descent\n",
    "Even in cases where we cannot solve the models analytically, and even when the loss surfaces are high-dimensional and nonconvex, it turns out that we can still train models effectively in practice. Moreover, for many tasks, these difficult-to-optimize models turn out to be so much better that figuring out how to train them ends up being well worth the trouble.\n",
    "\n",
    "The key technique for optimizing nearly any deep learning model, and which we will call upon throughout this book, consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function. This algorithm is called `gradient descent`. On convex loss surfaces, it will eventually converge to a global minimum, and while the same cannot be said for nonconvex surfaces, it will at least lead towards a (hopefully good) local minimum.\n",
    "\n",
    "The most naive application of gradient descent consists of taking the derivative of the true loss, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow. We must pass over the entire dataset before making a single update. Thus, we will often settle for sampling a random minibatch of examples every time we need to compute the update, a variant called `stochastic gradient descent`.\n",
    "\n",
    "In each iteration, we first randomly sample a minibatch $\\mathcal{B}$ consisting of a fixed number of training examples. We then compute the derivative (gradient) of the average loss on the mini batch with regard to the model parameters. Finally, we multiply the gradient by a predetermined step size $\\eta > 0$ and subtract the resulting term from the current parameter values.\n",
    "\n",
    "We can express the update mathematically as follows ($\\partial$ denotes the partial derivative) :\n",
    "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$\n",
    "\n",
    "To summarize, steps of the algorithm are the following:\n",
    "+ we initialize the values of the model parameters, typically at random; \n",
    "+ we iteratively sample random batches from the data (many times), updating the parameters in the direction of the negative gradient.\n",
    "\n",
    "For quadratic losses and linear functions, we can write this out explicitly as follows: Note that $\\mathbf{w}$ and $\\mathbf{x}$ are vectors. Here, the more elegant vector notation makes the math much more readable than expressing things in terms of coefficients, say $w_1, w_2, \\ldots, w_d$.\n",
    "$$ \\begin{aligned} \\mathbf{w} &\\leftarrow \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) && = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\ b &\\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b) && = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned} $$\n",
    "\n",
    "In the above equation, $|\\mathcal{B}|$ represents the number of examples in each minibatch (the `batch size`) and $\\eta$ denotes the `learning rate`. We emphasize that the values of the batch size and learning rate are manually pre-specified and not typically learned through model training. These parameters that are tunable but not updated in the training loop are called `hyper-parameters`. Hyperparameter tuning is the process by which these are chosen, and typically requires that we adjust the hyperparameters based on the results of the inner (training) loop as assessed on a separate validation split of the data.\n",
    "\n",
    "After training for some predetermined number of iterations (or until some other stopping criteria is met), we record the estimated model parameters, denoted $\\hat{\\mathbf{w}}, \\hat{b}$ (in general the `hat` symbol denotes estimates). Note that even if our function is truly linear and noiseless, these parameters will not be the exact minimizers of the loss because, although the algorithm converges slowly towards a local minimum it cannot achieve it exactly in a finite number of steps.\n",
    "\n",
    "Linear regression happens to be a convex learning problem, and thus there is only one (global) minimum. However, for more complicated models, like deep networks, the loss surfaces contain many minima. Fortunately, for reasons that are not yet fully understood, deep learning practitioners seldom struggle to find parameters that minimize the loss on training data. The more formidable task is to find parameters that will achieve low loss on data that we have not seen before, a challenge called generalization. We return to these topics throughout the book.\n",
    "\n",
    "##### Making Predictions with the Learned Model\n",
    "Given the learned linear regression model $\\hat{\\mathbf{w}}^\\top \\mathbf{x} + \\hat{b}$, we can now estimate the price of a new house (not contained in the training data) given its area $x_1$ and age (year) $x_2$. Estimating targets given features is commonly called `prediction` and `inference`.\n",
    "\n",
    "We will try to stick with `prediction` because calling this step `inference`, despite emerging as standard jargon in deep learning, is somewhat of a misnomer. In statistics, `inference` more often denotes estimating parameters based on a dataset. This misuse of terminology is a common source of confusion when deep learning practitioners talk to statisticians.\n",
    "\n",
    "##### Vectorization for Speed\n",
    "When training our models, we typically want to process whole minibatches of examples simultaneously. Doing this efficiently requires that we vectorize the calculations and leverage fast linear algebra libraries rather than writing costly for-loops in Python.\n",
    "\n",
    "To illustrate why this matters so much, we can consider two methods for adding vectors. To start we instantiate two $10000$-dimensional vectors containing all ones. In one method we will loop over the vectors with a Python for loop. In the other method we will rely on a single call to `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.65701 sec'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000 \n",
    "a = np.ones(n) \n",
    "b = np.ones(n)\n",
    "\n",
    "c = np.zeros(n) \n",
    "timer = Timer() \n",
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]\n",
    "'%.5f sec' % timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00063 sec'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.start() \n",
    "d = a + b \n",
    "'%.5f sec' % timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that the second method is dramatically faster than the first. Vectorizing code often yields order-of-magnitude speedups. Moreover, we push more of the math to the library and need not write as many calculations ourselves, reducing the potential for errors.\n",
    "\n",
    "### 3.1.2 The Normal Distribution and Squared Loss\n",
    "While you can already get your hands dirty using only the information above, in the following section we can more formally motivate the square loss objective via assumptions about the distribution of noise.\n",
    "\n",
    "Recall from the above that the squared loss $l(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$ has many convenient properties. These include a simple derivative $\\partial_{\\hat{y}} l(y, \\hat{y}) = (\\hat{y} - y)$.\n",
    "\n",
    "As we mentioned earlier, linear regression was invented by Gauss in 1795, who also discovered the normal distribution (also called the Gaussian). It turns out that the connection between the normal distribution and linear regression runs deeper than common parentage. To refresh your memory, the probability density of a normal distribution with mean $\\mu$ and variance $\\sigma^2$ is given as follows:\n",
    "$$p(z) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (z - \\mu)^2\\right).$$\n",
    "\n",
    "Below we define a Python function to compute the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(z, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * sigma**2) \n",
    "    return p * np.exp(- 0.5 / sigma**2 * (z - mu)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAE9CAYAAADDIoJJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeXzV1Z3/8de5N/u+kbBkBQKRQCABAgHBICq4Ye2iWPQno451HIZ2WrXMWDtqR4dxaceqY+tYK1apWrXiirIFUIgQEkggYQkQQtiz7+s9vz+yGELIeu/93tx8no8HD3Pv/S7vnNzkfjzf8z1Haa0RQgghhBgqTEYHEEIIIYToDylehBBCCDGkSPEihBBCiCFFihchhBBCDClSvAghhBBiSJHiRQghhBBDiovRAawlJCRER0dHGx3DZmpqavD29jY6hqGkDaQN2kk7SBuAtEE7Z26HPXv2FGutR3R93mmKl+joaDIyMoyOYTNpaWmkpqYaHcNQ0gbSBu2kHaQNQNqgnTO3g1LqRHfPy2UjIYQQQgwpUrwIIYQQYkiR4kUIIYQQQ4rTjHkRQgghutPU1ERRURH19fVGR7EJf39/8vLyjI4xKB4eHoSHh+Pq6tqn7aV4EUII4dSKiorw9fUlOjoapZTRcayuqqoKX19fo2MMmNaakpISioqKiImJ6dM+ctlICCGEU6uvryc4ONgpCxdnoJQiODi4Xz1jNi1elFKLlVKHlFL5SqlVPWz3A6WUVkrN6PTcv7Xtd0gptciWOYUQQjg3KVwcW39/PjYrXpRSZuBl4HpgEnCHUmpSN9v5Aj8Fvu303CRgKRAPLAb+t+14QgghhOiH0tJSrr32WmJjY7n22mspKyuz6/kfffRRIiIi8PHxsdoxbdnzkgzka62Paa0bgXeAW7rZ7jfAfwOd+4tuAd7RWjdorY8D+W3HE0IIIUQ/rF69moULF3LkyBEWLlzI6tWrbXau5ubmS567+eab2bVrl1XPY8viZQxwstPjorbnOiilkoAIrfVn/d1XCDG8tFg0GQWlvL+niD3nmimraTQ60rDU1NJEWkEaa/au4dPDn1LVUGV0JIdXUFBAXFwcy5cvZ8KECSxbtoyNGzcyd+5cYmNjOz7Ya2pquOeee0hOTiYxMZF169Z17D9v3jySkpJISkpix44dwHcz6951113ExcWxbNkytNaXnH/dunXcfffdANx999189NFHPeZdunQpn3323cfy8uXLef/993vMMW/ePJYsWcKkSZdcYGH27NmMGjVqAC13eYbdbaSUMgG/BZYP4hj3A/cDhIWFkZaWZpVsjqi6utqpv7++kDYYvm3w7Zlm3jvUSEn9d3+Y/3fvBuaNceGHE9zwcRt+4xns/V7QWvPJmU94o+ANypq+u+zwatKrxPrG2i1HZ31tA39/f6qqjCuyqquryc/P54033uCFF14gNTWVNWvW8MUXX/D555/z5JNP8te//pUnnniClJQUXnjhBcrLy1mwYAGzZs3C09OTDz/8EA8PD/Lz87n33nvZunUrtbW1ZGVlsWPHDsLDw7n22mvZsGEDKSkpF53/3Llz+Pj4UFVVhbe3N+fOneuxPZYsWcLbb7/N/PnzaWxsZOPGjTzzzDNorS+bIzMzk/T0dKKjo3s8dk+v1dfX9/k9bcvi5RQQ0elxeNtz7XyByUBa20CdkcDHSqklfdgXAK31q8CrADNmzNDOurYDOPfaFX0lbTD82qC5xcJ/fpbHG/sKmDzGj8fnj2PyGH82bk/npCmMtd8WcqwWXl8+g/Gh1ruePhTY871Q31zPvR/fy9oja7kq6ir+dfa/Eh8az9nqs8yNmItSiv/d/b/cMvEWxvjZr5O8r22Ql5fXcSvxE58cIPd0pVVzTBrtx3/cHH/Z1318fIiJiWH27NkATJkyhUWLFuHn50dycjKrV6/G19eXtLQ01q9fz8svvwxAY2MjZWVljB49mhUrVrB3717MZjOHDx/G19cXLy8vkpOTiYyMxNfXl+nTp3P+/Plub5vu/JxSqsdbq7///e+zatUq3Nzc2Lx5M1dddRWhoaFUVFT0mGPKlCm9tlVP5/Xw8CAxMbHXY4Bti5fdQKxSKobWwmMp8OP2F7XWFUBI+2OlVBrwkNY6QylVB6xVSv0WGA3EAta9YCaEcGhaax5bd4C/7irk3itj+PcbrsBsau1hiQ0084+pk/le4hjufzODZa+l87efzCEy2Mvg1M6nxdLCHR/cwUcHP+Lpq59m1ZWrOu4MGR80HoDi2mL+bdO/cazsGM9d95yRcR2Wu7t7x9cmk6njsclk6hgnorXmgw8+YOLEiRft+/jjjxMWFsa+ffuwWCx4eHh0e1yz2dztmJOwsDDOnDnDqFGjOHPmDKGhoT1m9fDwIDU1lS+//JJ3332XpUuXAvC73/3usjnsvaq1zYoXrXWzUmoF8CVgBl7XWh9QSj0JZGitP+5h3wNKqfeAXKAZ+GetdYutsgohHM+r247x112FPJg6jkcWx3W7TVJkIG/dN4ulr6Zz75rdrFsxFy83mXvTmh7e8DAfHfyI3y/+Pf8y61+63SbEK4Qd9+xgYsjEbl93JD31kBht0aJFvPjii7z44osopcjKyiIxMZGKigrCw8MxmUysWbOGlpb+fRwuWbKENWvWsGrVKtasWcMtt7TeO7Nr1y5eeukl3nzzzUv2uf3223nttdfIyMjgjTfeABh0Dmuy6TwvWuvPtdYTtNbjtNZPtT336+4KF611qtY6o9Pjp9r2m6i1/sKWOYUQjiW7qJxnvzzEDVNG8vCinj8Q40b68dIdSeRfqOY/1h2wU8LhoaS2hLey32LFzBWXLVzaxYfG42JyoaiyiNezXrdTQufy2GOP0dTUREJCAvHx8Tz22GMAPPjgg6xZs4apU6dy8ODBfvdyrFq1ig0bNhAbG8vGjRtZtap12rXCwkI8PT273ee6665j69atXHPNNbi5uQ0qxyOPPEJ4eDi1tbWEh4fz+OOP9yt/t7TWTvFv+vTp2plt2bLF6AiGkzYYHm3Q0NSir35ui5799EZdXtPY7TbdtcMz6/N01C8/1WmHzts4oWOw13vhTNUZXddU1+ftV36+UpueMOnM05k2TNWqr22Qm5tr2yAGq6ysHNB+Dz30kN63b5+V0wxcdz8nWq/UXPKZL8sDCCEcyps7Czh6oYanb52Cv1ffFmkDWLkwlrEh3vx63X7qm+Qq82ClF6XTYmlhpM9IPFw8et+hzRMLniDEK4SffPoTWizyc3Bkzz77LAkJCUbHGBApXoQQDqO4uoEXNh5hwcQRLIjreVBhV+4uZn7zvcmcKKllzY4C2wQcJk6Un2D+n+fz5NYn+71vgEcAz1/3PLtP7+av+/9qg3RCSPEihHAgL246Ql1TC7+66dKJrvpi7vgQ5k8YwR+2HqWqvsnK6YaPSP9I3v7+2zw488EB7f/jKT9mathUntj6BE0t8nMQ1ifFixDCIZyvquevu0/yg6Rwxo0Y+Jwtv7h2AmW1Tfz5mwLrhRtmlFL8KP5HhPmEDWh/kzLxmwW/Ib80nzf3XXonixCDJcWLEMIh/Gn7cZpbLPxT6rhBHWdqRADXTQrj/7Yfo7rh0jkvRM9WfL6C//76vwd9nJsm3MSsMbP4z+3/SbNFfg7CuqR4EUIYrry2kb+kn+DmqaOJDhn8ZFcPpI6jqr6ZD/YUWSHd8HGs7BivZLzChdoLgz6WUopfzv0lBeUFrDu4zgrphPiOFC9CCMO9v6eI2sYWfjJ/cL0u7ZIiA0mMDODP3xzHYrl0oTrRvVd2v4JC8a+z/9Uqx1sycQljA8fywrcvWOV4YmBKS0u59tpriY2N5dprr6WsrKz3nayktraWG2+8kbi4OOLj4zvmmBksKV6EEIayWDRvpZ9gelQgk0b7We24914ZQ0FJLZsOnrfaMZ1ZXVMdr+99nVuvuNVq6xOZTWZeX/I6f77lz1Y5nhiY1atXs3DhQo4cOcLChQtZvXq1zc7V3fIEDz30EAcPHiQrK4tvvvmGL74Y/LyzUrwIIQz1zdFiCkpquWt2lFWPuzh+JKP8PXgr/YRVj+us/pb7N0rrSvmnGf9k1eNeFX0V44Ks06M2VBUUFBAXF8fy5cuZMGECy5YtY+PGjcydO5fY2Fh27Wpduq+mpoZ77rmH5ORkEhMTWbduXcf+8+bNIykpiaSkJHbs2AF8tzDlXXfdRVxcHMuWLaN1XreLrVu3jrvvvhuAu+++m48++qjHvEuXLuWzzz7reLx8+XLef//9HnPMmzePJUuWMGnSxXcKenl5sWDBAgDc3NxISkqiqGjwl3OleBFCGOqt9BMEebtx/ZSRVj2ui9nEj6aHs+3IBU6X11n12M7olYxXmBg8kQXRC6x+7L1n97L0/aVUNVRZ/dhDRX5+Pr/4xS84ePAgBw8eZO3atXz99dc899xzPP300wA89dRTXH311ezatYstW7bw8MMPU1NTQ2hoKBs2bCAzM5N3332XlStXdhw3KyuL1atXk5uby7Fjx/jmm28uOfe5c+cYNWoUACNHjuTcuXM9Zr399tt57733gNaVrTdt2sSNN97YY47MzExeeOEFDh8+fNnjlpeX88knn7Bw4cK+N9xlyApmQgjDnKusZ2Peee6bF4O7i9nqx//RjAh+vzmf9/cUsXJhrNWP7yz2nd1HelE6/7PofzpWjLam+uZ6Nh/fTO6FXGaFz7L68fsrKzXrkudCbwtlzINjaKltIfuG7EteH7l8JKOWj6KxuJEDP7x4Da3EtMRezxkTE8OUKVMAiI+PZ+HChSilmDJlCgUFBQB89dVXfPzxxzz3XOvK3PX19RQWFjJ69GhWrFjB3r17MZvNFxUIycnJjBkzBpPJxLRp0ygoKODKK6+8bA6lVK8/4+uvv56f/vSnNDQ0sH79eubPn4+npycVFRU95oiJibnsMZubm7njjjtYuXIlY8eO7bW9eiPFixDCMJ/sO02LRXPbjAibHD8iyIu544N5L+MkKxaMx2Sy/gezM3gr+y1cTC7cmXCnTY4/a8wsin5ehJvZzSbHHwrc3d07vjaZTB2PTSZTxzgRrTUffPABEydevBjp448/TlhYGPv27cNiseDh4dHtcc1mc7djTsLCwjhz5gyjRo3izJkzhIb2PHu1h4cHqampfPnll7z77rssXboUgN/97neXzdHbIo33338/sbGx/OxnP+txu76S4kUIYZi/Z50iIdx/UJPS9ea2GRH89J29fHu8lJRxwTY7z1DVYmlh7f613BB7A8FetmkfpRRuZjcs2kJDcwOert2vZGwvPfWUmL3MPb7uFuLWp56WgVi0aBEvvvgiL774IkopsrKySExMpKKigvDwcEwmE2vWrKGlpX9rRi1ZsoQ1a9awatUq1qxZwy233ALArl27eOmll3jzzUsnErz99tt57bXXyMjI4I033gAYcI5f/epXVFRU8Nprr/Urd09kzIsQwhBHzlVx4HQl35tmnTtbLufaSWF4upr5JPu0Tc8zVJmUiQ9v+5Bfz/+1Tc9T11THuN+P45lvnrHpeYayxx57jKamJhISEoiPj+exxx4D4MEHH2TNmjVMnTqVgwcP9trL0dWqVavYsGEDsbGxbNy4seN25cLCQjw9uy8kr7vuOrZu3co111yDm5vbgHMUFRXx1FNPkZubS1JSEtOmTbNOEdPdUtND8d/06dMHvg73ENDXpd+dmbSBc7XBM+vz9Nh/+0yfr6zv9779bYcVazP1tCe+1I3NLf0+l6Maiu+F1DdSddxLcdpisVjleH1tg9zcXKucz1FVVlYOaL+HHnpI79u3z8ppBq67nxOQobv5zJeeFyGE3Vksmo+yTnPl+BBG+Lr3vsMg3ZwwirLaJr7JL7b5uYaSuqY6Vny+grwLeXY539L4pRwsPkjO+Ry7nE/07NlnnyUhIcHoGAMixYsQwu72FZVzqryOJVNH2+V8V00cga+HC5/sO2OX8w0VOedz+PPeP1NUaZ9lFH4w6QeYlZl39r9jl/MJ5yXFixDC7r48cA4Xk+KaKwa2anF/ubuYWRQ/kq8OnKW+qX+DHZ1Z8phkLjx8gQUx1p/bpTshXiFcM/Ya3tn/TreTqQnRV1K8CCHsSmvNVwfOMntsMP5ernY7740Jo6hqaGbHUbl01JmXqxcuJvvdeHpb/G0cLz/O3rN77XZO4XykeBFC2FX++WqOFdewKN4+vS7t5owLxtvNzIZcWesIYPPxzST+MZFDxYfset6bJtyEQrHukKw0LQZOihchhF19eeAsANdOsu5yAL1xdzFz1cQRbMo7JytNAx/mfcih4kNE+NtmgsDLCfUOZU7EHD4+9LFdzyuci02LF6XUYqXUIaVUvlLqknWwlVIPKKVylFJ7lVJfK6UmtT0frZSqa3t+r1LqD7bMKYSwny8PnGNaRAAj/T1639jKrrkijPNVDWSfqrD7uR2JRVv46OBHLBq/CC9XL7uff8nEJWSdzaKwotDu5x6OHnvsMRISEpg2bRrXXXcdp0/bd86jRx99lIiICHx8rDcZpc2KF6WUGXgZuB6YBNzRXpx0slZrPUVrPQ14Bvhtp9eOaq2ntf17wFY5hRD2c6q8jpxTFSyKt2+vS7sFE0MxmxQbc3temM7ZZZzO4FTVKW6Nu9WQ89+ZcCdf/8PXjPG17QSFotXDDz9MdnY2e/fu5aabbuLJJ5+02bm6W57g5ptv7lg521ps2fOSDORrrY9prRuBd4BbOm+gta7s9NAbkL5cIZzYprzWouE6O493aRfo7caMqEA2DPPiZd3BdZiVmZsm3GTI+Uf7jmZu5FzMJusvxumICgoKiIuLY/ny5UyYMIFly5axceNG5s6dS2xsbMcHe01NDffccw/JyckkJiaybt26jv3nzZtHUlISSUlJ7NixA4C0tDRSU1O56667iIuLY9myZd3exeXn59fxdU1NTa8LMy5dupTPPvus4/Hy5ct5//33e8wxb948lixZwqRJXfsoYPbs2R2rWluLLYuXMcDJTo+L2p67iFLqn5VSR2nteVnZ6aUYpVSWUmqrUmqeDXMKIexk66ELRAV72XQto95cOymMQ+eqKCypNSyD0dYfXU9KRApBnkGGZThUfIiffvFTKuqHxyW8/Px8fvGLX3Dw4EEOHjzI2rVr+frrr3nuued4+umnAXjqqae4+uqr2bVrF1u2bOHhhx+mpqaG0NBQNmzYQGZmJu+++y4rV373UZmVlcXq1avJzc3l2LFjfPPNN92ev/3Szdtvv91rz8vtt9/Oe++9B0BjYyObNm3ixhtv7DFHZmYmL7zwwkUrTduS4Qszaq1fBl5WSv0Y+BVwN3AGiNRalyilpgMfKaXiu/TUoJS6H7gfWlfNTEtLs294O6qurnbq768vpA2Gdhs0tmi2H6ll3hiXQX8Pg2kHv1oLAH/45Buui7bfrdrWNtA2KG0sJfNMJvdG32voeymnIoc/ZP+BmIYYpgVMG9Ax+toG/v7+VFVVdTy+4b0bet1n8djFrJyxsmP7ZfHLWBa/jJK6Eu765K6Ltv38ts97zRkVFUV0dDQ1NTVMmDCBOXPmUF1dTUxMDMeOHaOqqor169fz0Ucf8cwzres/1dXVkZeXx8iRI3nooYfIycnBbDaTn59PVVUVtbW1TJ8+nZEjR1JTU0N8fDx5eXlMnTr1kgyrVq1i1apVPP/88zz//PM8+uijl8175ZVXsnLlSoqLi9m4cSMpKSk0NzdTUVHRY46QkJCL2rk7Pb1eX1/f5/ekLYuXU0DnYezhbc9dzjvAKwBa6wagoe3rPW09MxOAjM47aK1fBV4FmDFjhk5NTbVWdofT3j04nEkbDO02+PpIMY0t37Ls6mmkxg3ustFg2+GV3DROay9SU5MHlcNIA22DN/e1riD84HUPkjQqycqp+m6eZR733XDfoFaY7msb5OXl4evr2/HYbO79cpW7u3vHPmazGQ8PD3x9fWkwN1yyf+djd8fHxwdPT8+O7dzd3QkICMDX1xc/Pz8sFgu+vr4opfj73//OxIkTL9r/8ccfJzw8nLVr12KxWDqyeHl54eXlhdlsxtfXFw8PD1xdXXvMc88993DDDTewevXqy27j6+vLggUL2LFjBx9//DF33nknvr6+PP/885fN4efn12s79NZWHh4eJCb2bcVuWxYvu4FYpVQMrUXLUuDHnTdQSsVqrY+0PbwRONL2/AigVGvdopQaC8QCx2yYVQhhY2mHzuPmYmL22GCjozA/NoR3M05S39SCh+vwGHfRbn3+ekK9Q5k2cmC9HdZiNpnxNA28cBmMtOVpA94+xCuk3/v31aJFi3jxxRd58cUXUUqRlZVFYmIiFRUVhIeHYzKZWLNmDS0t/Zsl+siRI8TGxgKwbt064uLiANi1axcvvfQSb7755iX73H777bz22mtkZGTwxhtvAAw6hzXZbMyL1roZWAF8CeQB72mtDyilnlRKLWnbbIVS6oBSai/wc1ovGQHMB7Lbnn8feEBrXWqrrEII29t6+AKzYoLwcjP8ajXzJ4ygvslCRkGZ0VHs7q6Eu/ivhf+FSRk/zdee03uY+X8zOXD+gNFRHMJjjz1GU1MTCQkJxMfH89hjjwHw4IMPsmbNGqZOncrBgwfx9vbu13FXrVrF5MmTSUhI4KuvvuKFF14AoLCwEE/P7gvI6667jq1bt3LNNdfg5uY2qByPPPII4eHh1NbWEh4ezuOPP96v/N3qbqnpofhv+vTpfVtze4jq69LvzkzaYOi2QVFZrY765af6/7YdtcrxBtsO1fVNevy/f6af+izXKnmMMFTfC50VlhdqHkc/v+P5Ae3f1zbIzR26P+e+qKysHNB+Dz30kN63b5+V0wxcdz8nIEN385lvfOkthHB6Ww9dACB14giDk7TydndhZnQQ2w5fMDqKXX1d+DX7zu4zOkaHCP8Irgi5gi+Pfml0lGHp2WefJSEhwegYAyLFixDC5rYePs+YAE9Db5Huav6EERw8W8W5ynqjo9jNQ189xAOfOdacn4vGLWLbiW3UNdUZHUUMIVK8CCFsqrnFwjf5JcyfMKLXybHsaX5say/QcOp9+fvtf+cPNzrWaiuLxi+ivrmerSe2Gh1FDCFSvAghbCr7VAXVDc3Miw0xOspFrhjlywhfd7YOo+JllO8opo68dA4QI82Pmo+72Z0v82176Uh3M/OscBz9/flI8SKEsKmdR0sAHOIW6c6UUlw5PoT0YyXD4oPtT5l/4rXM14yOcQkvVy+ujLySzQWbbXYODw8PSkqGx895KNJaU1JSgodH3xdrNf6eRSGEU9txtJgrRvkR5O1mdJRLpIwN5u9ZpzhyvpoJYb1PsDWU/Tb9t0T6R3Jf0n1GR7nEgugF/GrLryiuLSbEy/o9dOHh4RQVFXHhgnP2stXX1/frg98ReXh4EB4e3uftpXgRQthMQ3MLGQVlLJsVZXSUbqWMa+0N2nm0xKmLl7PVZ8m9kMvyqcuNjtKtBTELYAukFaTxw0k/tPrxXV1diYmJsfpxHUVaWlqfZ6Z1FnLZSAhhM1mF5TQ0W5gzzrEuGbWLCPIiPNCTHUeLjY5iU1uObwHg6pirDU7SvZmjZ3J/0v1E+kcaHUUMEdLzIoSwmR1HSzApSB5r3OrFvUkZG8xXueewWDQmk+PcDWVNm49vJsAjwPAlAS7H1ezKH2/+o9ExxBAiPS9CCJvZebSYKeEB+Hk47urNc8YHU1HXRO6Zyt43HqI2F2zmqqirMJscdx0nrTUHzh+gor7C6ChiCJDiRQhhE7WNzWQVlpPiYHcZdZUytnWAaPqxEoOT2MaJ8hMcKzvmsJeM2mWeyWTyK5P5/MjnRkcRQ4AUL0IIm9hdUEazRTvseJd2I/09GBvizY6jzlm8bClw7PEu7aaNnMbrS14nNTrV6ChiCJAxL0IIm9hxtBhXs2JGdKDRUXo1e1wwH+89TXOLBRezc/0/3ebjmxnhNYL4EfFGR+mR2WTmHxL/wegYYohwrt9SIYTDSD9aQmJEIF5ujv//SHPGBVPd0EzOKecbb2FSJm6ecLNDLc1wOcW1xby651XOVJ0xOopwcFK8CCGsrr0QmOXAdxl1Nium9dLWruOlBiexvje+9wZ/uuVPRsfok1OVp/jJpz9h0/FNRkcRDk6KFyGE1WWeKMOiYWb00CheRvi6MzbEm90FzlW8DLXp8CeHTsbf3Z/tJ7YbHUU4OClehBBWt7ugFJOCpCjHH+/SbmZ0ELsLyrBYhtYHfk8e+PQBFr650OgYfWY2mZkTMYfthVK8iJ5J8SKEsLpdx0uJH+2Pj7vjj3dpNyM6kIq6Jo6crzY6itUkjkpkbsRco2P0y7zIeeQV51Fc69yzHovBkeJFCGFVjc0W9p4sHzKXjNolx7Tm3eVEl44emPEATy540ugY/TIvah4AXxd+bXAS4cikeBFCWFXOqQoami0kxwydS0YAkUFehPq6s9tJBu2erzlPZcPQmzV45uiZuJvdZdyL6JEUL0IIq2of9Do9amj1vCilmBkTxO6C0iE30LU7/7X9vxjz2zE0W5qNjtIv7i7uJI9JlnEvokdSvAghrGr38VLGhngzwtfd6Cj9lhwdxJmKeorK6oyOMmjbC7czfdR0XExDZ9xRu3mR88g8k0l1o/OMPxLWZdPiRSm1WCl1SCmVr5Ra1c3rDyilcpRSe5VSXyulJnV67d/a9juklFpky5xCCOuwWDQZJ8qG3HiXdu25h/ot01UNVWSdzWJe5DyjowzIvKh5uJhcyLuQZ3QU4aBsVrwopczAy8D1wCTgjs7FSZu1WuspWutpwDPAb9v2nQQsBeKBxcD/th1PCOHAjpyvpqKuiZkxQ7N4mTjSF18PlyFfvKQXpWPRlo7Br0PNwpiFVKyqYOaYmUZHEQ7Klj0vyUC+1vqY1roReAe4pfMGWuvOo8m8gfYLzbcA72itG7TWx4H8tuMJIRxY+506M4fAekbdMZsUM6ICh/xMu9sLt2NSJmaHzzY6yoC4ml1xdxl6lx2F/diyeBkDnOz0uKjtuYsopf5ZKXWU1p6Xlf3ZVwjhWDIKSgn1dScyyMvoKAM2MyaIoxdqKKluMDrKgG0v3M60kdPwcwKVhkAAACAASURBVPczOsqAfXzoY1LfSKWppcnoKMIBGT6SS2v9MvCyUurHwK+Au/u6r1LqfuB+gLCwMNLS0myS0RFUV1c79ffXF9IGjt8G2w/WMj7AxNatW216Hlu2g2tZCwBvfLad6WGG/4m8rMu1QZOliZ2FO7lp1E0O/V7pTXZxNsVlxXy08SNGuI/odhtH/32wl+HYDrb8zTwFRHR6HN723OW8A7zSn3211q8CrwLMmDFDp6amDiKuY0tLS8OZv7++kDZw7DYoKquldP0WbkyOI3VOtE3PZct2SGlu4dk9X1HrPZrU1K7D9BzH5dogvSidhu0N3DHnDlInXfr6UJFKKr/iVz1u48i/D/Y0HNvBlpeNdgOxSqkYpZQbrQNwP+68gVIqttPDG4EjbV9/DCxVSrkrpWKAWGCXDbMKIQZpd8d4l6E5WLedu4uZqeH+ZBaWGR1lQNond7sy8kqDk1hHi6XF6AjCAdmseNFaNwMrgC+BPOA9rfUBpdSTSqklbZutUEodUErtBX5O2yUjrfUB4D0gF1gP/LPWWt7BQjiwjIIyfN1dmDjS1+gog5YUGciBU5XUNw29PzvLEpbx4W0fEuYTZnSUQXsi7QnG/X6cU0waKKzLphd0tdafA593ee7Xnb7+aQ/7PgU8Zbt0QghryiwsZ1pkAGaTMjrKoCVFBfLHbcc4cLpiyM0UPNp3NLdecavRMawi1DuUExUnKCgvICYwxug4woHIDLtCiEGraWjm0NlKEiMCjI5iFUmRrbd67zkxtC4dnak6w8u7XuZc9Tmjo1hFSkQKADtO7jA4iXA0UrwIIQZtX1E5Fg2JUUNzfpeuRrTd7p15otzoKP2yvXA7K75YwZnqM0ZHsYrJoZPxcfNhZ9FOo6MIByPFixBi0LIKWz/kkyKco3gBSIoMYE9h2ZAab/GjST+i4KcFTAmdYnQUq3AxuZA8Jll6XsQlpHgRQgxa5okyxo3wxt/L1egoVjM9KpALVQ1DapFGpRRRAVGYTc6zmsqc8Dlkn8umprHG6CjCgUjxIoQYFK01WSfLO8aJOIvEtu9nqNwyXdtUy50f3kl6UbrRUawqJSKFFt3C7tO7jY4iHIgUL0KIQSkoqaW0ppEkJxnv0i5upC9ebmYyh8ig3YzTGbyd8zbFtcVGR7Gq9vWZ5NKR6EyKFyHEoLR/uDtbz4uL2cTU8NZxL0PBzpOtg1qH6mKMlxPkGURcSJwM2hUXcdyFO4QQQ0JmYevkdLGhPkZHsbrpUYG8svUotY3NeLk59p/LnUU7iQ2KJcQrxOgoVvfInEdklWlxEcf+bRRCOLz2yelMTjA5XVdJUQG0WDT7TlaQMi7Y6DiXpbVmZ9FOFo9fbHQUm/iHxH8wOoJwMHLZSAgxYNXtk9M52SWjdokRQ2PQ7vHy45yvOU9KeIrRUWxCa82h4kPkl+YbHUU4CClehBADlt0+OV2kc8ys21WgtxtjR3g7/KDd9vEuTlu8oJn9p9k8+82zRkcRDkIuGwkhBswZJ6franpkIBvzzqG1RinHvDS2s2gn3q7eTA6dbHQUmzApE2u/v5bxQeONjiIchPS8CCEGzBknp+sqKSqQstomjhc77iRpO4t2kjwm2akmp+vq+tjriQ2ONTqGcBBSvAghBsRZJ6franpU+7gXx1znyKItBHkGsTBmodFRbKq6sZo39r7B/vP7jY4iHIAUL0KIAXHWyem6Gj/CB18PF4ddYdqkTGy4awOPzn/U6Cg2ZdEW7ll3Dx/kfmB0FOEApHgRQgyIs05O15XJpEiMDCTLQe84GkoLRw6Gn7sfU8KmyGR1ApDiRQgxQM48OV1XSZEBHDpXRVV9k9FRLnHb+7fxo7/9yOgYdpESnkJ6UToWbTE6ijCYFC9CiAFx5snpukqMDERr2Heywugol0gencyMUTOMjmEXs8NnU9FQwcHig0ZHEQaT4kUI0W/OPjldV9MiWuexccRLRw/PfZhfXvlLo2PYRfs8Ns62crboPylehBD9ln2ydXK6JCednK4rf09XYkN9HG6m3Qs1F6hpdNxbuK0tNjiWQI/Ajkn5xPAlxYsQot/aP8QTnXhyuq4SIwPIOlnuUANkn9j6BOG/Cx82Y0BMysTs8NkyaFdI8SKE6L+swnKnn5yuq6TIQMprmzjmQJPV7SzaSdKoJExq+PwpTwlPIfdCLhX1jjf+SNiPTd/xSqnFSqlDSql8pdSqbl7/uVIqVymVrZTapJSK6vRai1Jqb9u/j22ZUwjRd8Nlcrqu2uezyXKQyerqWurYd3Yfs8fMNjqKXc0On43ZZCb3Qq7RUYSBbFa8KKXMwMvA9cAk4A6l1KQum2UBM7TWCcD7wDOdXqvTWk9r+7fEVjmFEP0zXCan62r8CB983V0cZtzL4arDtOgWUiKcczHGy0mNTqViVcWw+77FxWzZ85IM5Gutj2mtG4F3gFs6b6C13qK1rm17mA6E2zCPEMIKhsvkdF2ZTIppkQEOs8J0bmVrz8Ps8OHV8+JqdsXL1cvoGMJgtixexgAnOz0uanvucu4Fvuj02EMplaGUSldKfc8WAYUQ/Zd1cvhMTtdVYmQgh89VUd3QbHQUcitziQ2KJcQrxOgodvdh3ofcuPbGYTNQWVzKxegAAEqpO4EZwFWdno7SWp9SSo0FNiulcrTWR7vsdz9wP0BYWBhpaWn2imx31dXVTv399YW0gWO0wfbcOiJ9YNu2rYZlMKodXMqbsWh489OtTAo2bgVnrTUHKg4wI3iG4e8HI2Scy+DY2WOc8TozLL//rhzh74K92bJ4OQVEdHoc3vbcRZRS1wCPAldprRvan9dan2r77zGlVBqQCFxUvGitXwVeBZgxY4ZOTU217nfgQNLS0nDm768vpA2Mb4PaxmaKvvqKB1PHkZo60bAcRrVDYm0Tv93zFZbASFJTY+1+/nYF5QWUbSvje9O/R+rMVMNyGCWVVJ7macN/HxzFcGwHW1422g3EKqVilFJuwFLgoruGlFKJwB+BJVrr852eD1RKubd9HQLMBWRouRAGyymqoMWiSRwmk9N15e/lyrgR3mQafMdR+wyzw228S1eONOeOsC+bFS9a62ZgBfAlkAe8p7U+oJR6UinVfvfQs4AP8Lcut0RfAWQopfYBW4DVWmspXoQwWNbJ1g/tacNocrquktpWmDbyg/OqqKv45cRfMiV0imEZjPbIhkf456x/NjqGMIhNx7xorT8HPu/y3K87fX3NZfbbAQzf30ohHFTmiTKig70I8nYzOophkqIC+dueIgpKaokJ8TYkwyjfUSweuRhX8/CZJLArb1dvDlYdpLKhEj93P6PjCDsbPtMyCiEGpX1yuuGyGOPltF8yM+qW6YbmBl7d8yoXGi4Ycn5HkRKRgkaz69Quo6MIA0jxIoTok1PldVyoahg2izFeTmyoLz4GTla39+xefvLpTzhYddCQ8zuKWWNmoVCySOMwJcWLEKJP2qfFH+49L2aTYlpEgGHLBCSPSebIvxxhesB0Q87vKPw9/In0iiT9VLrRUYQBpHgRQvRJVmE5Hq4mJo70NTqK4ZIiAzh4tpIaAyarU0oxPmg8Xi4yy2y8XzzpRely19EwJMWLEKJPsk6WkTAmAFez/NlIjAzEomFfkf17X37+5c/ZeGyj3c/riCb5TaK0rpTDJYeNjiLsTP4KCSF61dDcwoFTlcN2fpeu2tvB3peOzlSd4XfpvyPnXI5dz+uo4v3iAdhZJONehhspXoQQvco9XUlji0WKlzYBXm6MHeFNlp0H7crkdBeL9IrE391fBu0OQ1K8CCF6lSmDdS+RGBFIZmG5XcdbpBel42pyJXFUot3O6chMysS/z/t3rhnb7ZRhwok5xMKMQgjHllVYxmh/D8L8PIyO4jCSogL4ILOIEyW1RNtpsrr0U+kkjUrCw0V+Du0emfuI0RGEAaTnRQjRq6xCmZyuq6S29sg6aZ9LR82WZnaf2i2XjLrQWnO87Dhnq88aHUXYkRQvQogena+s51R5nYx36WJCmC/ebmYyT9hn0G7OuRzqmuukeOmioqGCsb8fy58y/2R0FGFHctlICNGj9sUYpeflYmaTYmpEgN1m2pXBut0L8AjgrVvfYlb4LKOjCDuSnhchRI+yCstxNSviR8vid10lRQZy8GwVtY22n6wu/VQ6Yd5hRPlH2fxcQ82yhGWMDxpvdAxhR1K8CCF6lFVYxqTR/ni4mo2O4nCSogJosWiyiypsfq5mSzOp0akopWx+rqGmvL6ct7Pf5nTVaaOjCDuR4kUIcVnNLRayiypIjJDxLt2ZFtF6Kc0el47e/v7b/PUHf7X5eYaiosoi7vz7nTLz8DAixYsQ4rIOnq2irqlFButeRpC3GzEh3nYbtCu9Lt2bNGISfu5+MlndMCLFixDistoH6ybJYN3LSowMYO/JMptOVvfUtqeY9+d5tFhabHaOocykTMwaM0uWCRhGpHgRQlxWVmEZIT5uhAd6Gh3FYSVFBlJc3cjJ0jqbnSPMJ4zYoFjMJhl3dDkp4SnknM+hurHa6CjCDqR4EUJc1t62yenkcsXltV9Ss+W4l/uS7uP1W1632fGdwezw2Vi0hd2ndhsdRdhBn4oXpZRJKZWolLpRKXW1UirU1sGEEMYqq2nkWHGNjHfpxcQwX7zczDYrXuqa6mhobrDJsZ1J+/w3culoeOixeFFKjVNKvQrkA6uBO4AHgY1KqXSl1D8opaT3RggntLeobXK6CBnv0hMXs4mp4QFkFdpm0O7anLX4rfbjRPkJmxzfWQR6BhIXEtcxmZ9wbr0VHv8JvAWM01ov0lrfqbX+odY6AbgF8AfusnVIIYT9ZRWWY1KQEO5vdBSHlxgZQN6ZSuoarT+gNr0oHW9XbyL8I6x+bGeTEp7CzqKddl3pWxijx+JFa32H1nqb7uadoLU+p7X+H631msvtr5RarJQ6pJTKV0qt6ub1nyulcpVS2UqpTUqpqE6v3a2UOtL27+7+fmNCiMHJKixj4kg/vN1lFZHeJEUG0mzRZBdZv/cl/VQ6s8JnYZJO7l7NDp9NaV0pJytPGh1F2Fhfx7wcVUo90OW5T3vZxwy8DFwPTALuUEpN6rJZFjCjrSfnfeCZtn2DgP8AZgHJwH8opaTvWgg7abHotsG6Mt6lL74btGvd4qWivoID5w8we4ysZ9QXP57yY8p/WU6kf6TRUYSN9bWUbwIWKKX+rJRya3tuTC/7JAP5WutjWutG4B1aLzV10Fpv0VrXtj1MB8Lbvl4EbNBal2qty4ANwOI+ZhVCDNKR81VUNTQzXeZ36ZNgH3eig73IsvKg3W9PfYtGMydijlWP66x83Hzwdfc1Ooawg772B9dqrW9XSj0CbFdK/Qjo7aLiGKBz310RrT0pl3Mv8EUP+/ZWLAkhrCSjoPVDeEa0McVLw+kGqrOqqT1Si1uoG2E/DgMga14WTSVN0Ah7gvbgEuBC8E3BhK9s/f+e2vxaPKI9MLnY/xJLYmQg248Uo7W22q3lO07uwKRMspJ0P7y57012ntzJKze9YnQUYUN9LV4UgNb6GaVUJvAVEGStEEqpO4EZwFX93O9+4H6AsLAw0tLSrBXJ4VRXVzv199cX0gb2a4PPsxvwc4Nj2bs4bs85Xl4CvgHOdnpuFuSNzmv92hcwQ3N9M1WWKjgJZbvKyE/Lh0bgRlr7k68AZrb9G49dZrTyaWiiuLqR97/Ywggv65zw032fMtZ7LHt27rnkNfl96L4NthRsYWfpTjZt2YRZDY9J/Ybje6Gvxcuv27/QWm9USi0CehtEewroPDw+vO25iyilrgEeBa7SWjd02je1y75pXffVWr8KvAowY8YMnZqa2nUTp5GWloYzf399IW1gvzb4j91bSIkNZsGCGTY7R0tNC+ffO0/F1xXE/SkOgEN/PUTT3Cb85/rjO9MXr4leuIa4fteTkdr6n+7aoaW+hQt/vkD1nmrKtpRR81oNvAZj/3sskY9EWrVHpDshpyr4S+7XuI2eSOq0wXcUt1haOLTzEP8v4f91+zOX34fu2+AqfdWwm1RxOL4XeixelFLRWusCrfUnnZ/XWp8AnlSt75AxWuuibnbfDcQqpWJoLUaWAj/ucvxE4I/AYq31+U4vfQk83WmQ7nXAv/Xj+xJCDNCFqgZOlNSybJZtBj3W5NZQ9Psizq89T0tVC54TPGkqb8I1wJWJf5w44OOaPcyMvHMk3Nn6uOFsA6XrSwm4qnUwbcnHJRQ+U0j4z8IJuTXE6peW4kb64ulqJquwnFusULy0T3U/N3KuFdINH8OtcBmueut5ebZtErp1wB7gAuBBa0fsAmAhrXcFXVK8aK2blVIraC1EzMDrWusDSqkngQyt9cfAs4AP8Le2N1yh1nqJ1rpUKfUbWgsggCe11qWD/F6FEH2w50TreJfpUVa7Mtyh5PMScm7MweRpYsRtIxh13yj85/rb5APHfaQ7o5aP+u4JMzSebST3tlzcI9yJ/GUko+4bhcndOkWMi9lEQri/1WbaDfcL5/9u/j+ujrnaKscbTu5ddy+Nlkb+cutfjI4ibKTH4kVr/aO225uXAfcAI4E6IA/4HHhKa13fw/6ft23X+bnOl6Cu6WHf1wFZzEMIO9tzohQ3FxOTx/hZ5Xg1eTU0nm0kcEEgAQsCiPnPGEb9ZBRuIW6972xFITeFEHx9MCWflnDyuZMcWXGEc2+fI/GbRKsVT0lRgfzftmPUN7Xg4Tq48RYhXiHcl3SfVXINN826ma+OfmXzS4XCOL3+L4fWOpfWmXY/obVoOU5rj8j7PRUuQoihac+JMhLG+OPuMrgP38bzjRx64BC7p+wmf2U+WmvMnmaiHo2ye+HSTpkVIbeEMG3bNBK+SiDikQiUUliaLVTsqBj08dsnq8s5NfhjvbP/HU5WyGRrA5ESnsL5mvMcLz9udBRhI33tL11D6/j93wMv0jrp3Ju2CiWEMEZ9Uwv7T1UyPWrgt0hrrTnzpzPsitvF2T+dZcyDY5i6eapD/R+wUoqga4MY8b0RAJx9/SxZc7M4sPQADacGvghix2R1JwZ36ehU5Snu+OAOPsj7YFDHGa5SwlMA2HlSFml0Vn2922iy1rrz7LhblFK5tggkhDDO/lMVNLZYBlW8lH1VxqH7DuE/358Jf5iA9xXeVkxoG2F3hdF4ppHC1YWUri9l/O/GM3L5yH4XXCE+7kQGeQ163Mso31HkPphLkKf1xx0NB5NDJ+Pt6s3Oop0sS1hmdBxhA33teclUSnXMkqSUmgVk2CaSEMIoGW09Bkn9LF601tTk1QAQeF0gUz6dwrQt04ZE4QJg9jQT/R/RzNw/E5+pPhy65xD5K/MHdKykyAAyC8sHtTigSZm4YsQVhPmEDfgYw5nZZCZ5TDI7i6TnxVn1tXiZDuxQShUopQqAncBMpVSOUirbZumEEHa150QZMSHehPi493mfxnON5Nycw54Ze6g/UY9SiuAbg1Emx7lM1Fee4zyZtmUa418YT+gdoQD9LkKSogK5UNXAqfK6Aed4attTrM9fP+D9Reulo31n91HbVNv7xmLI6etlI1lXSAgnp7Um80QZqRND+7xP6YZS8u7Mo7mimXHPjsM9su9Fj6NSJtWx3ABA/s/yUSbF2P8ei8mt9//fS4xo7bXKLCwnPNCr3+evbarl8a2P8/Cch1k8Xv70DlRKRAotuoWM0xnMj5pvdBxhZX3qedFan+jpn61DCiFsr6CklpKaxj6tZ6S15sTqE2QvzsZ1hCvTM6YT/i/hDjUo1xrae12K/qeIrCuzqCvovTclbpQvHq6mAQ/a3X1qN82WZlmMcZDa14OSQbvOyf6rlwkhHFJGQes8kH0ZrKuUovFUIyN+NIKk9CR8JvvYOp4hlFLEvhBL/Afx1B6uJXNmJuVby3vcx9VsIiE8YMArTO84uQP47o4ZMTAhXiGsTF5JfGi80VGEDfT1spEQwsllFpbh5+HC+BGXL0Rqj9TSUtOC7zRfxv1uHMqsnK63pTsjvj8C78ne5CzJIeeWHGYXzMY1wPWy2ydFBvLa9mPUNjbj5da/P7PfnPyGuJA4gr2CBxt72Hvh+heMjiBsRHpehBAAZBSUkRQViOkyA23Lvy4nMyWTQ/cdQmuNycU0LAqXdl4TvEhKT2LyR5M7Chdt6X4w76yYIJotmr2FPffSdGXRFnac3MHcCFnPyFpOVpykrM46SzYIxyHFixCCitomjpyvZsZlLhmde+cc+xbuwzXYlfh344dV0dKZa4ArgamtbXT6tdPk3JRDS03LJdslRQWiFOwq6N+SbIeKD1FWXybjXazkaOlRIv8nkr/l/s3oKMLKpHgRQpBxon28y8WTorUPzM27Iw+/ZD+SdiThOc7TiIiOR0Ppl6XsvXovjRcaL3rJ39OVK0b6set4/4qX7YXbAbgy8kqrxRzOxgaO5eUbXmZhzEKjowgrk+JFCMGu46W4mU0d09u3082asg1lhN4RSsKGBFyDLz/OY7gZ/Y+jmfzhZGqya8iam0Xd8YvvREqOCSKzsIzGZkufj7ntxDZG+owkNijW2nGHJaUUD858kHFB44yOIqxMihchBOnHS5ka4d+xErKl2UJzZTMmVxNTPp3CFW9dgdljcAs1OqOQW0KYumkqTcVNZM3JoqmsqeO15Jgg6pss7D/d90Uaa5tqWRizcNhelrOFivoK3t3/LudrzhsdRViRFC9CDHM1Dc3sP1VBckzrJSNLg4Xc23LJXpSNpcmC2dM8JGfLtRf/Of4kfpNI5KORuAZ+1zM1M7q1PXf349LRh7d/yF9u/YvVMw5nx8uPs/SDpWw4usHoKMKKpHgRYpjLLCyjxaJJjgmmubqZnJtzKP57MaF3hGJylT8RfeF9hTfhK1pn5a3YWUH59nJG+LoTE+Ld73Ev0utiXVNCp+Dv7s+2E9uMjiKsSP4yCTHM7TpeitmkmBbkS/aibMo2lRH3RtxFU+SLvtFak/+v+WQvzqZscxnJ0UFknCjDcplbqjtb8fkKbn33VjukHF7MJjNzI+d2DIYWzkGKFyGGuW+PlzJ5tB9FK45S+W0lk96dxMi7Rxoda0hSSjFl3RQ8x3qSc2MOc864U1HXxOHzVb3uG+UfxfjA8XZIOfzMi5xHXnEeF2ouGB1FWIkUL0IMY/VNLew9WU5yTBBjnx7L5A8nE/rDvi/MKC7lFubG1C1T8brCC/+HzpJw1NynS0cPz32YZ6971g4Jh5/2hRm/Lvza4CTCWqR4EWIY23uwhNSdJpKjgvAc60nIkhCjIzkFtxA3pm6eis8Ub64+7NZr8VJaV0pjS2OP24iBmzF6Bh4uHnLpyIlI8SLEMNVc3UzxnfncvsWN+DKZv8XaXANcmbpxKscfCmDX8VIslsvP9/LY5seI/F0kFt33OWFE37mZ3ZgdPlsG7ToRKV6EGIZa6lvYv2Q/XjmNfHKXidHzZBFAW3ANcGVGbDD1Zxv4dvYeKtK7n/NlW+E2po2chknJn2RbmRc5j6yzWVQ19D7+SDg+m/6mKKUWK6UOKaXylVKrunl9vlIqUynVrJT6YZfXWpRSe9v+fWzLnEIMJ5am1nlcytPKeXNJI/7fl8LFlpKjgzBZFDVnG8lenE3Vnos/PEtqS9h/fn/HuAxhG/Mi56G1Zt+5fUZHEVZgs+JFKWUGXgauByYBdyilJnXZrBBYDqzt5hB1Wutpbf+W2CqnEMNNdVY1pV+V4vGbMaTFNZEcI8WLLcWG+qDCXNiyygvXQFf2XbeP6uzqjtfbx2FI8WJbV0VfRekvS2XdKCdhy56XZCBfa31Ma90IvAPc0nkDrXWB1jobkAu9QtiJX7Ifsw7PYt+Vrb/+M2O6X0laWIfJpEgZF8yWilISNiVg8jSRvSibumOtayFtO7ENd7M7M0fPNDipc3MzuxHgEdD7hmJIsGXxMgY42elxUdtzfeWhlMpQSqUrpb5n3WhCDD/HHzvO2TfPAuAR6cG3x0uJCfEm1NfD4GTOL2VcCKcr6jnvp5n61VQ8J3hicm/987v1xFZmhc/C3cXd4JTOb/uJ7Sx+azEV9X1fb0o4JhejA/QgSmt9Sik1FtislMrRWh/tvIFS6n7gfoCwsDDS0tIMiGkf1dXVTv399YW0wSDa4K/Aq8ASOBh5kBaLZseRWmaPdhmSbTrU3gvm6tbO5Te+2EFqhCs8DjuP7KRqfxVZZ7K4O+rufn8/Q60NbKG/bbC3fC+Hzhziw00fEuMdY7tgdjYc3wu2LF5OARGdHoe3PdcnWutTbf89ppRKAxKBo122eZXWP8nMmDFDp6amDi6xA0tLS8OZv7++kDYYWBucfu00h189TOjSUK546wqUWZFZWEb9Vzv44bwppCaMtk1YGxpq7wWtNf+zbxMlLsGkpiZ2PP/7f/w9Olxz99y7SY1N7dcxh1ob2EJ/2yCVVH7Gz2wXyCDD8b1gy8tGu4FYpVSMUsoNWAr06a4hpVSgUsq97esQYC6Qa7OkQjip4k+LOfyTwwRdH0Tcm3Eoc+uifzuPlgCQMlYG69qDUoo544LZebQErb9b52jf5H14NHrg94gflmYZ+mcvMp/O0Gez4kVr3QysAL4E8oD3tNYHlFJPKqWWACilZiqlioAfAX9USh1o2/0KIEMptQ/YAqzWWkvxIkQ/1eyrwSfRh0nvTbpohehv8ouJG+lLsI+Ms7CXOeNCKK5uIP/8d3carbpzFS8Fv0TlR5Uc/snhiwobYRvv7n+X4GeCKaktMTqKGASbjnnRWn8OfN7luV93+no3rZeTuu63A5hiy2xCODOtNUopoh6NIvwX4Zg9zB2v1Te1kHGijLtmRxmYcPhJGdfay7XjaAmxYb4AxAbHEvvzWI5XHOfEkydwH+1OzG+cZyyGI4rwj6C8vpy0gjR+MOkHRscRAyTTOQrhZJpKmtg7fy+V31YCXFS4AGSeKKOx2cKccXLJyJ4igrwID/Rkx9FiOHBkWwAAIABJREFUAHae3Mlb2W/R2NJI9OPRRPwygqAbggxO6fxmjp6Jt6s3m49vNjqKGAQpXoRwIi11LeQsyaFydyW6uftLEDuOlmA2KZJj5IPS3uaMCyb9WCkWi+Yv2X9h5RcrMSszSinGrR6Hf4o/APVF9QYndV6uZlfmR81nc4EUL0OZFC9COAndosm7M4/KnZVMensS/nP9u93um6PFJIT74+shizHa25xxIVTUNZF7ppKXbniJzJ9kYjZd3DN25vUz7Jqwi4qdMheJrVwdczUHiw9ypuqM0VHEAEnxIoQT0FqT//N8ij8sZtxvxzHiByO63a6qvonsogrmjguxc0IB34172Xm0BJMyER0Qfck2wTcF4zbajZybc6g9XGvnhMPDgugFAGwp2GJwEjFQUrwI4QR0s6a+oJ7wfw0n4mcRl91u1/FSWixaxrsYJMzPg7EjvFmb8x7/9Ok/Udt0aXHiFupGwvoElFJkX59N4/lGA5I6t2kjpxHgEcCmY5uMjiIGSIoXIYY4rTUmVxOTP5zMuOfG9bjtjqMluLmYSIqS9YyMcuX4EPacX8+6Q+vwdPHsdhuv8V5M+XQKjWcaybkpR+aAsTKzyczVMVfz1bGv5Pb0IUqKFyGGsPJt5WSmZNJwpgFlViiT6nH77UcuMDM6EA9Xc4/bCduZOz6IGpVFQsiVKHX5n5ffLD8mvTOJ0Q+MxuQif6qtbdG4RRRVFpFXnGd0FDEAjry2kRCiBzW5Ney/ZT9uI906FvnryenyOg6fq+aH0y+ZWknYkaf3CSyqEn/TjF63DVny3dikuqN1eIz16LHgEX13Y+yNPHftcwR7yiXUoUjKeSGGoIYzDWRfn43Jw8SUL6bgGtT7nUPbj1wA4KoJobaOJ3qwrXAToCguvqLP+1TnVLN78m4KVxfaLtgwM8ZvDL+Y8wvCfMKMjiIGQIoXIYaY5qpmcm7MoamkiSmfTcEzuvtxE11tPXyBkX4eTAjzsXFC0ZP1+euJ9Inn6DkT56v6Np+Ld7w3IT8I4fi/H+fsW2dtnHD4KK8v593971LfLPPqDDVSvAgxxLRUtqAtmvj34/FN8u3TPs0tFr4+Usz8CSFy2cFA5fXlpBels2j8YgC2Hy7u037KpIh7PY6ABQEcuucQZZvLbBlz2NhxcgdLP1jK9hPbjY4i+kmKFyGGCK01WMB9jDsz9swgeHHfr9XvK6qgsr5ZLhkZbNOxTbToFpZNvZkQHze2tV3K6wuTm4n4D+PxnODJ/lv3w2kbBh0mUqNT2XnvTq6OudroKKKfpHgRYogoeKIAngBLowVl7l/vydbDFzD9//buPDyq6m7g+PfMTGYmyWTfCNlDEiAQQtgXUSguKOJuBZdK3ap1rbbW2ioutfWtWqvia91ttYq8riBQBRHBhSUsIQTIDiEL2fdtMjPn/WPCKkuATG4mcz7PMw8zd8699zeXm5nfPefcc4TzNl1FO18Wfom/yZ8pMZOZlhzGuvwaHI6e36rrFejFqBWjiL43GlRXjTPm4+XDpOhJPxnlWOn/VPKiKG6g4q0K9j6+F3xAeJ16s8/avGpGxwQS4KOmBNBSUnASt465FS+9F9OSQ6lrtZJT3nRK2zDHmEl4IgH0zjmQbI02F0XrGQrqCrhnxT2UN6uqLHeikhdF6edq/1tL7m25BJ0fBA9wyn1W6lutZJU2cHbKsacMUPrOg1Mf5NnznwVgWrLz/+NUmo6OYIVtZ29jx5U7cFjVIHanq9XayksbX+LLgi+1DkU5BSp5UZR+rHlLMzlX5WBJszDioxGnNTLTdwU1SAnnqORFU+XN5Vjth4b6D/MzMWKwP2tyq05vg0aIXxBPw9cN5N6Sq0aKPU2jIkYRaYlkRcEKrUNRToFKXhSlH3N0OPBJ8SFteRoGv9MbU3JNbjWBPl6Mig7s5eiUUzH/s/lMfWvqEctmDgtn89566ltPb/6iQTcOIv7JeCrfraT4keLeCNPjCCGYnTyb/xb894jkUunfVPKiKP2Qo8vZDBAwJYCxmWMxRZpOazt2h+Sb3CpmDA1Hf5KpAxTXunfivfx+6u+PWDZzeAQOCd+cbu0LEPfHOCJviaTkqRIqF1WeaZgeac7QOTRbm1m7d63WoSg9pJIXReln7O12sn6Wxd6n9wKcdL6iE9laUk9dq5WZw9Ut0lqbnTKbq1KvOmJZWlQAYX4mvt51+smLEILkV5KJezSOkAvVUPen49zEczEbzCzNXap1KEoPqeRFUfoRaZfsun4Xjd834j2kZyPnnsjKXZUYdEJ11tXYp7s+Jbsy+yfLdTrBzGHhfJtXjfUMZo7WGXQkPJ6AIcCAvd1O6+7WMwnX4/h4+TAzYSZL85aqvkNuQiUvitJPSCkp+E0BNZ/UMOTvQwi/+sxrS1btrGRSYgj+ZnWLtFZsDhu3LL2FZ3545pjvzxweQUunjQ3Ftb2yv9ybctl2zjbai9p7ZXue4pKhl1DcUMzO6p1ah6L0gEpeFKWf2PfsPspeKiP6/mhi7os54+0V17RSWN2qmow09sO+H6hrr+OSoZcc8/2zkkIxGXRn1HR0uLgFccguyfYLt9NV29Ur2/QEF6dcDMDSPNV05A5cmrwIIWYJIXKFEAVCiIeO8f7ZQogtQgibEOKqo967UQiR3/240ZVxKkp/YAgyEH5dOEOeGdIr2/t6l7Pz5rnD1VCsWlqauxQvnRfnDzn/mO97G/WclRTKql2VvdJk4TvMl5FLRtKxt4PsS7Kxt9vPeJueYLDfYH4+4ucEmAK0DkXpAZclL0IIPfAycCGQCswTQqQeVawEmA+8f9S6wcACYCIwAVgghAhyVayKoqUDPy6DbxnM8HeHn1EH3cOt3FnJ0Ag/YoJ9emV7yulZkreEGQkz8Df5H7fMzOERlNa3k1fZ0iv7DDwrkOHvDafpxyby787vlW16gg+v+pA7xt+hdRhKD7iy5mUCUCClLJJSWoFFwKWHF5BS7pFSbgeO7ql2AbBSSlknpawHVgKzXBiromiiZXsLGxI3ULeyDjj10XOPp6HNSubees5NVU1GWsqrzSOvNo9LUo7dZHTAgaa9Vbt671bn8KvCGfrGUGIfjO21bXoCm8NGVWvvNOEpruPK5CUK2HfY69LuZa5eV1HcQse+DrZftB104DOsd2tHVu2qwu6QqslIY5/s+gRwjiNyIhH+ZtKjA/gqZ3+v7j/ypkh8UnyQUtL4Y2OvbnugGvvaWG5deqvWYSgncXpDdvYTQojbgNsAIiIiWLNmjbYBuVBLS8uA/nw9MaCOQTNwD9AAvADrC9dD4clX6+kxeHdzB8FmQUPhNtYUDbzB6dzlXHh789sM8xtG0dYiiig6YdmhPlYW53Xxf8tXE+Zz8uvKUzoGS4B/4GyMP6dnq7gDV5wHFwddjMVgcYvz6wB3+XvoTa5MXsqAw2+ZiO5e1tN1px+17pqjC0kpXwNeAxg3bpycPn360UUGjDVr1jCQP19PDJRjYG+3k3VuFs3lzYxaMYqgn/W8O1dPjkFTRxe7Vq7ihsnxzJhxdDezgcEdzoXi+mLyvs3jmfOeYfqU6Sctn5DWyuJn1tBgiefqsxNPWv5UjoF9op2sjVk0/6WZUWeNImjmwOhC6IrzYPoRPz3uwR3+HnqbK5uNNgHJQogEIYQRmIsz/++JL4HzhRBB3R11z+9epihuT2fUYcmwkPpB6iklLj21elcVVruDi9IG9fq2lZ7Lrc0l2DuYK4df2aPycSG+jBjsz/IdFb0ei95bT9qSNHxSfNhx2Q6aNjX1+j4GktyaXD7c8aHWYSgn4LLkRUppA+7CmXTsAhZLKXOEEE8IIS4BEEKMF0KUAlcDrwohcrrXrQOexJkAbQKe6F6mKG5LOiTWKitCL0hZmELYFa4Z9XZZdgWD/M1kxAyMq2t3NStpFpW/rSQhKKHH61yUFsnWkgbKG3p/gDmvYC9GfTUKrzAvdlyyA3ubuoX6eF7e9DLzP59Pi7V37v5Sep9Lx3mRUi6XUqZIKYdIKZ/qXvaolHJJ9/NNUspoKaWvlDJESjnisHXfklImdT/edmWciuJqUkry78ln87jNLh04rKXTxrd51cwaOQidmohRM132LqSUGHSn1jJ/4UhnbdmKHb3bcfcAU6SJ9JXpDHtnGHofvUv2MRBclXoVHbYOluUt0zoU5TjUCLuK0gf2PL6H8pfLCb8mHEOw67qard5dhdXm4KK0SJftQzm5lza+RNJLSTR2nNodPolhFoYN8mNFdu83HR3gPcSb4AuCAahZWoO1yuqyfbmrqTFTifCN4KNdH2kdinIcKnlRFBcrXVjK3sf3MuiXg0j8W2KvjeVyLMu3VxDuZ2JcnGoy0lJKSAqzhswiwHzqo7XOToskc289+xs7XBDZIdYaK7uu3cX2C7dja7K5dF/uRq/Tc+XwK1mWt4xWq5rksj9SyYuiuFDNFzUU3F1AyKUhpLyW4tLEpamji9W5VVyomow0d3HKxbw8++XTWnf2KGet2Rfby3szpJ8whhpJXZxK6/ZW5zQCHaoPzOF+PuLntNva+Tz3c61DUY5BJS+K4kKB0wKJvj+a1A9S0Rlc++e2IrsCq83B5WOiXbof5cQ2lG5gf8vp91lJDLOQHh3AJ1t6OrLE6Qu5MIRh/x5G49pGdl69E4f16MHOPde0uGnEBsTy3vb3tA5FOQaVvCiKCzR+34i9zY4hwEDSc0novV3fOfKTLWUkhvqSHq0mltOKlJIbPr2B6z657oy2c3lGFDsrmsjd39xLkR1fxLwIkv83mdovaqn+qNrl+3MXOqHjurTr+KrwKypbem/aBqV3qORFUXpZ3Vd1bPvZNooeOvGIqr2ptL6NDcV1XJ4R5dKmKeXENpVvIr8un+vSzix5mZM+GL1O8MnW0l6K7MSibo9izPoxhM9Tc2Ed7vpR12OXdj7Y8YHWoShHUcmLovSi+jX17Lh0Bz7DfYh/LL7P9vv5Nmf/iMsy1BRgWno3613MBnOPB6Y7nhCLiXNSwvh8azkOh+yl6E7Mf6I/QghatreQf28+0t43++3PUsNSmRE/g7auNq1DUY7i1nMbKUp/0vh9I9kXZ2NONJO+Mh2vYK8+2a+Ukk+3ljE+PoiY4N6d4FHpuS57F4tyFnHJ0EtO6y6jo12eEcXq3VWsL6plSlJoL0TYM/Wr6il7sQx7q52hrw1FeHjn769/8bWqzeyHVM2LovQCh83B7vm7MUWZSP86HWOYsc/2vaOsiYKqFi7PUB11tfRl4ZfUtNVwfdr1vbK981IjsJgMfLLV9R13Dxdzfwxxj8Sx/8395N+Vj5SeXQMjhEBKqfq99DMqeVGUXqAz6Bi5ZCTpX6djGmTq033/3+Z9GA06ZquB6TT19ra3CfUJZVbSrF7ZntlLz+y0SJZnV9DS2bfjsMQ/Hk/MgzGUv1JO/t35yD5quuqv7lh2B+NeH4fNocbD6S9U8qIoZ6BhbQPFjxYjpcR3uC/maHOf7r/daufTrWVcNHIQAT5900yl/NT+lv0syV3C/PT5eOl77/9h7oQY2qx2Pt/Wt7UvQggSn04k+oFo2vPbkV2enbzMHTmXR89+FIdUt5L3Fyp5UZTTVP91Pdtnbaf6/6qxN2szwNey7AqaO2zMmxCryf4Vp3e2vYPNYePWsbf26nZHxwQybJAfH2ws6dXt9oQQgiHPDCFtaRo6kw5bow2HzTN/vKfHT+fWsbdi1Pddc7ByYip5UZTTUPdlHdkXZ+M9xJvRa0Zj8Nem7/sHG0tIDPNlQkKwJvtXnCpbKjkv8TxSQlJ6dbtCCK6bGMuOsia2lzb06rZ7un+dUYej00HWuVnsum4Xji7PTGCaO5tZuHEh+xr3aR2KgkpeFOWU1XxRQ/Yl2fgM8yH9m3SMEdpcjeVVNrN5bz3zxsequyE09vys51lx3QqXbPvSjCi8vfSa1L4coDPpCLsmjOrF1eRcnYOj0/MSmNr2Wu5ZcQ9vbn1T61AUVPKiKKfM0eHAMtrivKsoVLtq5Pc3lGDU67hyrLrLSEvlzc4xdvQ614yi7G/2Yk56JJ9vK6e5o8sl++iJ2N/GkvRSErWf17Ljsh3Y2zxrLqT4wHguSLqAN7a8oTru9gMqeVGUHmrLdQ5UFX5VOGN+GNNn47gcS3NHFx9tLuXCtEEE+6p2eK3sa9xH3D/ieDXzVZfuZ96EWNqsdj7b5trJGk8m+q5oUl5Poe6rOvJ+ladpLFq4bcxtlDWXsSxvmdaheDyVvCjKSUgpKfpTEZtGbqJpYxMAQq9tM81Hm0tp6bTxy6kJmsbh6fxMfjw+/XEuSLrApfsZHRNIWlQA73xf3Gcj7h7P4FsGM/KTkcQ/Ea9pHFqYM3QOMf4x/GPDP7QOxeOp5EVRTkDaJXl35FHyVAmD5g/Cb6yf1iFhd0je+WEPY+OCGB0TqHU4Hi3QHMjD0x4mPjDepfsRQnDzWQkUVrfybZ72kyeGXhqKd4I30iEpeKCA1pxWrUPqEwadgXsm3sOaPWvYWrFV63A8mkpeFOU47G12cq7OoeLVCmL/EEvKayma17gArN5dxd7aNm5StS6aWpK7hA93fNhnY39clBbJIH8zb35X3Cf764nO8k6q3q9i67StNP7QqHU4feKWMbfg6+XL8+uf1zoUj6aSF0U5jsr/VFLzWQ1JLySR+JfEfnNHz1vfFTM4wMwFIyK0DsVjSSl5aNVDPPPDMwj65rwwGnT8Ykoc3xXUsKuiqU/2eTLmaDMZP2TgFeJF1rlZVH+qfa2QqwWaA7k542YW7Vh0sLO20vdU8qIoRzkwjkXkLZGM3TSW6Hv6z908RQ12fiyq5cYp8Rj06s9XK0vzlrKrZhe/mfSbPk1qr50Qi7eXvl/VvngneJPxfQa+o3zJuSKH0hdLtQ7J5e6ddC82h42XNrykdSgeS337Kcph6lbVsXHYRtry2xBC9Is+LodbWtRFgLcX102K0zoUjyWl5M9r/0xiUCLXjLymT/cd6GPkmvExfLa1jOq2/jPWijHcyOhvRhNxQwS+ab5ah+NyiUGJLDhnATMTZ2odisdyafIihJglhMgVQhQIIR46xvsmIcSH3e9vEELEdy+PF0K0CyG2dT/+6co4FUVKSdk/y8i+MBu9jx6dqf/l9bsqmthaZeemqQlYTNqM6KvAyqKVbCrfxENTH8Kg6/v/h9vPGYJOCJYVazfmy7HovfUM//dwgmYEAc5mV2uNVeOoXGfB9AWcm3iu1mF4LJd9Qwsh9MDLwIVAKjBPCJF6VLGbgXopZRLwPPA/h71XKKUc3f243VVxKoqj00HebXnk35FP0HlBZHyfgTm2bydY7ImXvynArIf5U+K1DsWj/Xntn4n2j+YX6b/QZP+DAsz8fHw060ptlDe0axLDyXSWdZJ7Sy5bJm2hZUeL1uG4THVrNX/8+o80dnhGZ+X+xJWXlxOAAillkZTSCiwCLj2qzKXAv7qffwTMFP2lV6TiMUr+p4SKNyqIfTiWtKVpms1TdCIFVS0sy67g3DgvNXu0hr7d8y3rStbx4JQHMRlMmsVxx/QkAP75baFmMZyIKcpE+up0HK0OtkzcQuWiSq1Dcol9Tft4+vunWV28WutQPI6Q0jUDHgkhrgJmSSlv6X59AzBRSnnXYWV2dJcp7X5dCEwELEAOkAc0AX+SUq47xj5uA24DiIiIGLto0SKXfJb+oKWlBYvFonUYmur1Y2AH9EAHkIXzzOunFm7tYEeNnQXjJJFBnn0egDZ/D1JK7t52N5Udlbw34T1Meu2SF4DXtrawsUrw9NnehHr3v2ZOAGqBx4FsYC7wq97dfH/4XqzurCbMFKZpDP3hOLjKjBkzNkspxx29vP9dYjpVALFSylohxFjgMyHECCnlEfcHSilfA14DGDdunJw+fXrfR9pH1qxZw0D+fD3RW8dAOiSlz5ey/939ZKzLwOBngFlnHp+rbCmpJ/O/P3D/eSlE6ss8/jwAbf4eluUtI2dtDq/PeZ0Lxrh2RN2eqG1fTWZtJz80BfP3C0drHc5xOS5xUPi7QnxSfIiaHtWr2+5P34sVzRVE+kVqsu/+dBz6iivT9TIg5rDX0d3LjllGCGEAAoBaKWWnlLIWQEq5GSgEeneuecUjWWusZF+STeFvC/FO9EbatR1q/WSklDy9YjehFhM3n6UGpdPS+UPO551L32H+6PlahwJAiLeOm6Ym8Om2MnaU9d8+FzovHcn/SCbq187EpfrTauq+rNM4qt71yqZXSHwxkZJG7Wb+9jSuTF42AclCiAQhhBFnpeGSo8osAW7sfn4VsFpKKYUQYd0dfhFCJALJQJELY1U8QMO6BjJHZ1K/sp7khcmM+HgEXoH9u//IN7lVbCyu495zk/FVdxhpRkqJl96LG0ffqMkdRsdzx/QhBHp78T//3a11KD0ipWTfc/vYPms7Bb8twGHtP7d7n4nZKbORUvLIN49oHYrHcFnyIqW0AXcBXwK7gMVSyhwhxBNCiEu6i70JhAghCoD7gQO3U58NbBdCbMPZkfd2KeXAStWVPiWlpOihIvTeesasH0PUnVH9ZsTc4+m02Xnyi10khvoyd3zMyVdQXKKho4FR/xzFivwVWofyEwHeXtz9s2TW5dfwze4qrcM5KSEE6SvTGXzHYEqfK2XLlC205bdpHdYZiw2I5d6J9/Ju1rtsLt+sdTgewaW9vKSUy6WUKVLKIVLKp7qXPSqlXNL9vENKebWUMklKOUFKWdS9/GMp5Yju26THSCmXujJOZeBq3dmKtdqKEIIRi0cwdstY/DL618Bzx/P62iKKa1p57JIReKnRdDVT315PoDlQs/4MJ3P9pDiGhPmyYEkOHV12rcM5Kb23npT/TWHEJyPoKOogMyOTjn0dWod1xh6e9jDhvuHcvux27I7+///g7tQ3ojIgSbuzejpzTCZFv3e2OJqiTM7OuW5gX10bC78p4KK0QZydou2dDJ4uISiBtfPXMnpQ/+wUazToePKykZTUtfHyNwVah9NjYZeHMS5rHPEL4jHHOMdVsre5749+gDmA5y94nszyTF7d/KrW4Qx4KnlRBpy23Da2zdhG4W8LCb4gmMS/Jmod0imRUvL40hwEgj/NPnpcR6Wv2B12Hl/zOJUtlf2+iXHKkFAuz4jin98WUlDlPoPCmWPMxP4uFoCWHS38GPsj5a+W46ohPFxt7si5zEyYycNfP8z+lv1ahzOgqeRFGVCqP61m06hNtGa3MvTtoYz8bCTGCKPWYZ2ST7eWsWpXFb85L5nBgd5ah+OxnvvxOR779jG3GYDs4YuG4+2l56GPt2N3uN+Pv8HPgGW0hbzb88g6N4u2XPfrCyOE4H9n/y/ttnbuXH6n2yZh7kAlL8qAYO9wVjcHTA0g4oYIxu8aT+T8yH5/xXy0isZ2FizJYVxcEDef5V41RgNJdmU2j3zzCFcOv5K5I+dqHU6PhPmZePzSEWTurefVtf1z5N0TMceZSV+ZTso/U2je3MymtE3seXyP1mGdspSQFJ6c8SSf7PqED3M+1DqcAcs9OgAoynF0lnVS9Ici2gvbyViXgTHcyLA3hmkd1mmRUvLgR9ux2SXPXp2OXudeiddAYbVbufGzGwkwBfDK7FfcKgG+bHQUK3dW8vzKPM5JCWPE4ACtQzolQggG/2owoZeFUvhgIY4O97yV+oHJDwAwO3m2xpEMXKrmRXFL9lY7ex7fw4aUDVQtriJweiDS5t5VtK+uLWJdfg0PXzSM+FBfrcPxWA+ufJCt+7fy2pzXCPN1r87SQgieuiyNQB8j9y3aRpvVpnVIp8UYYWT4v4aT8BfnwIx1X9WRNSuLlu3u0Z9Hr9Pz4NQH8TP50WJtocvev2YAHwhU8qK4nZYdLWwYuoE9j+0hZHYIE3ZNIPGpRHRG9z2dfyys5W//3c3stEiunxSndTgea3HOYl7Y8AL3TLiHy4ZdpnU4pyXI18jzPx9NYXULv/842637XRyo9bJWWWne2Ezm6Ex237SbzrJOjSPrmRZrC5PemMRDqx46eWHllLjvt73iUaSU0D0xrXeSN/4T/Rm9bjQjFo/AO8G9O7VWNnVw9wdbiQ/15ekr09yqmWIg2V2zm5uX3Mzk6Mk8c/4zWodzRs5KDuWB84eyNKuct77fo3U4Z2zQ9YOYWDCR6PujqfxPJRuSN7DvH/u0DuukLEYLVwy/gouSL9I6lAFH9XlR+jUpJXXL6yh+tBjKwHGZA71Zz8iPR2odWq9o7bRx87820Wa18f6tE/Ez9+/pCgaquvY6Zr8/G2+DN4uvXoxR7153qB3Lr6cPIWtfA39ZvoukcAvnuPl4QV7BXiQ9m0TUnVEUP1yMzuy89nZ0OrC32vEK7p9/O0/MeOLg84aOBgLNgRpGM3ComhelX5J2SfUn1WyZtIXsi7Ox1dtgPqDXOrLeY7M7uPP9LeyqaObla8eQEuEeI/8ORAGmAC4fdjlL5y0l2j9a63B6hRCC536eztAIP+54bzPbSxu0DqlXeCd4k/pBKoN/NRiAircrWB+/nqI/FWGtsWoc3fG9vvl1hi4cSn5tvtahDAgqeVH6pfqv68m5Moeumi5SXkthQu4EmAU6w8A4ZR0Oye8/zmZNbjV/vmwkM4aFax2SR+q0dbKvcR96nZ5nz3+WidETtQ6pV/mZvXjnl+MJ9jXyy7c3UVTtHh1ee+JA82rg2YEEXxhMyV9KWB+7nrw78vrlGDHnxJ+DQzq44L0LqGiu0DoctzcwfgkUt9exr4PiR4opecY5pXzQuUGMXDKSiXkTGXzrYHReA+dUtTskv/toOx9vKeX+81KYNyFW65A81q1Lb+Wst8+i1dqqdSguE+5v5t83TQBg7mvrKahq1jii3uWb6suID0cwPmc8EddFUPF2Bbvn979ZtlNCUlh27TKqWquY/q/plDWVaR2SWxs4vwiK25EOSe2KWrIvzWZ9/Hr2PrWXliznlaHQCULnhCIurqgHAAAZO0lEQVT0A6vzapfdwQOLt/HxllJ+c24K98xM1jokj/bA5AdYcM4CfI0D+9b0xDALH9w2CYd0JjC79zdpHVKv8x3uy9DXhzK5ZDIpr6UAYK2xsmnUJkqeKcFapX2T0oSoCXx5/ZdUNFdw9jtns6dhj9YhuS2VvCiaKbi/gOyLsmla30Ts72OZWDSR1PcG7lw+je1dzH97I59tK+d3Fwzl3nNV4qKFiuYKXtzwIgDpg9K5KeMmjSPqGykRfnz4q0nodYKr//kja/OqtQ7JJYzhRixpFgC6qrowBBgoerCIH6N/JOfnOdR9VYfDpt3gd1Njp7LqF6uoa69j8puT2Vi2UbNY3JlKXpQ+Ya22UvZyGVsmb6F5i7PaOvKXkaQuSmXyvskk/iUR73j3vuX5RIprWrnqlR/YWFzHs1enc+eMJK1D8khbKrYw6c1JPPz1w+xt2Kt1OH1uSJiFT349lahAb375zib+9cMetx4H5mR8U33JWJfB+JzxRN0VRf3X9Wy/YDudpc5xYrrquzT5/BOiJvD9Td/jbfDmnHfOYdGORX0eg7tTyYviMvZWO6UvlbJt5jZ+iPyB/LvysbfZsTU6R/20pFsIvybcrQeX64nPt5Vx8YvrqG7p5F83TeCqsQPjbhZ3IqXkpQ0vMfnNyTikg3W/XEdcoGcOBhgV6M1Hd0xhxtAwFizJ4a73t9LYPrBHgPVN9SXp70lMLptM+qr0gxdKu+fvZn3Cegp/V0jDuoY+rZFJDUtlwy0bGBs5ljuX30lDx8C4G6yvqHFelF4j7ZLmrc04Wh0EnhMIeij6QxHmODOxD8YSPi/8YHWuJ6hp6eTJL3by+bZyxscH8cLcDDVLtAb2NOzhzuV3sjx/ORenXMw7l75DiE+I1mFpymIy8NoN43h1bRHPfZXLtn0N/PWKNM5287FgTkZv1hM0M+jg6/B54cguSekLpex7dh+GYANRd0eR8FhCn8QT5hvG6htXs7tmN4HmQOwOO1mVWYyJHNMn+3dnKnlRzkh7UTt1X9VRv6qehtUN2Opt+Kb7Mn7bePRmPRMLJmIaZNI6zD5ld0j+L3Mff12xm3arnd+cm8KdM4Zg0A/sGqb+xmq38uKGF1mwZgECwYuzXuSuCXepEYy76XSCO6YPYfKQEO7/cBu/eGsjs0dF8ujFqUT4m7UOr09EzI0gYm4EtkYbdV/VUbu0FoOf82fR3m5n+6ztBEwLIOhnQfhP9kfv3fsDTRn1RkZFjALgza1vcvsXt7Px1o2MGzyu1/c1kKjkRekxR5eD1u2tNG9tZvAtzgGiCh8spObjGkwxJkIvCyXovCCCfnboysaTEhcpJV/trOS5r3LJq2xhQkIwf7k8jaRwz6lt6i92Vu/k4vcvprihmDkpc1h40UJiA9Qt6ccyOiaQFfdN49Vvi1j4TQGrd1Vx45R4fnV2IkG+7j/ScE8YAgyEXx1O+NWHxluyVliRVknJ0yWUPFWCMAkCpgSQ8FQCAZMDkFL2eiI8b+Q8HNLB2MixAKwuXs34wePxM6kBLI+mkhflhJoym6haVEVzZjPNG5txtDvbhENmh2CKNJHweIKzs22yt8de0XZ02fliewVvf19MTnkTiaG+vDQvg9lpkeh0nnlMtNBibaGwrpD0QekkBiWSGpbKwosWcmHShR57bvaUyaDnnpnJXDp6MH9fmcerawt598c9XDsxlusnxREXMrBvJT8W70Rvxvw4BluTjcbvGqlf7axdpvtUqvm0hsLfFeI/0R//if5YxljwTfPFK/D0pynwM/lx+7jbAWjsaGTOB3PwNnjz6/G/5ldjf0WUf1RvfLQBQSUvHs5hc9BR3EF7XjttuW207milJauFYW8Nw5JuoW1nG2ULy7CMshB5ayQBUwLwn+KPKdJZo+I7wvO+1MBZy7K9tJElWeV8trWM2lYryeEW/nblKK4YE6WaiPrI4Ve/13x0DTurd1JwdwFmg5kvrv1C4+jcT1yILy/MzeDOGUm8+HU+b32/hze+K+aclDAuGx3FuakRWEye9bNh8DcQclEIIRcd2U/KEGzAkm6h4dsGqj6oOrh80t5JmGPNNHzXQOfeTnxG+OA9xPtgc1RPBZgDWHPjGp5Y+wR/Xvtn/rLuL1w27DKuTbuWC5MuxNvLs/vPedZZ6IGklNjqbXSUdNBZ0klHSQcdezoIuzKMgMkBNK5tJGtm1sHyXmFeWDIsOLqcNSzh14QTfm34gBmW/0y0WW1sKK5jbV41q3dXsbe2DaNex/ShYfxicjxTk0LUFX4faOxo5Lua71i8bDHL85fz480/EukXySNnPwKATqhz9UylRPix8Nox7G/s4P2NJSzetI/7crdhMjjP97NTwpiWFEZsiI/WoWomaHoQQdOdTeSdZZ20ZLXQmtOKKdp5Ybf/nf3sf3P/wfJeEV74DPNh9DejEULQvKUZR6cDU7QJY6TxmN+x46PGs3TeUorqi3hl0yv8K+tffLzrYyxGC3NS5nD+kPO5YvgVffOB+xmXJi9CiFnACzin03tDSvn0Ue+bgH8DY4Fa4Bop5Z7u9/4A3AzYgXuklF+6MlZ3c+CK09HloGFNA11VXVgrrVgrrXRVdREyJ4SwK8LoKOpgQ9KGI9bVmXX4DPUhYHIAltEWhr49FJ8UH3yG+uAVcmSVp87kmT8EXXYHe2vbyC5rIGtfI9lljWSXNmK1OzAZdExMDOHO6UlcMHIQAd79czbbgaDT1klebR6Z5ZlsKt/EpvJNbK3Yil3asRgt/CzhZzR2NhLpF8mk6ElahzvgDAowc/95Kdw3M5nNJfV8kVXOlzmVfJlTCUBMsDcZMUGMig4gLSqA4YP98ffAmdFNUSZMUaYjamdSXkkh+r5o2nPbactvo72gHUer4+AFTtHDRdR/We8srANjpBH/if6M/HgkAJUfVOJoc+AV5kVIWAhPDHmCJyc+ybqadXyY8yFLcpfwwY4PuGDIBQAsz19OYV2hx3RKd1nyIoTQAy8D5wGlwCYhxBIp5c7Dit0M1Espk4QQc4H/Aa4RQqQCc4ERwGBglRAiRUppd1W8fcFhc+Boc07fLnQCY4SzM1zDugZsjbaD79mb7JgTzIReEgpAztU5UACZZGJrsGFrsBFxYwTJ/0gGB2w/f/vBfQijwBhuxGeE84rIFG1iyN+HYI41Y4o1YY4z4xXmdfDk9gr2InJ+ZB8fCW1JKWnqsFHd3EFVcyfVzZ1UNXVS0djBntpWimtaKalrw+5wDl7l7aVnZJQ/86fGMy05lPHxwZi9BtD01hqRUlLXXgdAiE8ILdYWXtn0CjMTZzImcgzrS9cz9a2pOKSzFtDf5M/YyLE8dNZDhDWFccecOzDqPaNDqdZ0OsH4+GDGxwfz2CUjKKpp5bv8Gn4orCFzTx1LssoPlg21GIkP8SUuxJe4EB8i/E2E+5kJ8zMR5mcixNfoEc2qOi8dlpEWLCOP3WE/+YVk2gvb6SztdD72dWIIPPSTXPLXElqzj5xzK+DsAM779jzOG3IeWXOyKLAWUL2pGtrhrXFvscl/E3dPvBuAeS/OY2/XXob4DiHBL4HBfoOJiogiJi6GSEskQSIILx8vt010XFnzMgEokFIWAQghFgGXAocnL5cCj3U//whYKJxH8lJgkZSyEygWQhR0b+9HF8Z7TEXVLXzx/BIaKirBIcEG2CUE6rFfEQiA/sN6zCU6xpVPgC5JTmA21nhB6iNzAMi57yPaKhsQ3amXFBL7MDO2W51ZuvGJ/QTs92X0ntEAbE7cjNeoYBLiLkACWS0raA1txRhoRpoF0ltgjzDiWO68nVG30Epw0GBSoscjLTo2Vq4k1LuJhK067NLO+uTlSIDK7gfAUYNKRvjGkxiQ5ixf/gWxfsOI9kuhw9ZGZtWqIwvLn744UL69q5XNVatICswg3CeWJmsd2TVrnSXlkescvpkhARlE+MTRZK0lu2Ydw4MnE2SOoKa9jNw6Z83R/sr9LKrdikNK7A6J/cC/Dkmc70TM+lBq2kspbsok1mcaAgvlLQWUtmynw+ago8ve/XAc/fHxtk/E4hVAaFA1Rt9CbhlxBSnhwWAspN5agF7nnAW2oAUKdvz0PJmXNg+zwUxmeSY7qnYwf/R8ANbuXUt+bf5Rh+/IveuE7uAQ9d/u+ZaKlgrmjpwLwIr8FZQ2lR4sm1uRS97mvCPWtxgtXJt2LeC8+rLarVw27DIAFucsPpggHE+EbwSXD78cgA93fEigOZALkpxXc29tfYsOW8cx13NIB132LhKDErl02KUAPPrNo2QMyuDy4ZdjtVu5ecnNdNo6abY209jRSFNn0xEPieSByQ/w7PnPIhA8uOpB/jrzr4yJHENycDJ/mvYnhoYOZWzkWJJDkg82Ca1Zs0YlLhoRQjAkzMKQMAs3TokHnGMaZZc1kru/mT01zguA7wtq+HjLsc8dH6Mei8mAxWzAz+yFn8mAxWTA5KXDS6/DaNBh1OswGQ49Nxp06HUCvU6gEwKdgPySLso3lKAToBMCITj4vuheduD5cT/PCT/rCY/Eaa13xFtxQJwO8AacNz0U7+z+kn47GurtiDob1Nmhzkatv56V3e/rDQ6CqmKobWnDWiv59dZ7aJnOofeXG2gZ3MKykGXUW+qPGcuIfSN46b2Xwazjb7OeJjguivk3PgkdDl597jdYDVa8MGLACyNGDCl+GFL9MXTqMaxqY7AthgmW6UT9KZZhg/xPdLB6nSuTlyhg32GvS4Gj55s/WEZKaRNCNAIh3cvXH7XuT7pZCyFuA24DiIiIYM2aNb0V+0FfFFpZUvd38uM3//TNA6lULAQHR/BI8XvY9PDamNdp9m7D9B9nyM1j/k6dZc9P1z8wpcUs8O8Yxojq57B6wY7Ql9ATRdgHztv29k18Eoc4aiI1K7Dp0EufyimEbX/YWd58H772aQR33YGkixLvX5/0c/rZZh9W/i4Cu35BgO3n2EQlZea7Trr+keXvIcR6Hxb7uXTqdrHf9LuTrn90+fDOx/F2jKVV9z01pr8eKnic6VgirY8TIJzl9+n/Siov46eLp5Y1FDheOVRQAMf4zVuY/gapAeF8Vp7JiwUvck9SBgHNdbxe9Drv73v/pPEH1wQT4BXA60Wvs7h0MfEN8QA8vftpvqw8cYunQRhIbEw8WH5bwzYG1QwC4NHtj5JZn3nkCkfmLkSYIhhc67x1fcH2BbTZ2gjc70ysH8p8iOLW4hPuP9UvlaBKZ9v9HzL/QLR3NKZSZ7v9b77/DU22E0/iNy10GgH7AwB4dcOrnBN2DkGVQdgcNr7O+xqDMOBr8MVX70uwIZgY7xh8/Hzw1fvi7+VPQnvCwb/dZVOX4WPzOfh6hpgBtVBRW0EFFQf32dLS4pK/d3fS346BAIYBw0Jwfoujx2r3ockqaeyUNHQ6/222StptkjabnQ6bjfb2DvY3O5fZHNDlAJtDdv/rfJxwAP+d2X3w6fqJJpwdLQCGdj+O9m/n90VE7DxCOq9lUDnobZ04RANV/rWUhdZhF3UklrVg6fRnRXonRpugyuxgb2Mz3/87E6MVWsI20myux6a3YtN1YTN0OS/eD1T0x8DEvLMIXDqc91PLuCG1b4fFEK6a10EIcRUwS0p5S/frG4CJUsq7Diuzo7tMaffrQpwJzmPAeinle93L3wRWSCk/Ot7+xo0bJzMzM4/39mmrau4gc98uWrsOTSN/eFYtuvNoL52RpODhCAElTUU4HA4Sgpzz1xTV52F1dB4se3C9w7bjY/AmLmAIAMUN+Rj1RqL9ncOX59flkpW9jbSRaQf3L47K+v2Mfgz2i0EIQUHdbvxMAUT4RoKUFNQfmh7+yCuCQy8CTIEMskQipaSgPpcQ71CCvUPpsnext7HoJ1cSh1c1CiDYO+Rg+ZKmYsJ9I/A3BdBh66C8ufSw9Y48bge2EuYTjp/Zn/audipayoiwRGLxstBibaGqtRIhYOuWLWSMGYMAvLqvxgw659VYpF8kPl4+tFhbqGypJNo/GpPBRGNH40lrHgCi/KMw6o00dTZR115HjH8Mep2e+vZ6mjpPPgNvtH80ep2eho4GmjqbDo4pUttWS1tX20/KH11VG+3vnDKgrr2OTlsnkX7Opryatho6bZ0Hy/3w4w9MmTzliPX1Qk+EJeLg/hzSQZivc6TU6tZq7CdpbfXSeR0ccbamrQaDzkCgOfDg+kfXFB38DAiMeiNmgxmToW+/uNasWcP06dP7dJ/9jaccAyklNofEanNgc0iklDgkOKTku++/Z/LkKTgOLHPIQ8+ls6z9BCP+H+/cdu73RDH1/jbPRGZmJuPGuXZQOyklNmmjy95Jl8OGQ9rRCz3+pkCCfY0uGz1cCLFZSvmTD+fKmpcyIOaw19Hdy45VplQIYQACcHbc7cm6fSLcz8xFqRmntM7wyNFHvB426NROquSII4eGTgofh/f+FqYPP7ri6tgSQsce+Trs1PYfG3Lk/hPDTuXzexMXkn7E64SQoOOW/ikzccGHyodhIiHE+cPaVljFhJgTzzptMVqwBB9qYw4wBxBgDujx3v1N/vibDlV/BnkHEeTd8/gDzYEHf/jB2ZcjhJ4PRR/sHXzE61Cf0CNeh5nCTjjWw9HD3h9IYnrqJ/s7xfUVpbcJIfDSC7yO0U8m0KTzmNGAT6QmX8/IqJ5/zw0Eruw1tQlIFkIkCCGMODvgLjmqzBLgxu7nVwGrpbMqaAkwVwhhEkIkAMkcamRRFEVRFMWDuazmpbsPy13AlzhvlX5LSpkjhHgCyJRSLgHeBN7t7pBbhzPBobvcYpyde23Ane5+p5GiKIqiKL3DpeO8SCmXA8uPWvboYc87gKuPs+5TwFOujE9RFEVRFPcz8G+2VxRFURRlQFHJi6IoiqIobkUlL4qiKIqiuBWVvCiKoiiK4lZU8qIoiqIoiltRyYuiKIqiKG5FJS+KoiiKorgVl81t1NeEENXAXq3jcKFQoEbrIDSmjoE6Bgeo46COAahjcMBAPg5xUsqfzFMyYJKXgU4IkXmsyak8iToG6hgcoI6DOgagjsEBnngcVLORoiiKoihuRSUviqIoiqK4FZW8uI/XtA6gH1DHQB2DA9RxUMcA1DE4wOOOg+rzoiiKoiiKW1E1L4qiKIqiuBWVvLgZIcTdQojdQogcIcTftI5HK0KIB4QQUggRqnUsfU0I8Uz3ObBdCPGpECJQ65j6ihBilhAiVwhRIIR4SOt4tCCEiBFCfCOE2Nn9PXCv1jFpRQihF0JsFUJ8oXUsWhBCBAohPur+PtglhJisdUx9RSUvbkQIMQO4FEiXUo4AntU4JE0IIWKA84ESrWPRyEpgpJRyFJAH/EHjePqEEEIPvAxcCKQC84QQqdpGpQkb8ICUMhWYBNzpoccB4F5gl9ZBaOgF4L9SymFAOh50LFTy4l7uAJ6WUnYCSCmrNI5HK88DDwIe2WFLSvmVlNLW/XI9EK1lPH1oAlAgpSySUlqBRTiTeY8ipayQUm7pft6M8wcrStuo+p4QIhqYDbyhdSxaEEIEAGcDbwJIKa1SygZto+o7KnlxLynANCHEBiHEt0KI8VoH1NeEEJcCZVLKLK1j6SduAlZoHUQfiQL2Hfa6FA/80T6cECIeyAA2aBuJJv6B8yLGoXUgGkkAqoG3u5vO3hBC+GodVF8xaB2AciQhxCpg0DHe+iPO/69gnFXF44HFQohEOcBuGTvJMXgYZ5PRgHaiYyCl/Ly7zB9xNiH8py9jU/oHIYQF+Bi4T0rZpHU8fUkIcTFQJaXcLISYrnU8GjEAY4C7pZQbhBAvAA8Bj2gbVt9QyUs/I6U893jvCSHuAD7pTlY2CiEcOOe0qO6r+PrC8Y6BECIN59VGlhACnM0lW4QQE6SU+/swRJc70XkAIISYD1wMzBxoyesJlAExh72O7l7mcYQQXjgTl/9IKT/ROh4NTAUuEUJcBJgBfyHEe1LK6zWOqy+VAqVSygO1bh/hTF48gmo2ci+fATMAhBApgJGBOxnXT0gps6WU4VLKeCllPM4/3jEDLXE5GSHELJzV5ZdIKdu0jqcPbQKShRAJQggjMBdYonFMfU44M/c3gV1Syr9rHY8WpJR/kFJGd38PzAVWe1jiQvf33j4hxNDuRTOBnRqG1KdUzYt7eQt4SwixA7ACN3rQVbdyyELABKzsroFaL6W8XduQXE9KaRNC3AV8CeiBt6SUORqHpYWpwA1AthBiW/eyh6WUyzWMSdHG3cB/upP5IuCXGsfTZ9QIu4qiKIqiuBXVbKQoiqIoiltRyYuiKIqiKG5FJS+KoiiKorgVlbwoiqIoiuJWVPKiKIqiKIpbUcmLoiiKoihuRSUviqIoiqK4FZW8KIriFoQQtwshtnU/ioUQ32gdk6Io2lCD1CmK4la65/VZDfxNSrlU63gURel7quZFURR38wLOuWxU4qIoHkrNbaQoitvonk07DrhL41AURdGQajZSFMUtCCHGAv8Cpkkp67WOR1EU7ahmI0VR3MVdQDDwTXen3Te0DkhRFG2omhdFURRFUdyKqnlRFEVRFMWtqORFURRFURS3opIXRVEURVHcikpeFEVRFEVxKyp5URRFURTFrajkRVEURVEUt6KSF0VRFEVR3IpKXhRFURRFcSv/D3K2Oqdp01w3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-7, 7, 0.01)\n",
    "\n",
    "# Mean and variance pairs \n",
    "parameters = [(0, 1), (0, 2), (3, 1)] \n",
    "plot(x, [normal(x, mu, sigma) for mu, sigma in parameters], \n",
    "    xlabel='z', ylabel='p(z)', figsize=(9, 5), \n",
    "    legend=['mean %d, var %d' % (mu, sigma) for mu, sigma in parameters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, changing the mean corresponds to a shift along the $x$ axis, and increasing the variance spreads the distribution out, lowering its peak.\n",
    "\n",
    "One way to motivate linear regression with the mean squared error loss function is to formally assume that observations arise from noisy observations, where the noise is normally distributed as follows\n",
    "$$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon, \\text{where} \\epsilon \\sim \\mathcal{N}(0, \\sigma^2).$$\n",
    "\n",
    "Thus, we can now write out the likelihood of seeing a particular $y$ for a given $\\mathbf{x}$ via\n",
    "$$p(y|\\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right).$$\n",
    "\n",
    "Now, according to the maximum likelihood principle, the best values of $b$ and $\\mathbf{w}$ are those that maximize the likelihood of the entire dataset:\n",
    "$$P(Y\\mid X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)}).$$\n",
    "\n",
    "Estimators chosen according to the maximum likelihood principle are called `Maximum Likelihood Estimators` (`MLE`). While, maximizing the product of many exponential functions, might look difficult, we can simplify things significantly, without changing the objective, by maximizing the log of the likelihood instead. For historical reasons, optimizations are more often expressed as minimization rather than maximization. So, without changing anything we can minimize the `Negative Log-Likelihood` (`NLL`) $-\\log p(\\mathbf y|\\mathbf X)$. Working out the math gives us:\n",
    "$$-\\log p(\\mathbf y|\\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2.$$\n",
    "\n",
    "Now we just need one more assumption: that $\\sigma$ is some fixed constant. Thus we can ignore the first term because it does not depend on $\\mathbf{w}$ or $b$. Now the second term is identical to the squared error objective introduced earlier, but for the multiplicative constant $\\frac{1}{\\sigma^2}$. Fortunately, the solution does not depend on $\\sigma$. It follows that minimizing squared error is equivalent to maximum likelihood estimation of a linear model under the assumption of additive Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 From Linear Regression to Deep Networks\n",
    "So far we only talked about linear functions. While neural networks cover a much richer family of models, we can begin thinking of the linear model as a neural network by expressing it in the language of neural networks. To begin, let us start by rewriting things in a 'layer' notation.\n",
    "\n",
    "##### Neural Network Diagram\n",
    "Deep learning practitioners like to draw diagrams to visualize what is happening in their models. In `Fig. 3.1.2`, we depict our linear model as a neural network. Note that these diagrams indicate the connectivity pattern (here, each input is connected to the output) but not the values taken by the weights or biases.\n",
    "\n",
    "<img src=\"images/03_02.png\" style=\"width:400px;\"/>\n",
    "\n",
    "Because there is just a single computed neuron (node) in the graph (the input values are not computed but given), we can think of linear models as neural networks consisting of just a single artificial neuron. Since for this model, every input is connected to every output (in this case there is only one output!), we can regard this transformation as a `fully-connected layer`, also commonly called a `dense layer`. We will talk a lot more about networks composed of such layers in the next chapter on multilayer perceptrons.\n",
    "\n",
    "##### Biology\n",
    "Since linear regression (invented in 1795) predates computational neuroscience, it might seem anachronistic to describe linear regression as a neural network. To see why linear models were a natural place to begin when the cyberneticists/neurophysiologists Warren McCulloch and Walter Pitts began to develop models of artificial neurons, consider the cartoonish picture of a biological neuron in `Fig. 3.1.3`, consisting of `dendrites` (input terminals), the `nucleus` (CPU), the `axon` (output wire), and the `axon terminals` (output terminals), enabling connections to other neurons via `synapses`.\n",
    "\n",
    "<img src=\"images/03_03.png\" style=\"width:400px;\"/>\n",
    "\n",
    "Information $x_i$ arriving from other neurons (or environmental sensors such as the retina) is received in the dendrites. In particular, that information is weighted by synaptic weights $w_i$ determining the effect of the inputs (e.g., activation or inhibition via the product $x_i w_i$). The weighted inputs arriving from multiple sources are aggregated in the nucleus as a weighted sum $\\displaystyle y = \\sum_i x_i w_i + b$, and this information is then sent for further processing in the axon $y$, typically after some nonlinear processing via $\\sigma(y)$. From there it either reaches its destination (e.g., a muscle) or is fed into another neuron via its dendrites.\n",
    "\n",
    "Certainly, the high-level idea that many such units could be cobbled together with the right connectivity and right learning algorithm, to produce far more interesting and complex behavior than any one neuron alone could express owes to our study of real biological neural systems.\n",
    "\n",
    "At the same time, most research in deep learning today draws little direct inspiration in neuroscience. We invoke Stuart Russell and Peter Norvig who, in their classic AI text book `Artificial Intelligence: A Modern Approach`((Russell & Norvig, 2016), pointed out that although airplanes might have been inspired by birds, ornithology has not been the primary driver of aeronautics innovation for some centuries. Likewise, inspiration in deep learning these days comes in equal or greater measure from mathematics, statistics, and computer science.\n",
    "\n",
    "##### Summary\n",
    "+ Key ingredients in a machine learning model are training data, a loss function, an optimization algorithm, and quite obviously, the model itself.\n",
    "+ Vectorizing makes everything better (mostly math) and faster (mostly code).\n",
    "+ Minimizing an objective function and performing maximum likelihood can mean the same thing.\n",
    "+ Linear models are neural networks, too.\n",
    "\n",
    "##### Exercises\n",
    "1. Assume that we have some data $x_1, \\ldots, x_n \\in \\mathbb{R}$. Our goal is to find a constant $b$ such that $\\sum_i (x_i - b)^2$ is minimized.\n",
    "    + Find a closed-form solution for the optimal value of $b$.\n",
    "    + How does this problem and its solution relate to the normal distribution?\n",
    "    \n",
    "2. Derive the closed-form solution to the optimization problem for linear regression with squared error. To keep things simple, you can omit the bias $b$ from the problem (we can do this in principled fashion by adding one column to $X$ consisting of all ones).\n",
    "    + Write out the optimization problem in matrix and vector notation (treat all the data as a single matrix, all the target values as a single vector).\n",
    "    + Compute the gradient of the loss with respect to $w$.\n",
    "    + Find the closed form solution by setting the gradient equal to zero and solving the matrix equation.\n",
    "    + When might this be better than using stochastic gradient descent? When might this method break?\n",
    "    \n",
    "3. Assume that the noise model governing the additive noise $\\epsilon$ is the exponential distribution. That is, $p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$.\n",
    "    + Write out the negative log-likelihood of the data under the model $-\\log P(Y \\mid X)$.\n",
    "    + Can you find a closed form solution?\n",
    "    + Suggest a stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint - what happens near the stationary point as we keep on updating the parameters). Can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Linear Regression Implementation from Scratch\n",
    "Now that you understand the key ideas behind linear regression, we can begin to work through a hands-on implementation in code. In this section, we will implement the entire method from scratch, including the data pipeline, the model, the loss function, and the gradient descent optimizer. While modern deep learning frameworks can automate nearly all of this work, implementing things from scratch is the only to make sure that you really know what you are doing. Moreover, when it comes time to customize models, defining our own layers, loss functions, etc., understanding how things work under the hood will prove handy. In this section, we will rely only on `ndarray` and `autograd`. Afterwards, we will introduce a more compact implementation, taking advantage of `Gluon`'s bells and whistles. \n",
    "\n",
    "### 3.2.1 Generating the Dataset\n",
    "To keep things simple, we will construct an artificial dataset according to a linear model with additive noise. Out task will be to recover this model's parameters using the finite set of examples contained in our dataset. We will keep the data low-dimensional so we can visualize it easily. In the following code snippet, we generated a dataset containing $1000$ examples, each consisting of $2$ features sampled from a standard normal distribution. Thus our synthetic dataset will be an object $\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$.\n",
    "\n",
    "The true parameters generating our data will be $\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$ and our synthetic labels will be assigned according to the following linear model with noise term $\\epsilon$:\n",
    "$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$$\n",
    "\n",
    "You could think of $\\epsilon$ as capturing potential measurement errors on the features and labels. We will assume that the standard assumptions hold and thus that $\\epsilon$ obeys a normal distribution with mean of $0$. To make our problem easy, we will set its standard deviation to $0.01$. The following code generates our synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"Generate y = X w + b + noise.\"\"\"\n",
    "    X = np.random.normal(0, 1, (num_examples, len(w)))\n",
    "    y = np.dot(X, w) + b\n",
    "    y += np.random.normal(0, 0.01, y.shape)\n",
    "    return X, y\n",
    "\n",
    "true_w = np.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each row in features consists of a 2-dimensional data point and that each row in labels consists of a 1-dimensional target value (a scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [2.2122064 1.1630787] \n",
      "label: 4.662078\n"
     ]
    }
   ],
   "source": [
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By generating a scatter plot using the second feature `features[:, 1]` and labels, we can clearly observe the linear correlation between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f26282b27b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEvCAYAAADLvP+CAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2df3Cc9X3n319si1pyaiR5owIWli0J+UTGdUA4rrHjGIkUdxhobs5puN7ho5c6mWmISnJ3ra/M0fTo0LuWckrupsVtQ801pcXTpjC+0AYLFyw7BsvUdY2CkWTJ2IYaeWVTrPWwlvjeH7vfR9/97vd59tnd59ndZ/f9mmEk7T77PN/nMfN97+e3kFKCEEIIiSLXlHsBhBBCSKFQxAghhEQWihghhJDIQhEjhBASWShihBBCIgtFjBBCSGRZWO4F6Cxbtky2tbWVexmEEEIqiKNHj16QUsZs71WUiLW1tWF4eLjcyyCEEFJBCCFOu71HdyIhhJDIQhEjhBASWShihBBCIgtFjBBCSGShiBFCCIksFDFCCCGRhSJGCCEkslDECCGERBaKGCGEkMhScyI2PZPEU6+MY3omWe6lEEIIKZKaE7E9w2fw+ItvYc/wmXIvhRBCSJFUVO/EUrCtpzXjJyGEkOhScyLW1FCHr2xuL/cyCCGEBEDNuRMJIYRUDxQxQgghkYUiRgghJLJQxAghhEQWihghhJDIQhEjhBASWShihBBCIgtFjBBCSGShiBFCCIksFDFCCCGRpapFjB3rCSGkuqlqEWPHekIIqW6qugEwO9YTQkh1U9WWmOpY39RQF/i56aokhJDyU9UiFiZ0VRJCSPmpandimNBVSQgh5YciViAcrkkIIeWH7sSAYayMEEJKB0UsYBgrI4SQ0lFz7sTpmST2DJ/Btp7WULIWGSsjhJDSUXOWWNiWUphp/YQQQjKpOUuMlhIhhFQPNSdizCokhJDqoerdicwWJISQ6iUQERNCfFcI8b4Q4oT22m8KIc4JIY6l//u5IK6VL8wWJISQ6iUod+KfAvjfAJ4xXn9SSvl7AV2jIBgDI4SQ6iUQEZNSviqEaAviXEHDGBghhFQvYcfEviaEOJ52NzaGfC1CCCE1Rpgi9gcA2gGsBfAegCdsBwkhdgghhoUQw1NTUyEuhxBCSLURmohJKc9LKeeklB8D+CMA61yO2yWl7JFS9sRisbCWQwghpAoJTcSEENdrf34BwAm3YwkhhJBCCCSxQwjxLIDPAVgmhDgL4FEAnxNCrAUgAUwC+EoQ1yKEEEIUQWUn3m95+U+CODcpjLAbHRNCSCVQ9R07ahUWeRNCaoGa650YBYKwoljkTQipBWreEqvE3opBWFEcCUMIqQVqXsQq0e3W192CLV0x9HW3lHsphBBS0dS8O7ES3W77Rs5j/8kprF91Hu2bl+Q8nkkchJBapeZFrBJ7K+YrrMqaBFBx90IIIWFS8yJWieQrrJVoTRJCSCmgiFUBlWhNEkJIKaj5xI5CqcSsxmKpxnsihFQ3FLECqcSsxmKpxnsihFQ3dCcWSDXGoarxnggh1Y2QUpZ7DQ49PT1yeHi43MsghBBSQQghjkope2zv0Z1IQoHxNUJIKaCIVRGVJByMrxFCSgFjYnlSyd0xKqnomfE1QkgpoIjlSSUJhUkphSOXmLN2jRBSCihieVLJFkYphaNcYl7JljAhpPRQxEhObMJRLjGvZEuYEFJ6KGJ5UoubqO2ey+UurGRLmBBSeihieVJtm6gf99y2nlYkkrNIJOcwPZP0dOMV6+5jrI0Qkg9Msc+TapuY7CcVvqmhDvV1CzEwOJp1nJnWX2xqvfr87kOTFVMuQAipXKrOEquVwH9Q9+nXsnQ7znQ19nW34PCpeMFTqdX5E8nZmnPbEkLyp+pErFZiVkHdp+mecxNHNzeeKW5eU6n9CK+6zvRMEvV1C6vGbUsICYeqE7Fqi1m5EdZ95iuOprh5rSufc4cd+6oVi52QaqfqRKxWAv9B3ae5mRcrjl7r8jp3MaJSyGdrxWInpNqpOhEj+WFu5vmIYz7i4XXs9EwSD/35Gzg4HkciOYuH7+oq6h78UCsWOyHVDkWswii1m6uYzTwf8fA6ds/wGRwcj6f/EnmvQ78Hv8+vVix2QqodiliJybXJltrNVcxmno8Aeh2r6tAAge0b2vJeh34PT70yTjchITUERazE5BKpKLm58hFA27G6oJsuxEIt0nwKswuFSSGEVA4UsRKTS6TCcnNV4sarBD2RnHXS6dXadh+awMDgWN4xMlWY/fiLb6G+bkEoz5JJIYRUDhSxElOuWIzbxltOcZsvbJ7LWNv0TBJHT19KH1VcjCwMomQtE1LtUMRqBL8dNxRBiluuAupUYfMCZ217hs9gaOwCtnTFfMfIzGuE+UWBSSGEVA4UsRrBb8cNRSEuMzexynUur4Jp1n0RQrygiNUQSmRub2vCd14exSP3dKM9tiSwBBM3Icn3XIVYOnTxEVKbCClludfg0NPTI4eHh8u9jIogjFiVSj9vjzVgfGoGW7piePrBdYGcG6jM5BFCSPQRQhyVUvbY3gtkFIsQ4rtCiPeFECe015qEEC8JIUbTPxuDuFatUMhIE3Msism2nlbs3Loav/tvfhpbumJ46M7OQMedBDGmJtc9BEEprlEKquU+CCmGoOaJ/SmAu43Xfh3AoJSyE8Bg+m/iEyU4hbjzvvncMevGpkTm1hWNePrBdXjl7SlndpcNr00yrA200Hlk+nrU7+NTl7PWOD2TxDefO+Z6jSgJQ7Gz2wipBgKJiUkpXxVCtBkv3wfgc+nfdwP4ewC/FsT1aoFCehiqWV77T05hz/AZH5+Xxs9McrWKKiSRQl/rvpHzWa7HQmNbaj2HT8WxZvlSDAyOOc9CX+Oe4TPYf3IKW7pi1jZVUUoQYRyQkHATO1qklO+lf/9nAIVNSSQ50TfeJ7641tmUc7F9w0rPmV25WkW5vafjJhI2gQEKT1/f1tPqnHPN8uuwc+tq9HW3YP2q8xlrNDMfzTZVQQtDmHFCpvoTEmBiR9oS2yul/FT670tSyuu09y9KKbPiYkKIHQB2AMBNN9102+nTpwNZTy1RyQkVSiS2dMXwxBfXAoCnJeaGlwU3PZPE7kOTuJKcxeK6hdi+oQ1NDXW+nkuxnfhzfV7d/86tqyk4hBRI6IkdLpwXQlyfXsD1AN63HSSl3CWl7JFS9sRisRCXU73YEirKEc+ysa2nFVu6Yo6LU80sswmYuS79b2XBPbZ3JCvut/vQJAYGR7HrwATq6xY45/QTM8onGcV2vlzXKCS26Zcoxe8ICYsw3YkvANgO4HfSP58P8VrEoJh4VpCWXVNDXZaL0+365uu66/GRe7oBAH3dLbg69yb2n5zC7kOTePium6Fiene0NzvXmJ5JIpGcQ39vR2ACYnM1lqsXJsACb0KAgERMCPEsUkkcy4QQZwE8ipR4PSeE+I8ATgP4YhDXqiWKEZNUN/c5JJKzWd3cc228QW+OXh05zDWbP1Wc6+rcCdy2ogmN9XW4bcV1GBq7ACVeemxPt8IGBkexc+vqwFysNkEqZ1yKiR2EsNi5oik2nlLo53XxBOBLSIOy3szzqJR4lQSiXHNBxroIIZWNV0yMbacqmGK/aRf6eduQyURyzmnSG+YwT/M8yh25+9AErlz9OD08M/sapmiFbSFRJAmpDChiFUyxG3EQG7kSwPjljzAwOOo63yso15btPE0NdXj4ri5HUOvrFrrG+XKJbaF41ZMVahm6CSEFkhD/hJmdSCocP9ltSggX16nvO/b5XsW2nFJrAeZFwVyXyvTr627JWrd6D5ChdOMwsxD1rEM/WZD5ZDayEwch/qElVkIq7Ru2XxdgatOX6O/t9D3fyw/689DXAsC6LiWU8y7OzInQ87PJsgu4zdhasV3ydSvXjxWaT2YjEzYI8Q9FrIQEnfVXrCj63SxTmX5jnpl+hRQNJ5KzGBgcc11Lrk4i5kRowN2FqrebUpZcPs9NP28h8bd8MhvZiYMQ/1DESkjQ37CLFUW/m6W+bj+DL3PFiNSx/b2djkvOXIuyqmxiY06EdhMlfa36PRT73FifRUjlwJhYCQliVIlOmN0gdPR12+I1qcLiWfT3drrGiPR4lFr3vWtv8LyuV2xofOoyvvncMdze1uR08XCLLe0+NOEMA1WfcYut2dZrcntbE9pjDbi9rSnnswsCduYgxB1aYhFGtXDK16VYbBG1/hPIdje6HaNbL5mxrbn0URLbN6x01uRluT62dwT7T07hnemEM+BT79ah+iwC825HNQwUAJ5+cF1W81+3GJ1ubU3PJPGN545hMp7A7790En/25fV5Pb9CoOVHiDsUsYhTyAZXzKZoc0F6JT24HaP/rsfG9PR5L5FOtaEawUN3duLI5HRWtw7Vrko1Ha6vW4Db25rwnZdH8cg93VnWo/lc3AR0z/AZTMYTAIDu65c6r4eZtMNED0LcYceOiFPI5hnWhlvoeVUXetMSA4AnXzqJgcEx9Pd2WOvT3M6nMhHdupU8+dLbGBgcRX9vZ7r/YspF+djeETxyTzfaY0vyWiu71RMSHuzYUcUUkskWVvZboRZeqpj5Zpd3hfFzHjfRtDUdziZ7IOi+kfPYf3IK61edR/vmJdZruK21XNZSpZVtEFJqKGLEF342y1wbeSHzuLZvaHM6cJh4iaZNqPVr6U2D9RhaIjmLRHLOaZrsV5jLlRbPeBmpdShiFUalfrP2s1nm2sht58h1Xq9zeomm7TnakkvMIuj6uoXp1lYLQpn0bK7RzY3qF8bLSK1DEaswKuGbtU0AtvW4j3bxi63eTGUQ2jps5BJzL4GzPUe3rElVBL2tpxUXE0kcPhV31uUnA7TQCdJqXAwAaz9I2zl2H5oAIJzp1SyMJrUORazCqIRv1jYBaGqoQ33dAtcGvH6wdcdX1zALm/MRc9v4FttAzFxZk00Nddh9aBL7T05hzfJ3ndiXme1oCpWftdqKwVPuyzkA0te/typlAOBYioTUOhSxCqMSvln76elXbAd28xpmF3o3C02/zu5Dk7iSnMXIex9iaOyCIzLKwunv7fBlzemicvjUBQDAlfTIF7UGNZxz96HJrC75+fZO1AXNliTi9gxT1vAsAEH3ISFpKGIkCz89/UxLSlFoIoReM+bn87orLnW+Rdh/ckoToziuXP0YA4OZ41kAZMWhdEvrtYmLAKB17c/MdlTr060yPy7HfBoGuz1DNZKGEDIPRYwUhB9rLR9ydaFXZGYSzqUtsX/B0FjciWupONea5Uuxc+vqDGFMJOc08RMAJK4kP0Z/byfuXXsD1ix/FymBa3Nd3/GzHziCaUsOKaQZsE4luJQJiQoUMeLg1xXodVzYgzxtrjhzPSoJRY81KWFMJUYAt950Hf76jbM4c/EKAGDn1tVorK/LOVDTVoOmRHNjxzLPxBe/z7cSXMqERAU2AK5y8mkeazbcdftsGEMb9Wt5rVlveqwP0tQbK6sklIHBMewZPpPh7rt37Y3YuXU16usWOALW1lzvNAf2ui/zekDKrdrX3YKdW1fjthXXOde03Ve5hl2G3UCYDYpJOaElVuXkk+XnlmxhfjYMd5efoZiAv7icWpteuGzeS193Cx59/gSuzkm8NjGN33/pbQyNXcDGjmUZo10uJpJOK6p9I+czzmGe0+YKzdWPsZhWXcqtum/kvOfnwy7bqISyEFK7UMSqnHwExy3ZwvxsGO4u/Vp6rZZXQbDXvaWssfnCZfPYxvo6bOqMOSIQv/wRhsYu4LYV12WIlcpKTM6eQE9bU0bavvnTluBhpvCbzy2f6dr6efVklFyxuLBjbIzhkXJCEatyihGcUsZm9GupGNP6VecBwLUg2Jy2rFLuF9ctxL1rb8ioFTMFZvehCQwMjiF++SM0L7kWd3/qeoy+fxn3rr0RABwR7etuwdW5N3F1LpUQok+3tomWOm8iOYuH7+qyHqM3GvYrAKbYqeP7uluwftV5z8+H/e/IGB4pJxQxEjjFts4yN3avgmB1rcysQ+Bv/uEcTk8n0N/baZ0RphoKv3jin3Hm4hVs7GjG0FgcV+dOQErg4Hgc61edx1c2t2NT5zI8/uJb2NjRnJW4kW1Jpc57+NQ0nnzpbWzf0JYlbGoWGjCCpx9cl/d0bZ3GegoIqW0oYiRwiomR6LEeJT7uHe7nr/XA+hVoa67HT/7EIhw/9wFOTyewsWMZAOkakzp+9pLjiksk53BHe0rIAGQM2dTFNCWUwnFRmsds39DmnPe1iWnU1y2A2YlfzUJTc80KEXzGoQhJQREjgVNMjCRXrMec+aWucWB0CpPxBD6zshFtzfWYjCfQff0ncOXqx7ijvdnpAKJQqfJff/YNDI3F8cY7l7CxoxkPrF+B8anLeOSebqeFlRLVF469i/7eDujCqBoJm+dVPQ77ulvwl6+/g40dy3Dv2hsAAO2xJU6afvxyErsOnEL8chLNS+oyWmfZYmDqmoxDEZKCIkYCp5gYSa5Yj+mKU9fSsw0n4wls6Yphcd1Cx8W4byQVX9OFoKmhDt++/1bsPjSBo6cvYmgsjkULrsHB8Tj2jaRmiinxeG74DManZpz0fj0L0cwUTAlmqpD6hWPnsOvAhLMGNadMnXdFUz0AYOS9Dxwr0Jb5qGrfdHcmLTBCKGIkB6UeDaNvzmrD19fx0J2dAICH7uzMaBjcHluCTZ0xPP7iW9jSFcMTX1yrnVVmWES6VaZaOelCpIvntp5WHBidwtBYHHe0N1uzDHcfmsTA4CgOjF5wejgqK3LHplXY2NGM7uuXZgiy3o9xS1cMj9zTjReOveuIlC3zUTVgPn72A2sjYr94/ZtW6iggQtygiBFPKiX2otaxc+tqPP3gOmuNmJnODsx39XjqlfEMi6ixpy5rYrNNPJsa6nDbiiYMjcXR09ZkHeapmgWvWlaP21Z04kpyFrFPXIsfjccxfHoab7xzCZs6Y1mfVYKsXKPmlADzed/e1uT0iLQ1Is73WerPzs97fqAIklJDESOeFBt7KbaY11Zv5bauXAM09Q7wttEoepG3vl41XVovgtY/r5oFNzZcm+4UMorr6hfiUmIWZy5eQXusIcP6U5/d0hVzSgnaNy+xugx1vvPyKKZnrqI91gAVl9ObG/t9vl7/psX+e1fKlx5SO1DEiCfFxl4K3dTMz5nrMP/OJZZmB3h9s87VLaSpoQ593S3Y8cwwxqdmAMDplN/X3ZLRcxFAhjuxtXExxqdmMuJhSqyuJGexZvl1vl2GelZj6poLs7r+FztMtNh/by8RpJVGwoAiRkKlkG/2qaGWs+jv7fT9uXzF0jYaRc9AtCWUjE/NoD3W4AifsqJM15+enXjv2huctlDq3lK9EyV2HZjAlq4YLibmN3c9Vqa65KvP7Rs5nyFstlZXuQZ4BkGhDaBppZEwoIgRK0F9ay7km72aYKx3x8hFMRaAWuNTr4xndeVQn+/85BJcnZP4xl03Owkgbtczk0XUMU++9DaGJ6dxcDyO/t5ObOxoxv6TU7g6dyIjM9Hskq+eiduMMV3oEslZbOxYliWCQVKoGLEsgIQBRYxYKXSjCkL88tns9OuZ7Z3mu3nMYmBwLOe9uF13z/AZ7DowgZ1bV+PI5LTv56KyFlXcSqX7b+mK4d61N+Do6dQAzu7rl+K2FY1Os2Kb8OvuSzeU+G/saLZak+bzKmQiN1D8zDhCgoQiRqwUulEF4TLKZ7Pzimep9/p7O536rlzXNYUQyBYQm5goEbi9rQnfeXk0Hb+S6XelEwdTjYz3DJ9Jd81vxuK6BbiSnMOuA6cAwNqhZN/Ieac+TrkJbckvyhVpy4bs627R6uzsE7n9uCEpRqSSCF3EhBCTAD4EMAdgVkrZE/Y1SfEUulGV2mVku54e4zp8Ko57196A9lhmzZmKWW3f0Jax2asJzcp60jd+1ZBY/d7YU+ec50pyFrsOTDjdQq7OvYnbVjRix6aVUO2mdHFS2ZJHT1/EwOAoNnY0p9+RVqvIFiuzJb94uSL1ujTz38c8/7aeVuszUs+JCRqkUiiVJbZFSnmhRNciZaQSvqVPXpjBN587htbGeuw/OYU1y89lZCYqtxsA1NctcBIklIBt7FjmxK7cNn5lsanzpPo0Ap/tjGHlsgRaGxdjYHAUn1nZiNcmUm5DXcTUqJihsbhT7KwSQJTomOnzpkCZP93ERYn5Q3d2Yv2qZqv4mOe3PSP9+TFBg1QKdCeSSOC2Qdvcie2xBoxPzaCtuT79qsg4R193C+KXP8LIex86bkGVbbilK4Y1y5diYHDMEZc1y9+Fcg2aWY3qPN+462Zs6lzmCMrXn/2HjGsfPT2dVftlFmfrKfgAnPT5RHLWyUBUgqsmSgNwshvN2J8eE0xZj81Wd6nCvDe9rk6HCRqkkiiFiEkAPxRCSABPSSl3leCaJGLkclHlM2VaxaUeurMTRyans9LPAaB5ybUYGpvAY3tTMSbzPEo09Notc54ZAIy+fxlDYxewqXNZxsTpobELaI814NfuXo3vvDxqzRZUoqGL0gvHzkGl5r9w7Bz6eztxJTmHgcG3nFEupntQ9XU0Y38qsWTHZ1c5r/uNfZl1deZ7QVlgdE2SYimFiG2UUp4TQnwSwEtCiLeklK+qN4UQOwDsAICbbrqpBMshhRD2ZpPLRaXHufSeieaGqn5/+sF1AIBbVzQ66zdrz8wYk56qbrv2tp7WjOegW2+mq1Gd+8jktOOmU2tXjYLVZ5QbUy+SVuNcdm5d7Zz3b/7hXdy79saMYumrcxJDYxewpSuWFbtS1uPiRddkWFi2OjQTs6mx7d89iP8n6JokxRK6iEkpz6V/vi+E+D6AdQBe1d7fBWAXAPT09EjrSUjZCXuzyeWi0mu5bOtwS9bQ12/WntmSIPTYmLqGLpT69U13oL5W27lfOPYuBgZH8WeHT+PMxSvOVGm9CfCa5fOWmN6I+Plj5zAZT+CXnj6C7//KHU7K/o5NK7FogUhnQyJD4LdvWJlRCO21Nh39GSgrT3/etkGkxU4tKNY1SYuudglVxIQQDQCukVJ+mP798wB+K8xrknAIOw7i10VlW4cpPGYigtvnbNfUrSvT6jPPY7oD9eNswtff24E72ptxcDxV2Dzy3of49v0dGedTLjzdGmxqqMPnb/kp7Hr1FE5PJ/DN5445opVIzmVkTuquwouJJA6MTiF+OYmvfq7dujYdM4amYou6pak/6/7eDl+lC274FR4/x9Giq13CtsRaAHxfCKGu9edSyr8N+ZokBCoh69BtHUp47mhvRk9bk2sXjXxF8g//fgy7Dkzg5bfO4w/+XY/Vfamu79cVqgqcG+sX4csbV+IPXxnHm+c+QF93i2vCylc2t+Orm9uxeNE1OHr6Yka7q1Tbqey+jXuGz+DwqTiGxlL/jb7/oWsMzLSsVGzN5krURX77hpVFWT1+hcfPcUw2qV1CFTEp5SkAPx3mNQhxc+sVgi5SI+99CAB4beJiRhzMvI7K5NM7bthiSrsPTWBoLI7WxsU4c/EK/njolNNu6rG9I04cT78n3Rp8+K4uZ7K1ykw0RVV3Fd7e1oRTUzNoXlLnGQNTIrFj00qnm4iqq9PH0gT9rP0Kj5/jKuVLFik9TLEnkSesbLlv3XcLHn3+hDPQ0hzfomJwm2+O4flj72IyngAgrRmEAHAl+TEAIPaJa3Hm4hV0X78U3TcsxZvnPnDcg+Y9mTFA1blDFVubI2T0TiJHJqdxejqBn//0DdjUGUP88kd48qW3s2KGqo4MEBmjYczn4WaJForfc1GgiBcUMVI2Ki0Yb0vq+LMvr3fe1y0CvRh473ElYICqC1PH3t7WBGC+Qz4AfOqGpVi6eBF+YV1rRicRG0pgbm9ryqgNM0UVQIZoqhqv/t4Op82VSsL4v4cn8ccP3O5kbiphXLP8uqwYVyGxpkr7dyXVDUWMlI1KC8a7pcwr3IqBN98cwxM/PJkWJJnRxPepV8adriFKVADgmcOZfRBN9DiVElWz56HpZtNnlF25Ooddr57Cxo5mbN+wEn3dLXj17SmcePcDTM9cxZefOYJ93/icMyvt8Kk4Pt16HXb/aDIjPudW2uDF7kMTGBgcc+raCAkTihgpG5UWjDfX47ZpK4HRExu+98vrHdefXhStarqOnp7G0FjcsXSGJ1MJGrsPTWRs9NlJFh1OksWa5e9mTH023Wwq7T5VW5aqVhkai6fnlwEHx+N4YP1N2PtP72F65qoTI1OW2DvTiXQ6/Xx8Lldpgx1h/LRbZ7TYSBBQxEjZKCTWke/G5+d4/RhbPViubEQ9iQPITuOvr1uAobE42mMNjpXT09aUTrUXGedWVsyOTSsd8Xrh2DnHFTkwOGbtHKJfd/76AqqDvn7Mr6Zjdqp4WxWBb745pnXgz3Vud7ZvaMvImLQ9M7fXCMkXihiJFPlufPPNdGczWknlOqfXpm2+57YmXdxefXsKB8fj+MvX30Hzkmtx79obsjZ6ALhyNZX8ASG0QZ2p2Jtel2UTZ/NLgTnSxdbZRJ1/59bVuHVFY0aGpE4+Xzhsx9qeZ9CWOC272oQiRiJFvhufHi9yEz+/hdBu77mtSRc3ZXmNvPchhsYmnOP17MLdhyZx7J1UHdniRdc4x6jYm55V+ORLJzEwOOZ0/SjUTVeIkBTiGrQ9z6CzDmnZ1SYUMRIp8t349K4aADJiSoWe0++adIG4mEji+NlLeOjOzvQU51mnQa9C/X5HeyoZQ53bnhyRckPqoujlpnMTmVzdO/xarkELSCFWVaXFWElpoIiRmiBXN3p90wRQVBKCLcamMh/Xr2p21tHf24H+3k4kkrPYfPMnnYGat9y41LX5rjq3ckeazYQBe1q+Guypu1Vt96nQ3bAp5i3BQl2D+TzDQkSR9WS1CUWM1AxeG62t5goozNLwirGpejG9fuvxF9/C8bMfYDKewMaOZXjz3AcZAzlzrUMVJo9PXcajz7+Jq3NzeG3iotPh/tnX38FkPIEtXTEAwqkn6/zkEuw6MGFNhdfdsMpCVD0pC3UNmsXiXoJGq4r4hSJGagavjdbLurC972VV9HW3OI13zXT4VCLFqNNNX8W8riQ/xprl1wGQGUn1TYgAABhaSURBVAM51686n1Gj5bW5P/r8CaeN1ZauGFobF2NoDI6APfHFtQDmx7xcnVNDI0TWuTLdsBJqOGahFikAxC9/hI0dy9DX3ZLzS4F6NkzUILmgiBGCbIGzWRr6puqVkfjY3hHXxru24uH6uoUYGHzLSak/fjbVhqo9tgTtm5dkpfubm7sSi1XLlmBoLI7PrGzEE19ci92HJgEAGzuWZaxBn2+mXJFeMTPdSsunXsy0bncdSMXudPenl6XFRA3iB4oYIT4x3WH6T/2Y/SensLFjGaSUWY13bcXDZjur+Q4fgOoIokbDjE9dxo5nhjNmfKl1mZOdAThuSz2mtvvQJC7OfIT/vOcfsWjBNY6YuU181gXOr5svVX82h/7eDs01mcqyVEKZS5joUiR+oIgRouHlLtM3VT8ZiQAy3Gl+zmWLRSn3HzCCRHIO41MzaGuuz7qWvmbltkzFwebReygqHtuban+lYnBf+D8H8bO3/BR+YV0r9o2czxp+2dfd4sw0c+v9qK6jru+eZemO17w2QhTXlHsBhFQSyqpRrZp01KbqdyP1Ot58T23UQEootm9oQ39vB3ZsWoXOlk/gjvbmtJCl4lif7VyGPcNnMD51OUN0x6cu48GnX8ftbU3Y0hVzLEH99f7eTjyw/iasufEn0dq4GA/d2Ymmhjo8ck83mhoW4fR0ArsOnMJje0fS1qLMsPBUpuNje0cy7kndw/RMEtt6Wp01P/TnbzglDvoxtr/z/TchhJYYIRrFurDmU9PnnI4cfkTPjP8oy2V+KnQnPntzzHH9qeJtM4NRCQyAdPuokYwUewBOVw517iOT07h1RSP2jZzH9MxVtDXX4/PdKUts/ar5FH8lOA/d2amdf95FqfpDqnu/5calODgex8HxuONSNe/Tz/OiW5F4QREjRMPmJrS5GNVwStOlpmq0rhguuFy4bdQ2V2H75iXOROdUY+BzzkBOJVyP3NPtNPYFRrKEx3ZNt2splODs3Lo6oz2V7qJMuQ9lRh2c3r/R7WciOevZuqsUrkS2rYomFDFCcmDLkpu3bDInMs/P5lqalWThhVuMTX/d3GTV6/V1C9NF3Kk6LrWexp46HBi94MwKM/simtfM1b1DNTg2x7KkygTmAEin04hbn0q3a6ZEeWHW8yrlPDNmQ0YTihghObBZSbrF43Zs0N/m3TZZL3eblKoWTHpu7mZNl5pavX1DW1ZW5teffcNxG6omw6Yr0I81q+M3UcYPhYoR3ZbRhCJGSJp8+gu2x5Y49VaNPXVZneTdMur8Wgm249w2Wbdr7hk+g4PjcWzpimV0BwHcx8skknNaNiSgW1eqBEAVVA9PTqfjYfNDMNV1zPsrNFboJxXffFaFihHbVkUTihghaQod82I73q0J7zefO5bVSsqGOR1ZJU8AEhcT8+691JyxlNCohsIHRi/g2/d/Ossq9Oo6osem9p+cQmvjYpy5eMUZDZNIzmH3oQncu/ZGJJKzOHr6ojZwc34IptusNZUteSU5i4HB0Qwxu5hIWuOLboJvvm5LiqEY1Q4UMULS5PsN3ut423vzhdDN1m76mWROR9aTJ4YnL2b1VqyvW4j5ac4XnGxAZT2ZjYKffOkkjp6+hKGxCxnr3HzzJ3H87AdobarHMz86jcWLrsmqLauvW4hv3fepjPOZs9H0OrnHX3wLd7Q34+B4HK1N9Wkxm0uL2Sz2Hn/PmSj9yD3djqCpAmwgU/BN0aIbsLahiBGSptAxL37fMwuZ3SY0A9nTkbf1tOLA6AUMjV3ALTcuddLt1yxPWWLquCvJjzHy3gdOEoba8HXBA+AM2tzSFctoo7WxYxmGxi5gx6ZVGYkpKnFDNRAG7EM2zd9V0seB0dS1T/7zv+C1iYtO4osq3m6PNTgCphJmVK9Hr4xN/VmzKLo2oYgRUiIyM/GypzrbjtX//vb9n84aF6MPygSA5iV1GBqLY9/IebRvXuIc29fd4tR8AfMtoMzxKvHLH2Fo7AIW111jnRKtr13Fw8yBnQrl9gMk3njnUvpVgQfW34Sjpy/hW/fdgsb6uowYmZ4w45WxqYRXFyxmF9YmFDFCIoAZB3JrxOtmpQCplHt1DrMFlC6wzUuuzUqjV1xMJHH4VNxxTyqLTqX362tVFufGjmY8sH4FXh2dwmsT0xh9/0NMz1zFY3tT5Qn6+ttjS7JKAWx4jbuhW7G2YNspQkpMIW2U9M9MzyQRn0nijvZmx22o0NtZmS2d/FxXfV7Fo9T11Hn0llN93S34zMomfGZlY8Y65gVGYktXDENjcZy5mMBkPIGmhkWYnrmKpoZF2P4zbTlbTrmxrac1qw4v37ZgpDqgJUZIifFjMejZiNs3rMz4zJ7hM9j16ikAwAvHzrlOarYlQCSSs4hfTuLJl962ugBta9TPY3YEeW1iGgAc96Xt/uYLpVMC2B5rwPjUDHb/aNJXpqYNZiASBUWMkBLjdwry/ETlVAKI7jazJVoAyBinYnMtqtllqfMu8JWYYqbq6x1B1DpsFpFC/W7OMTPjdGz7RAqBIkaIB8VsrMV8Vhcqm8WmJ4bov6ssRJVib8uQ1Od6+V27W4KFSvjwg34eZbXZejMCTMwg/qGIEeJBMRtrMZ81BUIXFfO8+rmVteMmULa5XjaxDaJnYb4izsQMUggUMUI8KGZjDXJT9jNVGvB2VbqJShCZfno3EuXOnG81NevaENhcuy11Ph/okqw9KGKEeFBMAkGQyQdmXCrffoJAfg2E81276kbSHmtw3Jl6obSbVefWQsqv8NnWQZdkbUERIyQPgvymn8+58u1K4WVdqRowvRVVsRu+fu4Xjp1Lx/Pm+ye6FXe7tZDyEj7A/dnRJVl7UMQIyYMgv+kXci6/n/HazF84dg4Dg2N4bvhMumdhZpPifERaP95tvpmXVee2zs03x3D87KWsOjiF23PIZUHS3Vh9UMQIyYMgv+kXci6/n7Ft5mrj7+/txJauGPafnHJ6J5rHAP4mLNtGrORzX+Y61fnU+tavOp+RwZjvczChu7H6CF3EhBB3AxgAsADAH0spfyfsaxISFkHGuQo5VzHXtxUh+3XH5YqnJZKzGbGsvu4W6/nHpy5bx66Y5zNryEwKfQ50N1YfYn7yawgnF2IBgLcB3AXgLIAjAO6XUo7Yju/p6ZHDw8OhrYcQkh+qye+V5MdYXLfAtcuH2S9RWVI7t67OEJtf/KPDODgexx3tzfjeL68v5a14QjdjZSOEOCql7LG9F3bvxHUAxqSUp6SUSQB/AeC+kK9JSKQwexyW6rN+SHUOGcOuA6dw/Owl1+P0Tvj9vR145J5up7ehvsZbblwKALjlxqUYn7qMB59+HeNTl4teZ7HPId9+lmE/d+KfsEXsRgD6/xVn068RQtIU0hA4iM/6YVtPK/p7O7Cxo9lJnXfbwFWrrPq6hWiPLXGSOvQ1fnVzO3ZuXY2vbm7PaCZcCPo6/DwHL+ExGwp7Hatq4sJ87sQ/ZU/sEELsALADAG666aYyr4aQ0lMpBdU2VIcPr44hQGpjTyTn0N/bkbUWtzU+ck83rs6dQOcnP5FjyrUdvwXg5vG2GjS3BBP9HvX3bEkxpDyELWLnAOj/ysvTrzlIKXcB2AWkYmIhr4eQiqMSCqpzxYTcGgIrlBW2c+vqrM/rnzXnoG3qjOHxF99C85L8U+PdCsBz1ZDZatDMz6gek4nkXJbAmtdlPK28hC1iRwB0CiFWIiVeXwLwb0O+JiEkT0zLw2tjtglnX3eLMyzTC1MAzZ/mCBqvbiO2dejtr/Rj9eNtxdfm+VXHf73ezY1KS9uvNVENVcSklLNCiK8B+DukUuy/K6V8M8xrEkLyx5Z+77deTB+Wqeq63DbSXBaTbQSNvi63idMKP64+m/jZrEu/5QaVlrZfaaIaNqHHxKSUPwDwg7CvQwjJpJC2Vop86sVswuFnI7Ud09fdggOjF9B9/SesvRxNd6TNDajW7VUK4CWuXq+pcyeSc0gkZx1XY6FiEYbVVGmiGjZlT+wghIRDsaNgbK66RHIW/b2drhaL2oj9uBdtm+2+kfMYGruATZ3LrJt6Losxl6AUa6Uo0QGAgcGxDGuxENR6VOf/IISs1qZeh51iTwgJCD3t20+dkpk2Xuw1Vc1Yfd0CqxWjv7Zv5Dz2n5zCvpHzruc2Ez6mZ5I512x+pq+7xfN4sxat2GcyL4Ky6Ger1qMKw5muXxi0xAiJCLoVASCnRRHEN/J809gV+RzrNeRTobvdADjJGypd3g0VqwNG8PSD64p+JrncleZa/bhxcw0yJd5QxAiJCF7JB/nid6N1S2PPhXms29Tnvu4Wq4vSxBRwFYMDhKeYP3RnJ96ZTuChOzs91+v3efh5Bvm6LGvN/Rc0FDFCIoK52eU7wVnH70Yb1AZrdrtXPRYPn4o7PRb9iqn5mtusMgA4MjmN8akZHJmcxq0rGp3X3YZxArmFJ9fzdbNCay31vVRQxAipMvxsyPlmsLml1ts2Za/CZNXtvr+3Azu3rs7oVu+3Nk2PA+YS2TDS5HM9X7c11Vrqe6mgiBFSZfjZkPO1sNxS622bsldhcqrQOLPlk5oXZqbP57MWG16iaD6jfJ6HLcW+kGuSYKCIEVJlhBFjyacY2Guz9lpboV0/cg3sBLLFrthWX/V1C9LdPFKC7NVX0nbNQlyLdEfaoYgRUmaisDnlUwycSyDc7lel5btNc3Y7v9fATrf+h/muzUQX0kIyOAtxLdIdaYciRkiZifrmlK8I55oSbRMAPZtx38h5a7zN/Fw+/Q9zrc3ErSGyXwuvENci3ZF2Qp3snC+c7ExqkShYYl6oWNaWrpivrhNmzZefe9evYZsY7edaQVpipToPSeE12ZmWGCFlphx1QkFustt6Wp1U+T3DZ3Lei9doFq9rAMjIZsxFIfcYdEkBEE3rOkpQxAipQYLcZP10ncg14yuXKOni4hUv0wmjL6Ff6PorHeydSEgNEkRfRR1b/0QdJShmf0C3z9l6Q/rpF6lj60vodo58+1LmItfzIMFBS4yQGqRULkw9IQPwb5nYrKhC2jmZFqKf2jYgd19KvzA2Fj4UMUJI0RRSq+WFLc5WiIvO75w02+teAzj9ihNjY+FDESOEFE0hafMm0zNJ7D40AUBg+4a2LCtK7/rhNd3ZCzU40xQgW19KlXSi+j3qzYtVF33zfk0YGwsfihghpGi8arXyKeYdGBwDAKeuK4wehGZxcq42Uarfo7qebZK1G+xQHz4UMUJI0fjZrP10f08kZwEIT3EoxLrRr+3WbcOtI4ne79G8PuNc5YciRggpCX66vz98V1fO8xRi3bgN3gyjWTLAhI5SQhEjhJSEcsaHvJoGFyuIOuq8ieSs4xqlOzFcWCdGCCkJ5aydMq/tVrfmh77uFmzpilm77c8LnHDmpRVbc0a8oSVGCKk5irEKvbrtm/Eyv221SOFQxAghnjC+k4kuVLbu+oWWGJDCoDuREOJJMa63SkW/p3zbTOmuSXWex/aO5NVWyySIVle1Ci0xQogn1WhN+E2z93uefLrr22Bnj8KhiBFCPKmEgt0gXZq6C9Crr6OfaxbSXd9GGF8UasUNTBEjhFQ8QVoqenNhs3WUvvGX0joK44tCrVh3FDFCSMVjs1QKtTS8XIBmSyrzmlEi6uv3C0WMEFLx2CyVYi2Nxvrsc5op8qWyYMJw/VWCG7gUMDuREBI5pmeSSCRn0d/bmbel4ZVtWa6C7GrMAC0VFDFCSORQHe/r6xb4mgqtE/RU63xwW1s51xR16E4khEQOr3iPn0bD5XKzua2tVlx/YUARI4REDq9Nv5ITGip5bVFFSCnLvQaHnp4eOTw8XO5lEEIIqSCEEEellD2290KLiQkhflMIcU4IcSz938+FdS1CSG3Cdk0kbHfik1LK3wv5GoSQGqVWCnqJO4yJEUIiS7XEmGqlRVQYhJ1i/zUhxHEhxHeFEI0hX4sQUmMEWddVTtck68QKpygRE0LsE0KcsPx3H4A/ANAOYC2A9wA84XKOHUKIYSHE8NTUVDHLIYSQgimnkORbJ8ZY4DxFuROllH1+jhNC/BGAvS7n2AVgF5DKTixmPYQQUijldE3mWyfGWOA8YWYnXq/9+QUAJ8K6FiGEFIJu0ZSr5ZS5Dj/ollutW2VhxsT+pxDin4QQxwFsAfBwiNcihJC8qZRYVL7rsE2XLvc9lIvQshOllP8+rHMTQkgQVEp2YzHrqJR7KBfs2EEIIXnAdPjSU5aOHYQQUo3Uuvuu0mCxMyGE5EGtu+8qDYoYIYTkAcemVBZ0JxJCCIksFDFCCCGRhSJGCCEkslDECCGERBaKGCGEkMhCESOEEBJZKGKEEEIiC0WMEEJIZKGIEUIIiSwUMUIIIZGFIkYIITVEtQ3RpIgRQogPqmXzr7Yu/GwATAghPlCbP4BINwCuti78FDFCCPFBtWz+1daFnyJGCCE+qLbNv1pgTIwQQkhkoYgRQgiJLBQxQgghkYUiRgghJLJQxAghhEQWihghhJDIQhEjhBASWShihBBSY1RLCy2AIkYIITVHNfVPZMcOQgipMaqlhRZAESOEkJqjmlpo0Z1ICCEkslDECCGERBaKGCGEkMhCESOEEBJZKGKEEEICp1S1aBQxQgghgVOqWrSiREwIsU0I8aYQ4mMhRI/x3k4hxJgQ4qQQ4meLWyYhhJAosa2nFTu3rg69Fq3YOrETAP41gKf0F4UQ3QC+BOAWADcA2CeEuFlKOVfk9QghhESAUtWiFWWJSSl/LKU8aXnrPgB/IaX8SEo5AWAMwLpirkUIIYSYhBUTuxGA7gg9m34tCyHEDiHEsBBieGpqKqTlEEIIqUZyuhOFEPsA/JTlrd+QUj5f7AKklLsA7AKAnp4eWez5CCGE1A45RUxK2VfAec8B0KN5y9OvEUIIIYERljvxBQBfEkJcK4RYCaATwOshXYsQQkiNUmyK/ReEEGcB/AyA/yeE+DsAkFK+CeA5ACMA/hbArzAzkRBCSNAUlWIvpfw+gO+7vPfbAH67mPMTQgghXrBjByGEkMhCESOEEBJZKGKEEEIii5CyckqzhBBTAE6X4dLLAFwow3VLBe8v2vD+og3vr3hWSCljtjcqSsTKhRBiWErZk/vIaML7iza8v2jD+wsXuhMJIYREFooYIYSQyEIRS7Gr3AsIGd5ftOH9RRveX4gwJkYIISSy0BIjhBASWShiaYQQ/10IcVwIcUwI8UMhxA3lXlOQCCF+VwjxVvoevy+EuK7cawoSIcQ2IcSbQoiPhRBVkQkmhLhbCHFSCDEmhPj1cq8naIQQ3xVCvC+EOFHutQSNEKJVCLFfCDGS/v+yv9xrChIhxE8IIV4XQvxj+v6+Vba10J2YQgjxk1LKf0n//nUA3VLKr5Z5WYEhhPg8gJellLNCiP8BAFLKXyvzsgJDCPGvAHwM4CkA/0lKOVzmJRWFEGIBgLcB3IXUUNkjAO6XUo6UdWEBIoT4LIDLAJ6RUn6q3OsJEiHE9QCul1K+IYT4BICjAH6+Wv79hBACQIOU8rIQYhGAIQD9UsrDpV4LLbE0SsDSNACoKnWXUv5QSjmb/vMwUjPeqgYp5Y+llCfLvY4AWQdgTEp5SkqZBPAXAO4r85oCRUr5KoDpcq8jDKSU70kp30j//iGAH8Nlun0UkSkup/9clP6vLHsmRUxDCPHbQogzAH4RwH8r93pC5JcAvFjuRRBPbgRwRvv7LKpoE6wlhBBtAD4N4LXyriRYhBALhBDHALwP4CUpZVnur6ZETAixTwhxwvLffQAgpfwNKWUrgO8B+Fp5V5s/ue4vfcxvAJhF6h4jhZ/7I6SSEEIsAfBXAH7V8PZEHinlnJRyLVJenXVCiLK4hIuaJxY1pJR9Pg/9HoAfAHg0xOUETq77E0L8BwD3AOiVEQyG5vHvVw2cA9Cq/b08/RqJCOlY0V8B+J6U8q/LvZ6wkFJeEkLsB3A3gJIn6dSUJeaFEKJT+/M+AG+Vay1hIIS4G8B/AXCvlDJR7vWQnBwB0CmEWCmEqAPwJQAvlHlNxCfpxIc/AfBjKeXvl3s9QSOEiKkMZyHEYqQSkMqyZzI7MY0Q4q8AdCGV4XYawFellFXzzVcIMQbgWgDx9EuHqyz78gsAvgMgBuASgGNSyp8t76qKQwjxcwD+F4AFAL6bnpZeNQghngXwOaS6oJ8H8KiU8k/KuqiAEEJsBHAAwD8htacAwH+VUv6gfKsKDiHEGgC7kfp/8xoAz0kpf6ssa6GIEUIIiSp0JxJCCIksFDFCCCGRhSJGCCEkslDECCGERBaKGCGEkMhCESOEEBJZKGKEEEIiC0WMEEJIZPn/n6sbeO0bmzUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_figsize((7, 5)) \n",
    "plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Reading the Dataset\n",
    "Recall that training models consists of making multiple passes over the dataset, grabbing one minibatch of examples at a time, and using them to update our model. Since this process is so fundamental to training machine learning algorithms, its worth defining a utility function to shuffle the data and access it in minibatches.\n",
    "\n",
    "In the following code, we define a `data_iter` function to demonstrate one possible implementation of this functionality. The function takes a batch size, a design matrix, and a vector of labels, yielding minibatches of size `batch_size`. Each minibatch consists of a tuple of features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = np.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, note that we want to use reasonably sized minibatches to take advantage of the GPU hardware, which excels at parallelizing operations. Because each example can be fed through our models in parallel and the gradient of the loss function for each example can also be taken in parallel, GPUs allow us to process hundreds of examples in scarcely more time than it might take to process just a single example.\n",
    "\n",
    "To build some intuition, let us read and print the first small batch of data examples. The shape of the features in each minibatch tells us both the minibatch size and the number of input features. Likewise, our minibatch of labels will have a shape given by batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1279001   1.5367466 ]\n",
      " [ 0.48916838  0.7603386 ]\n",
      " [-1.4315921  -0.01926271]\n",
      " [-0.6409365   0.9908941 ]\n",
      " [ 0.46760127 -0.16282491]\n",
      " [ 1.2690786   0.5477537 ]\n",
      " [-1.2106425  -1.006717  ]\n",
      " [ 0.22596185  0.529344  ]\n",
      " [-0.52414197  1.9040878 ]\n",
      " [ 1.8919657   0.6615249 ]] \n",
      " [-3.2772422   2.5764859   1.4025525  -0.44816723  5.6876993   4.8570065\n",
      "  5.2098837   2.8673012  -3.3083458   5.7306504 ]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we run the iterator, we obtain distinct minibatches successively until all the data has been exhausted (try this). While the iterator implemented above is good for didactic purposes, it is inefficient in ways that might get us in trouble on real problems. For example, it requires that we load all data in memory and that we perform lots of random memory access. The built-in iterators implemented in `Apache MXNet` are considerably more efficient and they can deal both with data stored in file and data fed via a data stream.\n",
    "\n",
    "### 3.2.3 Initializing Model Parameters\n",
    "Before we can begin optimizing our model's parameters by gradient descent, we need to have some parameters in the first place. In the following code, we initialize weights by sampling random numbers from a normal distribution with mean 0 and a standard deviation of $0.01$, setting the bias $b$ to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.normal(0, 0.01, (2, 1))\n",
    "b = np.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initialized our parameters, our next task is to update them until they fit our data sufficiently well. Each update requires taking the gradient (a multi-dimensional derivative) of our loss function with respect to the parameters. Given this gradient, we can update each parameter in the direction that reduces the loss.\n",
    "\n",
    "Since nobody wants to compute gradients explicitly (this is tedious and error prone), we use automatic differentiation to compute the gradient. See `Section 2.5` for more details. Recall from the `autograd` chapter that in order for the system to know that it should store a gradient for our parameters, we specified to attach gradients to both $w$ and $b$ on the above codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.attach_grad()\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Defining the Model\n",
    "Next, we must define our model, relating its inputs and parameters to its outputs. Recall that to calculate the output of the linear model, we simply take the matrix-vector dot product of the examples $\\mathbf{X}$ and the models weights $w$, and add the offset $b$ to each example. Note that below $Xw$ is a vector and $b$ is a scalar. Recall that when we add a vector and a scalar, the scalar is added to each component of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    return np.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Defining the Loss Function\n",
    "Since updating our model requires taking the gradient of our loss function, we ought to define the loss function first. Here we will use the squared loss function as described in the previous section. In the implementation, we need to transform the true value $y$ into the predicted value's shape $\\hat{y}$. The result returned by the following function will also be the same as the $\\hat{y}$ shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Defining the Optimization Algorithm\n",
    "As we discussed in the previous section, linear regression has a closed-form solution. However, this is not a book about linear regression, it is a book about deep learning. Since none of the other models that this book introduces can be solved analytically, we will take this opportunity to introduce your first working example of `stochastic gradient descent` (`SGD`).\n",
    "\n",
    "At each step, using one batch randomly drawn from our dataset, we will estimate the gradient of the loss with respect to our parameters. Next, we will update our parameters (a small amount) in the direction that reduces the loss. Recall from `Section 2.5` that after we call `backward` each parameter (param) will have its gradient stored in `param.grad`. The following code applies the `SGD` update, given a set of parameters, a learning rate, and a batch size. The size of the update step is determined by the learning rate `lr`. Because our loss is calculated as a sum over the batch of examples, we normalize our step size by the batch size (`batch_size`), so that the magnitude of a typical step size does not depend heavily on our choice of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 Training\n",
    "Now that we have all of the parts in place, we are ready to implement the main training loop. It is crucial that you understand this code because you will see nearly identical training loops over and over again throughout your career in deep learning.\n",
    "\n",
    "In each iteration, we will grab minibatches of models, first passing them through our model to obtain a set of predictions. After calculating the loss, we call the `backward` function to initiate the backwards pass through the network, storing the gradients with respect to each parameter in its corresponding `.grad` attribute. Finally, we will call the optimization algorithm `sgd` to update the model parameters. Since we previously set the batch size `batch_size` to $10$, the loss shape $l$ for each minibatch is ($10$, $1$).\n",
    "\n",
    "In summary, we will execute the following loop:\n",
    "+ Initialize parameters $(\\mathbf{w}, b)$\n",
    "+ Repeat until done\n",
    "    + Compute gradient $\\displaystyle\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{\\mathcal{B}} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^i, y^i, \\mathbf{w}, b)$\n",
    "    + Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n",
    "\n",
    "In the code below, $l$ is a vector of the losses for each example in the minibatch. Because $l$ is not a scalar variable, running `l.backward()` adds together the elements in $l$ to obtain the new variable and then calculates the gradient.\n",
    "\n",
    "In each epoch (a pass through the data), we will iterate through the entire dataset (using the `data_iter` function) once passing through every examples in the training dataset (assuming the number of examples is divisible by the batch size). The number of epochs `num_epochs` and the learning rate `lr` are both hyper-parameters, which we set here to $3$ and $0.03$, respectively. Unfortunately, setting hyper-parameters is tricky and requires some adjustment by trial and error. We elide these details for now but revise them later in `Chapter 11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.025138868018984795\n",
      "epoch 2, loss 8.920243999455124e-05\n",
      "epoch 3, loss 5.104434603708796e-05\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03  # Learning rate\n",
    "num_epochs = 3  # Number of iterations\n",
    "net = linreg  # Our fancy linear model\n",
    "loss = squared_loss  # 0.5 (y-y')^2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Assuming the number of examples can be divided by the batch size, all\n",
    "    # the examples in the training dataset are used once in one epoch\n",
    "    # iteration. The features and tags of minibatch examples are given by X\n",
    "    # and y respectively\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, w, b), y)  # Minibatch loss in X and y\n",
    "        l.backward()  # Compute gradient on l with respect to [w, b]\n",
    "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print(f'epoch {epoch+1}, loss {float(train_l.mean())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, because we synthesized the data ourselves, we know precisely what the true parameters are. Thus, we can evaluate our success in training by comparing the true parameters with those that we learned through our training loop. Indeed they turn out to be very close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in estimating w [ 3.0565262e-04 -3.0755997e-05]\n",
      "Error in estimating b [0.00037813]\n"
     ]
    }
   ],
   "source": [
    "print('Error in estimating w', true_w - w.reshape(true_w.shape)) \n",
    "print('Error in estimating b', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we should not take it for granted that we are able to recover the parameters accurately. This only happens for a special category problems: strongly convex optimization problems with \"enough\" data to ensure that the noisy samples allow us to recover the underlying dependency. In most cases this is not the case. In fact, the parameters of a deep network are rarely the same (or even close) between two different runs, unless all conditions are identical, including the order in which the data is traversed. However, in machine learning, we are typically less concerned with recovering true underlying parameters, and more concerned with parameters that lead to accurate prediction. Fortunately, even on difficult optimization problems, `stochastic gradient descent` can often find remarkably good solutions, owing partly to the fact that, for deep networks, there exist many configurations of the parameters that lead to accurate prediction.\n",
    "\n",
    "##### Summary\n",
    "We saw how a deep network can be implemented and optimized from scratch, using just `ndarray` and auto differentiation, without any need for defining layers, fancy optimizers, etc. This only scratches the surface of what is possible. In the following sections, we will describe additional models based on the concepts that we have just introduced and learn how to implement them more concisely.\n",
    "\n",
    "##### Exercises\n",
    "1. What would happen if we were to initialize the weights $\\mathbf{w} = 0$. Would the algorithm still work?\n",
    "2. Assume that you are `Georg Simon Ohm` trying to come up with a model between voltage and current. Can you use auto differentiation to learn the parameters of your model.\n",
    "3. Can you use `Planck's Law` to determine the temperature of an object using spectral energy density?\n",
    "4. What are the problems you might encounter if you wanted to compute the second derivatives? How would you fix them?\n",
    "5. Why is the `reshape` function needed in the `squared_loss` function?\n",
    "6. Experiment using different learning rates to find out how fast the loss function value drops.\n",
    "7. If the number of examples cannot be divided by the batch size, what happens to the `data_iter` function's behavior?\n",
    "\n",
    "\n",
    "## 3.3 Concise Implementation of Linear Regression\n",
    "Broad and intense interest in deep learning for the past several years has inspired both companies, academics, and hobbyists to develop a variety of mature open source frameworks for automating the repetitive work of implementing gradient-based learning algorithms. \n",
    "\n",
    "In the previous section, we relied only on:\n",
    "+ `ndarray` for data storage and linear algebra\n",
    "+ auto differentiation for calculating derivatives\n",
    "\n",
    "In practice, because data iterators, loss functions, optimizers, and neural network layers (and some whole architectures) are so common, modern libraries implement these components for us as well.\n",
    "\n",
    "In this section, we will show you how to implement the linear regression model from `Section 3.2` concisely by using framework's high-level APIs.\n",
    "\n",
    "### 3.3.1 Generating the Dataset\n",
    "To start, we will generate the same dataset as in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = np.array([2, -3.4]) \n",
    "true_b = 4.2 \n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Reading the Dataset\n",
    "Rather than rolling our own iterator, we can call upon the data module to read data:\n",
    "+ the first step will be to instantiate an `ArrayDataset`. This object's constructor takes one or more `ndarrays` as arguments. Here, we pass in features and labels as arguments\n",
    "+ next, we will use the `ArrayDataset` to instantiate a `DataLoader`, which also requires that we specify a `batch_size` and specify a `Boolean` value shuffle indicating whether or not we want the `DataLoader` to shuffle the data on each epoch (pass through the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True): \n",
    "    \"\"\"Construct a Gluon data loader.\"\"\"\n",
    "    dataset = gluon.data.ArrayDataset(*data_arrays)\n",
    "    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `data_iter` in much the same way as we called the `data_iter` function in the previous section. To verify that it is working, we can read and print the first minibatch of instances. Comparing to `Section 3.2.2`, here we use iter to construct an Python iterator and then use next to obtain the first item from the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.43901733, -0.1586038 ],\n",
       "        [ 0.11626463, -0.31604034],\n",
       "        [ 2.0317695 , -1.0455701 ],\n",
       "        [ 0.25080684, -1.2058929 ],\n",
       "        [-1.9351275 ,  0.39395174],\n",
       "        [ 0.9052883 , -2.0125992 ],\n",
       "        [ 1.4248887 ,  0.33760387],\n",
       "        [ 0.2772456 , -0.10989486],\n",
       "        [-0.05150638,  0.22568646],\n",
       "        [ 0.76925284,  0.62415916]]),\n",
       " array([ 3.8539762,  5.513534 , 11.811262 ,  8.78959  , -1.0148638,\n",
       "        12.846732 ,  5.90655  ,  5.130535 ,  3.32866  ,  3.6037154])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Defining the Model\n",
    "When we implemented linear regression from scratch, we defined our model parameters explicitly and coded up the calculations to produce output using basic linear algebra operations. You should know how to do this. But once your models get more complex, and once you have to do this nearly every day, you will be glad for the assistance. The situation is similar to coding up your own blog from scratch. Doing it once or twice is rewarding and instructive, but you would be a lousy web developer if every time you needed a blog you spent a month reinventing the wheel.\n",
    "\n",
    "For standard operations, we can use the framework's predefined layers, which allow us to focus especially on the layers used to construct the model rather than having to focus on the implementation. To define a linear model, we first import the `nn` module, which defines a large number of neural network layers (note that `nn` is an abbreviation for neural networks). We will first define a model variable `net`, which will refer to an instance of the `Sequential` class. The `Sequential` class defines a container for several layers that will be chained together. Given input data, a `Sequential` passes it through the first layer, in turn passing the output as the second layer's input and so forth. In the following example, our model consists of only one layer, so we do not really need `Sequential`. But since nearly all of our future models will involve multiple layers, we will use it anyway just to familiarize you with the most standard workflow.\n",
    "\n",
    "Recall the architecture of a single-layer network as shown in `Fig. 3.3.1`. The layer is said to be fully-connected because each of its inputs are connected to each of its outputs by means of a matrix-vector multiplication.\n",
    "\n",
    "<img src=\"images/03_04.png\" style=\"width:450px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Gluon`, the fully-connected layer is defined in the `Dense` class. Since we only want to generate a single scalar output, we set that number to $1$.\n",
    "\n",
    "It is worth noting that, for convenience, `Gluon` does not require us to specify the input shape for each layer. So here, we do not need to tell `Gluon` how many inputs go into this linear layer. When we first try to pass data through our model, e.g., when we execute `net(X)` later, `Gluon` will automatically infer the number of inputs to each layer. We will describe how this works in more detail in the chapter `Deep Learning Computation`. \n",
    "\n",
    "### 3.3.4 Initializing Model Parameters\n",
    "Before using net, we need to initialize the model parameters, such as the weights and biases in the linear regression model. We will import the initializer module from `MXNet`. This module provides various methods for model parameter initialization. `Gluon` makes init available as a shortcut (abbreviation) to access the initializer package. By calling `init.Normal(sigma=0.01)`, we specify that each weight parameter should be randomly sampled from a normal distribution with mean $0$ and standard deviation $0.01$. The bias parameter will be initialized to zero by default. Both the weight vector and bias will have attached gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above may look straightforward but you should note that something strange is happening here. We are initializing parameters for a network even though `Gluon` does not yet know how many dimensions the input will have! It might be $2$ as in our example or it might be $2000$. `Gluon` lets us get away with this because behind the scenes, the initialization is actually deferred. The real initialization will take place only when we for the first time attempt to pass data through the network. Just be careful to remember that since the parameters have not been initialized yet, we cannot access or manipulate them.\n",
    "\n",
    "### 3.3.5 Defining the Loss Function\n",
    "In `Gluon`, the loss module defines various loss functions. We will use the imported module `loss` with the pseudonym `gloss` to avoid confusing it for the variable holding our chosen loss function. In this example, we will use the `Gluon` implementation of squared loss (`L2Loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 Defining the Optimization Algorithm\n",
    "`Minibatch SGD` and related variants are standard tools for optimizing neural networks and thus `Gluon` supports `SGD` alongside a number of variations on this algorithm through its Trainer class. When we instantiate the Trainer, we will specify the parameters to optimize over (obtainable from our net via `net.collect_params()`), the optimization algorithm we wish to use (sgd), and a dictionary of hyper-parameters required by our optimization algorithm. `SGD` just requires that we set the value `learning_rate`, (here we set it to `0.03`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.gluon.trainer.Trainer at 0x7f26282b2748>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.7 Training\n",
    "You might have noticed that expressing our model through `Gluon` requires comparatively few lines of code. We did not have to individually allocate parameters, define our loss function, or implement stochastic gradient descent. Once we start working with much more complex models, `Gluon`'s advantages will grow considerably. However, once we have all the basic pieces in place, the training loop itself is strikingly similar to what we did when implementing everything from scratch.\n",
    "\n",
    "To refresh your memory: for some number of epochs, we will make a complete pass over the dataset (`train_data`), iteratively grabbing one minibatch of inputs and the corresponding ground-truth labels. For each minibatch, we go through the following ritual:\n",
    "+ Generate predictions by calling `net(X)` and calculate the loss $l$ (the forward pass).\n",
    "+ Calculate gradients by calling `l.backward()` (the backward pass).\n",
    "+ Update the model parameters by invoking our `SGD` optimizer (note that trainer already knows which parameters to optimize over, so we just need to pass in the minibatch size.\n",
    "\n",
    "For good measure, we compute the loss after each epoch and print it to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.042365\n",
      "epoch 2, loss: 0.000163\n",
      "epoch 3, loss: 0.000050\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(features), labels)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compare the model parameters learned by training on finite data and the actual parameters that generated our dataset. To access parameters with `Gluon`, we first access the layer that we need from net and then access that layer's `weight` and `bias`. To access each parameter's values as an `ndarray`, we invoke its data method. As in our from-scratch implementation, note that our estimated parameters are close to their ground truth counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in estimating w [[-0.00021648 -0.00075078]]\n",
      "Error in estimating b [0.00076628]\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data()\n",
    "print('Error in estimating w', true_w.reshape(w.shape) - w)\n",
    "b = net[0].bias.data()\n",
    "print('Error in estimating b', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ Using `Gluon`, we can implement models much more succinctly.\n",
    "+ In `Gluon`, the data module provides tools for data processing, the nn module defines a large number of neural network layers, and the loss module defines many common loss functions.\n",
    "+ `MXNet`'s module `initializer` provides various methods for model parameter initialization.\n",
    "+ Dimensionality and storage are automatically inferred (but be careful not to attempt to access parameters before they have been initialized).\n",
    "\n",
    "\n",
    "##### Exercises\n",
    "1. If we replace `l = loss(output, y)` with `l = loss(output, y).mean()`, we need to change `trainer.step(batch_size)` to `trainer.step(1)` for the code to behave identically. Why?\n",
    "2. Review the `MXNet` documentation to see what loss functions and initialization methods are provided in the modules `gluon.loss` and `init`. Replace the `loss` by Huber's loss.\n",
    "3. How do you access the gradient of `dense.weight`?\n",
    "\n",
    "\n",
    "## 3.4 Softmax Regression\n",
    "In `Section 3.1`, we introduced linear regression, working through implementations from scratch in `Section 3.2` and again using `Gluon` in `Section 3.3` to do the heavy lifting.\n",
    "\n",
    "Regression is the hammer we reach for when we want to answer `how much?` or `how many?` questions. If you want to predict the number of dollars (the price) at which a house will be sold, or the number of wins a baseball team might have, or the number of days that a patient will remain hospitalized before being discharged, then you are probably looking for a regression model.\n",
    "\n",
    "In practice, we are more often interested in classification: asking not `how much?` but `which one?`\n",
    "+ Does this email belong in the spam folder or the inbox?\n",
    "+ Is this customer more likely to sign up or not to sign up for a subscription service?\n",
    "+ Does this image depict a donkey, a dog, a cat, or a rooster?\n",
    "+ Which movie is Aston most likely to watch next?\n",
    "\n",
    "Colloquially, machine learning practitioners overload the word classification to describe two subtly different problems: \n",
    "+ those where we are interested only in hard assignments of examples to categories\n",
    "+ those where we wish to make soft assignments, i.e., to assess the probability that each category applies\n",
    "\n",
    "The distinction tends to get blurred, in part, because often, even when we only care about hard assignments, we still use models that make soft assignments.\n",
    "\n",
    "### 3.4.1 Classification Problems\n",
    "To get our feet wet, let us start off with a simple image classification problem. Here, each input consists of a $2\\times2$ grayscale image. We can represent each pixel value with a single scalar, giving us four features $x_1, x_2, x_3, x_4$. Further, let us assume that each image belongs to one among the categories \"cat\", \"chicken\" and \"dog\".\n",
    "\n",
    "Next, we have to choose how to represent the labels. We have two obvious choices. Perhaps the most natural impulse would be to choose $y \\in {1, 2, 3}$, where the integers represent `{dog, cat, chicken}` respectively. This is a great way of storing such information on a computer. If the categories had some natural ordering among them, say if we were trying to predict `{baby, toddler, adolescent, young adult, adult, geriatric}`, then it might even make sense to cast this problem as regression and keep the labels in this format.\n",
    "\n",
    "But general classification problems do not come with natural orderings among the classes. Fortunately, statisticians long ago invented a simple way to represent categorical data: the one hot encoding. A one-hot encoding is a vector with as many components as we have categories. The component corresponding to particular instance's category is set to 1 and all other components are set to 0.\n",
    "$$y \\in {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.$$\n",
    "\n",
    "In our case, $y$ would be a three-dimensional vector, with $(1, 0, 0)$ corresponding to \"cat\", $(0, 1, 0)$ to \"chicken\" and $(0, 0, 1)$ to \"dog\".\n",
    "\n",
    "##### Network Architecture\n",
    "In order to estimate the conditional probabilities associated with each class, we need a model with multiple outputs, one per class. To address classification with linear models, we will need as many linear functions as we have outputs. Each output will correspond to its own linear function. In our case, since we have 4 features and 3 possible output categories, we will need 12 scalars to represent the weights, ($w$ with subscripts) and 3 scalars to represent the biases ($b$ with subscripts). We compute these three logits, $o_1, o_2$, and $o_3$, for each input:\n",
    "$$ \\begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\ o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\ o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3. \\end{aligned} $$\n",
    "\n",
    "We can depict this calculation with the neural network diagram shown in `Fig. 3.4.1`. \n",
    "\n",
    "<img src=\"images/03_05.png\" style=\"width:500px;\"/>\n",
    "\n",
    "Just as in linear regression, softmax regression is also a single-layer neural network. And since the calculation of each output, $o_1, o_2$, and $o_3$, depends on all inputs, $x_1$, $x_2$, $x_3$, and $x_4$, the output layer of softmax regression can also be described as fully-connected layer.\n",
    "\n",
    "To express the model more compactly, we can use linear algebra notation. In vector form, we arrive at $\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$, a form better suited both for mathematics, and for writing code. Note that we have gathered all of our weights into a $3\\times4$ matrix and that for a given example $\\mathbf{x}$, our outputs are given by a matrix-vector product of our weights by our inputs plus our biases $\\mathbf{b}$.\n",
    "\n",
    "##### Softmax Operation\n",
    "The main approach that we are going to take here is to interpret the outputs of our model as probabilities. We will optimize our parameters to produce probabilities that maximize the likelihood of the observed data. Then, to generate predictions, we will set a threshold, for example, choosing the `argmax` of the predicted probabilities.\n",
    "\n",
    "Put formally, we would like outputs $\\hat{y}_k$ that we can interpret as the probability that a given item belongs to class $k$. Then we can choose the class with the largest output value as our prediction $\\mathrm{argmax}_k y_k$. For example, if $\\hat{y}_1$, $\\hat{y}_2$, and $\\hat{y}_3$ are $0.1$, $0.8$, and $0.1$, respectively, then we predict category $2$, which (in our example) represents \"chicken\".\n",
    "\n",
    "You might be tempted to suggest that we interpret the logits $o$ directly as our outputs of interest. However, there are some problems with directly interpreting the output of the linear layer as a probability. Nothing constrains these numbers to sum to 1. Moreover, depending on the inputs, they can take negative values. These violate basic axioms of probability presented in `Section 2.6`.\n",
    "\n",
    "To interpret our outputs as probabilities, we must guarantee that (even on new data), they will be nonnegative and sum up to 1. Moreover, we need a training objective that encourages the model to estimate faithfully probabilities. Of all instances when a classifier outputs $0.5$, we hope that half of those examples will actually belong to the predicted class. This is a property called `calibration`.\n",
    "\n",
    "The softmax function, invented in 1959 by the social scientist `R Duncan Luce` in the context of choice models does precisely this. To transform our logits such that they become nonnegative and sum to $1$, while requiring that the model remains differentiable, we first exponentiate each logit (ensuring non-negativity) and then divide by their sum (ensuring that they sum to $1$).\n",
    "$$ \\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}. $$\n",
    "\n",
    "It is easy to see $\\hat{y}_1 + \\hat{y}_2 + \\hat{y}_3 = 1$ with $0 \\leq \\hat{y}_i \\leq 1$ for all $i$. Thus, $\\hat{y}$ is a proper probability distribution and the values of $\\hat{\\mathbf{y}}$ can be interpreted accordingly. Note that the softmax operation does not change the ordering among the logits, and thus we can still pick out the most likely class by:\n",
    "\n",
    "$$ \\hat{\\imath}(\\mathbf{o}) = \\operatorname*{argmax}_i o_i = \\operatorname*{argmax}_i \\hat y_i. $$\n",
    "\n",
    "The logits $\\mathbf{o}$ then are simply the pre-softmax values that determining the probabilities assigned to each category. Summarizing it all in vector notation we get ${\\mathbf{o}}^{(i)} = \\mathbf{W} {\\mathbf{x}}^{(i)} + {\\mathbf{b}}$, where ${\\hat{\\mathbf{y}}}^{(i)} = \\mathrm{softmax}({\\mathbf{o}}^{(i)})$.\n",
    "\n",
    "##### Vectorization for Minibatches\n",
    "To improve computational efficiency and take advantage of GPUs, we typically carry out vector calculations for minibatches of data. Assume that we are given a minibatch $\\mathbf{X}$ of examples with dimensionality $d$ and batch size $n$. Moreover, assume that we have $q$ categories (outputs). Then the minibatch features $\\mathbf{X}$ are in $\\mathbb{R}^{n \\times d}$, weights $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$, and the bias satisfies $\\mathbf{b} \\in \\mathbb{R}^q$.\n",
    "\n",
    "$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned} $$\n",
    "\n",
    "This accelerates the dominant operation into a matrix-matrix product $\\mathbf{W} \\mathbf{X}$ vs the matrix-vector products we would be executing if we processed one example at a time. The softmax itself can be computed by exponentiating all entries in $\\mathbf{O}$ and then normalizing them by the sum.\n",
    "\n",
    "### 3.4.2 Loss Function\n",
    "Next, we need a loss function to measure the quality of our predicted probabilities. We will rely on likelihood maximization, the very same concept that we encountered when providing a probabilistic justification for the least squares objective in linear regression (`Section 3.1`).\n",
    "\n",
    "##### Log-Likelihood\n",
    "The softmax function gives us a vector $\\hat{\\mathbf{y}}$, which we can interpret as estimated conditional probabilities of each class given the input $x$, e.g., $\\hat{y}_1$ = $\\hat{P}(y=\\mathrm{cat} \\mid \\mathbf{x})$. We can compare the estimates with reality by checking how probable the actual classes are according to our model, given the features.\n",
    "\n",
    "$$ P(Y \\mid X) = \\prod_{i=1}^n P(y^{(i)} \\mid x^{(i)}) \\text{ and thus } -\\log P(Y \\mid X) = \\sum_{i=1}^n -\\log P(y^{(i)} \\mid x^{(i)}). $$\n",
    "\n",
    "Maximizing $P(Y \\mid X)$ (and thus equivalently minimizing $-\\log P(Y \\mid X)$) corresponds to predicting the label well. This yields the loss function (we dropped the superscript $(i)$ to avoid notation clutter):\n",
    "\n",
    "$$ l = -\\log P(y \\mid x) = - \\sum_j y_j \\log \\hat{y}_j. $$\n",
    "\n",
    "For reasons explained later on, this loss function is commonly called the `cross-entropy loss`. Here, we used that by construction $\\hat{y}$ is a discrete probability distribution and that the vector $\\mathbf{y}$ is a one-hot vector. Hence the sum over all coordinates $j$ vanishes for all but one term. Since all $\\hat{y}_j$ are probabilities, their logarithm is never larger than $0$. Consequently, the loss function cannot be minimized any further if we correctly predict $y$ with certainty, i.e., if $P(y \\mid x) = 1$ for the correct label. Note that this is often not possible. For example, there might be label noise in the dataset (some examples may be mislabeled). It may also not be possible when the input features are not sufficiently informative to classify every example perfectly.\n",
    "\n",
    "##### Softmax and Derivatives\n",
    "Since the softmax and the corresponding loss are so common, it is worth while understanding a bit better how it is computed. Plugging $o$ into the definition of the loss $l$ and using the definition of the softmax we obtain:\n",
    "\n",
    "$$ l = -\\sum_j y_j \\log \\hat{y}_j = \\sum_j y_j \\log \\sum_k \\exp(o_k) - \\sum_j y_j o_j = \\log \\sum_k \\exp(o_k) - \\sum_j y_j o_j. $$\n",
    "\n",
    "To understand a bit better what is going on, consider the derivative with respect to $o$. We get\n",
    "\n",
    "$$ \\partial_{o_j} l = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j = P(y = j \\mid x) - y_j. $$\n",
    "\n",
    "In other words, the gradient is the difference between the probability assigned to the true class by our model, as expressed by the probability $P(y \\mid x)$, and what actually happened, as expressed by $y$. In this sense, it is very similar to what we saw in regression, where the gradient was the difference between the observation $y$ and estimate $\\hat{y}$. This is not coincidence. In any exponential family model, the gradients of the log-likelihood are given by precisely this term. This fact makes computing gradients easy in practice.\n",
    "\n",
    "##### Cross-Entropy Loss\n",
    "Now consider the case where we observe not just a single outcome but an entire distribution over outcomes. We can use the same representation as before for $y$. The only difference is that rather than a vector containing only binary entries, say $(0, 0, 1)$, we now have a generic probability vector, say $(0.1, 0.2, 0.7)$. The math that we used previously to define the loss $l$ still works out fine, just that the interpretation is slightly more general. It is the expected value of the loss for a distribution over labels.\n",
    "\n",
    "$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_j y_j \\log \\hat{y}_j. $$\n",
    "\n",
    "This loss is called the cross-entropy loss and it is one of the most commonly used losses for multiclass classification. We can demystify the name by introducing the basics of information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Information Theory Basics\n",
    "Information theory deals with the problem of encoding, decoding, transmitting and manipulating information (also known as data) in as concise form as possible.\n",
    "\n",
    "##### Entropy\n",
    "The central idea in information theory is to quantify the information content in data. This quantity places a hard limit on our ability to compress the data. In information theory, this quantity is called the entropy of a distribution $p$, and it is captured by the following equation:\n",
    "$$ H[p] = \\sum_j - p(j) \\log p(j). $$\n",
    "\n",
    "One of the fundamental theorems of information theory states that in order to encode data drawn randomly from the distribution $p$, we need at least $H[p]$ \"nats\" to encode it. If you wonder what a \"nat\" is, it is the equivalent of bit but when using a code with base $e$ rather than one with base 2. One nat is $\\frac{1}{\\log(2)} \\approx 1.44$ bit. $H[p] / 2$ is often also called the binary entropy.\n",
    "\n",
    "##### Surprisal\n",
    "You might be wondering what compression has to do with prediction. Imagine that we have a stream of data that we want to compress. If it is always easy for us to predict the next token, then this data is easy to compress! Take the extreme example where every token in the stream always takes the same value. That is a very boring data stream! And not only it is boring, but it is easy to predict. Because they are always the same, we do not have to transmit any information to communicate the contents of the stream. Easy to predict, easy to compress.\n",
    "\n",
    "However if we cannot perfectly predict every event, then we might some times be surprised. Our surprise is greater when we assigned an event lower probability. For reasons that we will elaborate in the appendix, Claude Shannon settled on $\\log(1/p(j)) = -\\log p(j)$ to quantify one's surprisal at observing an event $j$ having assigned it a (subjective) probability $p(j)$. The entropy is then the expected surprisal when one assigned the correct probabilities (that truly match the data-generating process). The entropy of the data is then the least surprised that one can ever be (in expectation).\n",
    "\n",
    "##### Cross-Entropy Revisited\n",
    "So if entropy is level of surprise experienced by someone who knows the true probability, then you might be wondering, what is cross-entropy? The cross-entropy from $p$ to $q$, denoted $H(p, q)$, is the expected surprisal of an observer with subjective probabilities $q$ upon seeing data that was actually generated according to probabilities $p$. The lowest possible cross-entropy is achieved when $p=q$. In this case, the cross-entropy from $p$ to $q$ is $H(p, p)= H(p)$. Relating this back to our classification objective, even if we get the best possible predictions, we will never be perfect. Our loss is lower-bounded by the entropy given by the actual conditional distributions $P(\\mathbf{y} \\mid \\mathbf{x})$.\n",
    "\n",
    "##### Kullback-Leibler Divergence\n",
    "Perhaps the most common way to measure the distance between two distributions is to calculate the Kullback-Leibler divergence $D(p|q)$. This is simply the difference between the cross-entropy and the entropy, i.e., the additional cross-entropy incurred over the irreducible minimum value it could take:\n",
    "$$ D(p|q) = H(p, q) - H[p] = \\sum_j p(j) \\log \\frac{p(j)}{q(j)}. $$\n",
    "\n",
    "Note that in classification, we do not know the true $p$, so we cannot compute the entropy directly. However, because the entropy is out of our control, minimizing $D(p|q)$ with respect to $q$ is equivalent to minimizing the cross-entropy loss.\n",
    "\n",
    "In short, we can think of the cross-entropy classification objective in two ways: (i) as maximizing the likelihood of the observed data; and (ii) as minimizing our surprise (and thus the number of bits) required to communicate the labels.\n",
    "\n",
    "##### Model Prediction and Evaluation\n",
    "After training the softmax regression model, given any example features, we can predict the probability of each output category. Normally, we use the category with the highest predicted probability as the output category. The prediction is correct if it is consistent with the actual category (label). In the next part of the experiment, we will use accuracy to evaluate the model’s performance. This is equal to the ratio between the number of correct predictions and the total number of predictions.\n",
    "\n",
    "##### Summary\n",
    "+ We introduced the softmax operation which takes a vector and maps it into probabilities.\n",
    "+ Softmax regression applies to classification problems. It uses the probability distribution of the output category in the softmax operation.\n",
    "+ Cross-entropy is a good measure of the difference between two probability distributions. It measures the number of bits needed to encode the data given our model.\n",
    "\n",
    "##### Exercises\n",
    "1. Show that the Kullback-Leibler divergence $D(p|q)$ is nonnegative for all distributions $p$ and $q$. Hint: use Jensen's inequality, i.e., use the fact that $-\\log x$ is a convex function.\n",
    "2. Show that $\\log \\sum_j \\exp(o_j)$ is a convex function in $o$.\n",
    "3. We can explore the connection between exponential families and the softmax in some more depth\n",
    "    + Compute the second derivative of the cross-entropy loss $l(y,\\hat{y})$ for the softmax.\n",
    "    + Compute the variance of the distribution given by $\\mathrm{softmax}(o)$ and show that it matches the second derivative computed above.\n",
    "4. Assume that we have three classes which occur with equal probability, i.e., the probability vector is $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$.\n",
    "    + What is the problem if we try to design a binary code for it? Can we match the entropy lower bound on the number of bits?\n",
    "    + Can you design a better code. Hint: what happens if we try to encode two independent observations? What if we encode $n$ observations jointly?\n",
    "5. Softmax is a misnomer for the mapping introduced above (but everyone in deep learning uses it). The real softmax is defined as $\\mathrm{RealSoftMax}(a, b) = \\log (\\exp(a) + \\exp(b))$.\n",
    "    + Prove that $\\mathrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$.\n",
    "    + Prove that this holds for $\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b)$, provided that $\\lambda > 0$.\n",
    "    + Show that for $\\lambda \\to \\infty$ we have $\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$.\n",
    "    + What does the soft-min look like?\n",
    "    + Extend this to more than two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
