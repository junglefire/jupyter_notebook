{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import d2l\n",
    "import os\n",
    "\n",
    "from mxnet import autograd, np, npx, gluon, init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  06. Convolutional Neural Networks\n",
    "In earlier chapters, we came up against image data, for which each example consists of a 2D grid of pixels. Depending on whether we're handling black-and-white or color images, each pixel location might be associated with either one or multiple numerical values, respectively. Until now, our way of dealing with this rich structure was deeply unsatisfying. We simply discarded each image's spatial structure by flattening them into 1D vectors, feeding them through a (fully connected) MLP. Because these networks invariant to the order of the features we could get similar results regardless of whether we preserve an order corresponding to the spatial structure of the pixels or if we permute the columns of our design matrix before fitting the MLP's parameters. Preferably, we would leverage our prior knowledge that nearby pixels are typically related to each other, to build efficient models for learning from image data.\n",
    "\n",
    "This chapter introduces convolutional neural networks (CNNs), a powerful family of neural networks that were designed for precisely this purpose. CNN-based architectures are now ubiquitous in the field of computer vision, and have become so dominant that hardly anyone today would develop a commercial application or enter a competition related to image recognition, object detection, or semantic segmentation, without building off of this approach.\n",
    "\n",
    "Modern ConvNets, as they are called colloquially owe their design to inspirations from biology, group theory, and a healthy dose of experimental tinkering. In addition to their sample efficiency in achieving accurate models, convolutional neural networks tend to be computationally efficient, both because they require fewer parameters than dense architectures and because convolutions are easy to parallelize across GPU cores. Consequently, practitioners often apply CNNs whenever possible, and increasingly they have emerged as credible competitors even on tasks with 1D sequence structure, such as audio, text, and time series analysis, where recurrent neural networks are conventionally used. Some clever adaptations of CNNs have also brought them to bear on graph-structured data and in recommender systems.\n",
    "\n",
    "First, we will walk through the basic operations that comprise the backbone of all convolutional networks. These include the convolutional layers themselves, nitty-gritty details including padding and stride, the pooling layers used to aggregate information across adjacent spatial regions, the use of multiple channels (also called filters) at each layer, and a careful discussion of the structure of modern architectures. We will conclude the chapter with a full working example of `LeNet`, the first convolutional network successfully deployed, long before the rise of modern deep learning. In the next chapter, we will dive into full implementations of some popular and comparatively recent CNN architectures whose designs represent most of the techniques commonly used by modern practitioners.\n",
    "\n",
    "## 6.1 From Dense Layers to Convolutions\n",
    "The models that we have discussed so far remain (to this day) appropriate options when we are dealing with tabular data. By tabular, we mean that the data consists of rows corresponding to examples and columns corresponding to features. With tabular data, we might anticipate that the patterns we seek could involve interactions among the features, but we do not assume any structure a priori concerning how the features interact.\n",
    "\n",
    "Sometimes, we truly lack knowledge to guide the construction of craftier architectures. In these cases, a multilayer perceptron may be the best that we can do. However, for high-dimensional perceptual data, these structure-less networks can grow unwieldy.\n",
    "\n",
    "For instance, let's return to our running example of distinguishing cats from dogs. Say that we do a thorough job in data collection, collecting an annotated dataset of 1-megapixel photographs. This means that each input to the network has 1 million dimensions. Even an aggressive reduction to 1,000 hidden dimensions would require a dense (fully connected) layer characterized by $10^9$ parameters. Unless we have lots of GPUs, a talent for distributed optimization, and an extraordinary amount of patience, learning the parameters of this network may turn out to be infeasible.\n",
    "\n",
    "A careful reader might object to this argument on the basis that 1 megapixel resolution may not be necessary. However, while we might be able to get away with 100,000 pixels, our hidden layer of size $1000$ grossly underestimated the number of hidden nodes that it takes to learn good representations of images, so a practical system will still require billions of parameters. Moreover, learning a classifier by fitting so many parameters might require collecting an enormous dataset. And yet today both humans and computers are able to distinguish cats from dogs quite well, seemingly contradicting these intuitions. That is because images exhibit rich structure that can be exploited by humans and machine learning models alike. Convolutional neural networks are one creative way that machine learning has embraced for exploiting some of the known structure in natural images.\n",
    "\n",
    "### 6.1.1 Invariances\n",
    "Imagine that you want to detect an object in an image. It seems reasonable that whatever method we use to recognize objects should not be overly concerned with the precise location of the object in the image. Ideally, our system should exploit this knowledge. Pigs usually do not fly and planes usually do not swim. Nonetheless, we should still recognize a pig were one to appear at the top of the image. We can draw some inspiration here from the children's game 'Where's Waldo' (depicted in `Fig. 6.1.1`). \n",
    "\n",
    "<img src=\"images/06_01.jpg\" style=\"width:700px;\"/>\n",
    "\n",
    "The game consists of a number of chaotic scenes bursting with activity. Waldo shows up somewhere in each, typically lurking in some unlikely location. The reader's goal is to locate him. Despite his characteristic outfit, this can be surprisingly difficult, due to the large number of distractions. However, what Waldo looks like does not depend upon where Waldo is located. We could sweep the image with a Waldo detector that could assign a score to each patch, indicating the likelihood that the patch contains Waldo. CNNs systematize this idea of spatial invariance, exploiting it to learn useful representations with few parameters.\n",
    "\n",
    "We can now make these intuitions more concrete by enumerating a few desiderata to guide our design of a neural network architecture suitable for computer vision:\n",
    "1. In the earliest layers, our network should respond similarly to the same patch, regardless of where it appears in the image (translation invariance).\n",
    "2. The earliest layers of the network should focus on local regions, without regard for the contents of the image in distant regions (locality). Eventually, these local representations can be aggregated to make predictions at the whole image level.\n",
    "\n",
    "Let us see how this translates into mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Constraining the MLP\n",
    "To start off, we can consider an MLP with $h \\times w$ images as inputs (represented as matrices in math, and as 2D arrays in code), and hidden representations **similarly organized as** $h \\times w$ **matrices / 2D arrays**. Let that sink in, we now conceive of not only the inputs but also the hidden representations as possessing spatial structure. Let $x[i, j]$ and $h[i, j]$ denote pixel location $(i, j)$ in the input image and hidden representation, respectively. Consequently, to have each of the $h \\times w$ hidden nodes receive input from each of the $h \\times w$ inputs, we would switch from using weight matrices (as we did previously in MLPs) to representing our parameters as four-dimensional weight tensors.\n",
    "\n",
    "We could formally express this dense layer as follows:\n",
    "\n",
    "$$h[i, j] = u[i, j] + \\sum_{k, l} W[i, j, k, l] \\cdot x[k, l] = u[i, j] + \\sum_{a, b} V[i, j, a, b] \\cdot x[i+a, j+b].$$\n",
    "\n",
    "The switch from $W$ to $V$ is entirely cosmetic (for now) since there is a one-to-one correspondence between coefficients in both tensors. We simply re-index the subscripts $(k, l)$ such that $k = i+a$ and $l = j+b$. In other words, we set $V[i, j, a, b] = W[i, j, i+a, j+b]$. The indices $a, b$ run over both positive and negative offsets, covering the entire image. For any given location $(i, j)$ in the hidden layer $h[i, j]$, we compute its value by summing over pixels in $x$, centered around $(i, j)$ and weighted by $V[i, j, a, b]$.\n",
    "\n",
    "Now let us invoke the first principle established above: `translation invariance`. This implies that a shift in the inputs $x$ should simply lead to a shift in the activations $h$. This is only possible if $V$ and $u$ do not actually depend on $(i, j)$, i.e., we have $V[i, j, a, b] = V[a, b]$ and $u$ is a constant. As a result, we can simplify the definition for $h$.\n",
    "\n",
    "$$h[i, j] = u + \\sum_{a, b} V[a, b] \\cdot x[i+a, j+b].$$\n",
    "\n",
    "This is a convolution! We are effectively weighting pixels $(i+a, j+b)$ in the vicinity of $(i, j)$ with coefficients $V[a, b]$ to obtain the value $h[i, j]$. Note that $V[a, b]$ needs many fewer coefficients than $V[i, j, a, b]$. For a 1 megapixel image, it has at most 1 million coefficients. This is 1 million fewer parameters since it no longer depends on the location within the image. We have made significant progress!\n",
    "\n",
    "Now let us invoke the second principle: `locality`. As motivated above, we believe that we should not have to look very far away from $(i, j)$ in order to glean relevant information to assess what is going on at $h[i, j]$. This means that outside some range $|a|, |b| > \\Delta$, we should set $V[a, b] = 0$. Equivalently, we can rewrite $h[i, j]$ as\n",
    "\n",
    "$$h[i, j] = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} V[a, b] \\cdot x[i+a, j+b].$$\n",
    "\n",
    "This, in a nutshell, is a **convolutional layer**. When the local region (also called a `receptive field`) is small, the difference as compared to a fully-connected network can be dramatic. While previously, we might have required billions of parameters to represent just a single layer in an image-processing network, we now typically need just a few hundred, without altering the dimensionality of either the inputs or the hidden representations. The price paid for this drastic reduction in parameters is that our features are now translation invariant and that our layer can only incorporate local information, when determining the value of each hidden activation. All learning depends on imposing inductive bias. When that bias agrees with reality, we get sample-efficient models that generalize well to unseen data. But of course, if those biases do not agree with reality, e.g., if images turned out not to be translation invariant, our models might struggle even to fit our training data.\n",
    "\n",
    "### 6.1.3 Convolutions\n",
    "Before going further, we should briefly review why the above operation is called a `convolution`. In mathematics, the convolution between two functions, say $f, g: \\mathbb{R}^d \\to R$ is defined as\n",
    "\n",
    "$$\\big[f\\circledast g\\big]\\big(x\\big) = \\int_{\\mathbb{R}^d}f(z)g(x-z)dz.$$\n",
    "\n",
    "That is, we measure the overlap between $f$ and $g$ when one function is \"flipped\" and shifted by $x$. Whenever we have discrete objects, the integral turns into a sum. For instance, for vectors defined on $\\ell_2$, i.e., the set of square summable infinite dimensional vectors with index running over $\\mathbb{Z}$ we obtain the following definition.\n",
    "\n",
    "$$\\big[f\\circledast g\\big]\\big(i\\big) = \\sum_a f(a) g(i-a).$$\n",
    "\n",
    "For two-dimensional arrays, we have a corresponding sum with indices $(i, j)$ for $f$ and $(i-a, j-b)$ for $g$ respectively. This looks similar to definition above, with one major difference. Rather than using $(i+a, j+b)$, we are using the difference instead. Note, though, that this distinction is mostly cosmetic since we can always match the notation by using $\\tilde{V}[a, b] = V[-a, -b]$ to obtain $h = x \\circledast \\tilde{V}$. Our original definition more properly describes a cross correlation. We will come back to this in the following section.\n",
    "\n",
    "### 6.1.4 Waldo Revisited\n",
    "Returning to our Waldo detector, let's see what this looks like. The convolutional layer picks windows of a given size and weighs intensities according to the mask $V$, as demonstrated in `Fig. 6.1.2`. We might aim to learn a model so that wherever the \"waldoness\" is highest, we should find a peak in the hidden layer activations.\n",
    "\n",
    "<img src=\"images/06_02.jpg\" style=\"width:500px;\"/>\n",
    "\n",
    "There's just one problem with this approach. So far, we blissfully ignored that images consist of 3 channels: red, green and blue. In reality, images are not two-dimensional objects but rather $3^{\\mathrm{rd}}$ order tensors, characterized by a height, width, and channel, e.g., with shape $1024 \\times 1024 \\times 3$ pixels. While the first two of these axes concern spatial relationships, the $3^{\\mathrm{rd}}$ can be regarded as assigning a multidimensional representation to each pixel location.\n",
    "\n",
    "We thus index $\\mathbf{x}$ as $x[i, j, k]$. The convolutional mask has to adapt accordingly. Instead of $V[a, b]$, we now have $V[a, b, c]$.\n",
    "\n",
    "Moreover, just as our input consists of a $3^{\\mathrm{rd}}$ order tensor, it turns out to be a good idea to similarly formulate our hidden representations as $3^{\\mathrm{rd}}$ order tensors. In other words, rather than just having a single activation corresponding to each spatial location, we want an entire vector of hidden activations corresponding to each spatial location. We could think of the hidden representation as comprising a number of 2D grids stacked on top of each other. As in the inputs, these are sometimes called `channels`. They are also sometimes called `feature maps`, as each provides a spatialized set of learned features to the subsequent layer. Intuitively, you might imagine that at lower layers, some channels could become specialized to recognize edges, others to recognize textures, etc. To support multiple channels in both inputs and hidden activations, we can add a fourth coordinate to $V$: $V[a, b, c, d]$. Putting everything together we have:\n",
    "$$h[i, j, k] = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c V[a, b, c, k] \\cdot x[i+a, j+b, c].$$\n",
    "\n",
    "This is the definition of a convolutional neural network layer. There are still many operations that we need to address. For instance, we need to figure out how to combine all the activations to a single output (e.g., whether there is a Waldo anywhere in the image). We also need to decide how to compute things efficiently, how to combine multiple layers, appropriate activation functions, and how to make reasonable design choices to yield networks that are effective in practice. We turn to these issues in the remainder of the chapter.\n",
    "\n",
    "##### Summary\n",
    "+ Translation invariance in images implies that all patches of an image will be treated in the same manner.\n",
    "+ Locality means that only a small neighborhood of pixels will be used to compute the corresponding hidden activation.\n",
    "+ Channels on input and output allow our model to capture multiple aspects of an image at each spatial location.\n",
    "\n",
    "##### Exercises\n",
    "1. Assume that the size of the convolution mask is $\\Delta = 0$. Show that in this case the convolutional mask implements an MLP independently for each set of channels.\n",
    "2. Why might translation invariance not be a good idea after all? When might it not make sense to allow for pigs to fly?\n",
    "3. What problems must we deal with when deciding how to treat activations corresponding to pixel locations at the boundary of an image?\n",
    "4. Describe an analogous convolutional layer for audio.\n",
    "5. Do you think that convolutional layers might also be applicable for text data? Why or why not?\n",
    "6. Prove that $f \\circledast g = g \\circledast f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Convolutions for Images\n",
    "Now that we understand how convolutional layers work in theory, we are ready to see how they work in practice. Building on our motivation of convolutional neural networks as efficient architectures for epxloring structure in image data, we stick with images as our running example.\n",
    "\n",
    "### 6.2.1 The Cross-Correlation Operator\n",
    "Recall that strictly speaking, convolutional layers are a (slight) misnomer, since the operations they express are more accurately described as cross correlations. In a convolutional layer, an input array and a correlation kernel array are combined to produce an output array through a cross-correlation operation. Let's ignore channels for now and see how this works with two-dimensional data and hidden representations. In `Fig. 6.2.1`, the input is a two-dimensional array with a height of 3 and width of 3. We mark the shape of the array as $3 \\times 3$ or ($3$, $3$). The height and width of the kernel are both $2$. Note that in the deep learning research community, this object may be referred to as a `convolutional kernel`, a `filter`, or simply the layer's `weights`. The shape of the kernel window is given by the height and width of the kernel (here it is $2 \\times 2$).\n",
    "\n",
    "<img src=\"images/06_03.png\" style=\"width:600px;\"/>\n",
    "\n",
    "In the two-dimensional cross-correlation operation, we begin with the convolution window positioned at the top-left corner of the input array and slide it across the input array, both from left to right and top to bottom. When the convolution window slides to a certain position, the input subarray contained in that window and the kernel array are multiplied (elementwise) and the resulting array is summed up yielding a single scalar value. This result gives the value of the output array at the corresponding location. Here, the output array has a height of 2 and width of 2 and the four elements are derived from the two-dimensional cross-correlation operation:\n",
    "$$ 0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\ 1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\ 3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\ 4\\times0+5\\times1+7\\times2+8\\times3=43. $$\n",
    "\n",
    "Note that along each axis, the output is slightly smaller than the input. Because the kernel has width and height greater than one, we can only properly compute the cross-correlation for locations where the kernel fits wholly within the image, the output size is given by the input size $H \\times W$ minus the size of the convolutional kernel $h \\times w$ via $(H-h+1) \\times (W-w+1)$. This is the case since we need enough space to *shift* the convolutional kernel across the image (later we will see how to keep the size unchanged by padding the image with zeros around its boundary such that there is enough space to shift the kernel). Next, we implement this process in the `corr2d` function, which accepts the input array $X$ and kernel array $K$ and returns the output array $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = np.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the input array $X$ and the kernel array $K$ from the figure above to validate the output of the above implementation of the two-dimensional cross-correlation operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19., 25.],\n",
       "       [37., 43.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = np.array([[0, 1], [2, 3]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Convolutional Layers\n",
    "A convolutional layer cross-correlates the input and kernels and adds a scalar bias to produce an output. The two parameters of the convolutional layer are the kernel and the scalar bias. When training models based on convolutional layers, we typically initialize the kernels randomly, just as we would with a fully connected layer.\n",
    "\n",
    "We are now ready to implement a two-dimensional convolutional layer based on the `corr2d` function defined above. In the `__init__` constructor function, we declare weight and bias as the two model parameters. The forward computation function forward calls the `corr2d` function and adds the bias. As with $h \\times w$ cross-correlation we also refer to convolutional layers as $h \\times w$ convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Block):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=kernel_size)\n",
    "        self.bias = self.params.get('bias', shape=(1,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight.data()) + self.bias.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Object Edge Detection in Images\n",
    "Let's take a moment to parse a simple application of a convolutional layer: detecting the edge of an object in an image by finding the location of the pixel change. First, we construct an *image* of $6\\times 8$ pixels. The middle four columns are black (0) and the rest are white (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct a kernel $K$ with a height of $1$ and width of $2$. When we perform the cross-correlation operation with the input, if the horizontally adjacent elements are the same, the output is 0. Otherwise, the output is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[1, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to perform the cross-correlation operation with arguments $X$ (our input) and $K$ (our kernel). As you can see, we detect 1 for the edge from white to black and -1 for the edge from black to white. All other outputs take value $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the kernel to the transposed image. As expected, it vanishes. The kernel $K$ only detects vertical edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.T, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 Learning a Kernel\n",
    "Designing an edge detector by finite differences $[1, -1]$ is neat if we know this is precisely what we are looking for. However, as we look at larger kernels, and consider successive layers of convolutions, it might be impossible to specify precisely what each filter should be doing manually.\n",
    "\n",
    "Now let us see whether we can learn the kernel that generated $Y$ from $X$ by looking at the (input, output) pairs only. We first construct a convolutional layer and initialize its kernel as a random array. Next, in each iteration, we will use the squared error to compare $Y$ to the output of the convolutional layer. We can then calculate the gradient to update the weight. For the sake of simplicity, in this convolutional layer, we will ignore the bias.\n",
    "\n",
    "We previously constructed the `Conv2D` class. However, since we used single-element assignments, `autograd` has some trouble finding the gradient. Instead, we use the built-in `Conv2D` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2, loss 4.949\n",
      "batch 4, loss 0.831\n",
      "batch 6, loss 0.140\n",
      "batch 8, loss 0.024\n",
      "batch 10, loss 0.004\n"
     ]
    }
   ],
   "source": [
    "# Construct a convolutional layer with 1 output channel (channels will be introduced \n",
    "# in the following section) and a kernel array shape of (1, 2)\n",
    "conv2d = nn.Conv2D(1, kernel_size=(1, 2))\n",
    "conv2d.initialize()\n",
    "\n",
    "# The two-dimensional convolutional layer uses four-dimensional input and output in \n",
    "# the format of (example, channel, height, width), where the batch size (number of \n",
    "# examples in the batch) and the number of channels are both 1\n",
    "X = X.reshape(1, 1, 6, 8)\n",
    "Y = Y.reshape(1, 1, 6, 7)\n",
    "\n",
    "for i in range(10):\n",
    "    with autograd.record():\n",
    "        Y_hat = conv2d(X)\n",
    "        l = (Y_hat - Y) ** 2\n",
    "    l.backward()\n",
    "    # For the sake of simplicity, we ignore the bias here\n",
    "    conv2d.weight.data()[:] -= 3e-2*conv2d.weight.grad()\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'batch {i+1}, loss {float(l.sum()):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error has dropped to a small value after 10 iterations. Now we will take a look at the kernel array we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9895   , -0.9873705]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data().reshape(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the learned kernel array is remarkably close to the kernel array $K$ we defined earlier.\n",
    "\n",
    "### 6.2.5 Cross-Correlation and Convolution\n",
    "Recall our observation from the previous section of the correspondence between the cross-correlation and convolution operators. The figure above makes this correspondence apparent. Simply flip the kernel from the bottom left to the top right. In this case, the indexing in the sum is reverted, yet the same result can be obtained. In keeping with standard terminology with deep learning literature, we will continue to refer to the cross-correlation operation as a convolution even though, strictly-speaking, it is slightly different.\n",
    "\n",
    "##### Summary\n",
    "+ The core computation of a two-dimensional convolutional layer is a two-dimensional cross-correlation operation. In its simplest form, this performs a cross-correlation operation on the two-dimensional input data and the kernel, and then adds a bias.\n",
    "+ We can design a kernel to detect edges in images.\n",
    "+ We can learn the kernel's parameters from data.\n",
    "\n",
    "##### Exercises\n",
    "1. Construct an image X with diagonal edges.\n",
    "    + What happens if you apply the kernel K to it?\n",
    "    + What happens if you transpose X?\n",
    "    + What happens if you transpose K?\n",
    "2. When you try to automatically find the gradient for the Conv2D class we created, what kind of error message do you see?\n",
    "3. How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel arrays?\n",
    "4. Design some kernels manually.\n",
    "    + What is the form of a kernel for the second derivative?\n",
    "    + What is the kernel for the Laplace operator?\n",
    "    + What is the kernel for an integral?\n",
    "    + What is the minimum size of a kernel to obtain a derivative of degree $d$?\n",
    "   \n",
    "   \n",
    "## 6.3 Padding and Stride\n",
    "In the previous example, our input had both a height and width of $3$ and our convolution kernel had both a height and width of $2$, yielding an output representation with dimension $2\\times2$. In general, assuming the input shape is $n_h\\times n_w$ and the convolution kernel window shape is $k_h\\times k_w$, then the output shape will be\n",
    "$$(n_h-k_h+1) \\times (n_w-k_w+1).$$\n",
    "\n",
    "Therefore, the output shape of the convolutional layer is determined by the shape of the input and the shape of the convolution kernel window.\n",
    "\n",
    "In several cases, we incorporate techniques, including padding and strided convolutions, that affect the size of the output. As motivation, note that since kernels generally have width and height greater than $1$, after applying many successive convolutions, we tend to wind up with outputs that are considerably smaller than our input. If we start with a $240 \\times 240$ pixel image, $10$ layers of $5 \\times 5$ convolutions reduce the image to $200 \\times 200$ pixels, slicing off $30 \\%$ of the image and with it obliterating any interesting information on the boundaries of the original image. `Padding` is the most popular tool for handling this issue.\n",
    "\n",
    "In other cases, we may want to reduce the dimensionality drastically, e.g., if we find the original input resolution to be unwieldy. `Strided` convolutions are a popular technique that can help in these instances.\n",
    "\n",
    "### 6.3.1 Padding\n",
    "As described above, one tricky issue when applying convolutional layers is that we tend to lose pixels on the perimeter of our image. Since we typically use small kernels, for any given convolution, we might only lose a few pixels, but this can add up as we apply many successive convolutional layers. One straightforward solution to this problem is to add extra pixels of filler around the boundary of our input image, thus increasing the effective size of the image. Typically, we set the values of the extra pixels to $0$. In `Fig. 6.3.1`, we pad a $3 \\times 3$ input, increasing its size to $5 \\times 5$. The corresponding output then increases to a $4 \\times 4$ matrix.\n",
    "\n",
    "<img src=\"images/06_04.png\" style=\"width:600px;\"/>\n",
    "\n",
    "In general, if we add a total of $p_h$ rows of padding (roughly half on top and half on bottom) and a total of $p_w$ columns of padding (roughly half on the left and half on the right), the output shape will be\n",
    "\n",
    "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
    "\n",
    "This means that the height and width of the output will increase by $p_h$ and $p_w$ respectively.\n",
    "\n",
    "In many cases, we will want to set $p_h=k_h-1$ and $p_w=k_w-1$ to give the input and output the same height and width. This will make it easier to predict the output shape of each layer when constructing the network. Assuming that $k_h$ is even here, we will pad $p_h/2$ rows on both sides of the height. If $k_h$ is odd, one possibility is to pad $\\lceil p_h/2\\rceil$ rows on the top of the input and $\\lfloor p_h/2\\rfloor$ rows on the bottom. We will pad both sides of the width in the same way.\n",
    "\n",
    "Convolutional neural networks commonly use convolutional kernels with odd height and width values, such as $1$, $3$, $5$, or $7$. Choosing odd kernel sizes has the benefit that we can preserve the spatial dimensionality while padding with the same number of rows on top and bottom, and the same number of columns on left and right.\n",
    "\n",
    "Moreover, this practice of using odd kernels and padding to precisely preserve dimensionality offers a clerical benefit. For any two-dimensional array $X$, when the kernels size is odd and the number of padding rows and columns on all sides are the same, producing an output with the same height and width as the input, we know that the output $Y[i, j]$ is calculated by cross-correlation of the input and convolution kernel with the window centered on $X[i, j]$.\n",
    "\n",
    "In the following example, we create a two-dimensional convolutional layer with a height and width of $3$ and apply $1$ pixel of padding on all sides. Given an input with a height and width of $8$, we find that the height and width of the output is also $8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For convenience, we define a function to calculate the convolutional layer.\n",
    "# This function initializes the convolutional layer weights and performs corresponding \n",
    "# dimensionality elevations and reductions on the input and output\n",
    "def comp_conv2d(conv2d, X):\n",
    "    conv2d.initialize()\n",
    "    # (1, 1) indicates that the batch size and the number of channels\n",
    "    # (described in later chapters) are both 1\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # Exclude the first two dimensions that do not interest us: batch and channel\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "# Note that here 1 row or column is padded on either side, so a total of 2\n",
    "# rows or columns are added\n",
    "conv2d = nn.Conv2D(1, kernel_size=3, padding=1)\n",
    "X = np.random.uniform(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the height and width of the convolution kernel are different, we can make the output and input have the same height and width by setting different padding numbers for height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we use a convolution kernel with a height of 5 and a width of 3. The padding numbers \n",
    "# on both sides of the height and width are 2 and 1, respectively\n",
    "conv2d = nn.Conv2D(1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Stride\n",
    "When computing the cross-correlation, we start with the convolution window at the top-left corner of the input array, and then slide it over all locations both down and to the right. In previous examples, we default to sliding one pixel at a time. However, sometimes, either for computational efficiency or because we wish to downsample, we move our window more than one pixel at a time, skipping the intermediate locations.\n",
    "\n",
    "We refer to the number of rows and columns traversed per slide as the `stride`. So far, we have used strides of $1$, both for height and width. Sometimes, we may want to use a larger stride. `Fig. 6.3.2` shows a two-dimensional cross-correlation operation with a stride of $3$ vertically and $2$ horizontally. We can see that when the second element of the first column is output, the convolution window slides down three rows. The convolution window slides two columns to the right when the second element of the first row is output. When the convolution window slides three columns to the right on the input, there is no output because the input element cannot fill the window (unless we add another column of padding).\n",
    "\n",
    "<img src=\"images/06_05.png\" style=\"width:600px;\"/>\n",
    "\n",
    "In general, when the stride for the height is $s_h$ and the stride for the width is $s_w$, the output shape is\n",
    "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n",
    "\n",
    "If we set $p_h=k_h-1$ and $p_w=k_w-1$, then the output shape will be simplified to $\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$. Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be $(n_h/s_h) \\times (n_w/s_w)$.\n",
    "\n",
    "Below, we set the strides on both the height and width to $2$, thus halving the input height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2D(1, kernel_size=3, padding=1, strides=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at a slightly more complicated example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2D(1, kernel_size=(3, 5), padding=(0, 1), strides=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of brevity, when the padding number on both sides of the input height and width are $p_h$ and $p_w$ respectively, we call the padding $(p_h, p_w)$. Specifically, when $p_h = p_w = p$, the padding is $p$. When the strides on the height and width are $s_h$ and $s_w$, respectively, we call the stride $(s_h, s_w)$. Specifically, when $s_h = s_w = s$, the stride is $s$. By default, the padding is $0$ and the stride is $1$. In practice, we rarely use inhomogeneous strides or padding, i.e., we usually have $p_h = p_w$ and $s_h = s_w$.\n",
    "\n",
    "##### Summary\n",
    "+ Padding can increase the height and width of the output. This is often used to give the output the same height and width as the input.\n",
    "+ The stride can reduce the resolution of the output, for example reducing the height and width of the output to only $1/n$ of the height and width of the input ($n$ is an integer greater than $1$).\n",
    "+ Padding and stride can be used to adjust the dimensionality of the data effectively.\n",
    "\n",
    "##### Exercises\n",
    "1. For the last example in this section, use the shape calculation formula to calculate the output shape to see if it is consistent with the experimental results.\n",
    "2. Try other padding and stride combinations on the experiments in this section.\n",
    "3. For audio signals, what does a stride of $2$ correspond to?\n",
    "4. What are the computational benefits of a stride larger than $1$.\n",
    "\n",
    "\n",
    "## 6.4 Multiple Input and Output Channels\n",
    "While we have described the multiple channels that comprise each image (e.g., color images have the standard RGB channels to indicate the amount of red, green and blue), until now, we simplified all of our numerical examples by working with just a single input and a single output channel. This has allowed us to think of our inputs, convolutional kernels, and outputs each as two-dimensional arrays.\n",
    "\n",
    "When we add channels into the mix, our inputs and hidden representations both become three-dimensional arrays. For example, each RGB input image has shape $3\\times h\\times w$. We refer to this axis, with a size of 3, as the channel dimension. In this section, we will take a deeper look at convolution kernels with multiple input and multiple output channels.\n",
    "\n",
    "### 6.4.1 Multiple Input Channels\n",
    "When the input data contains multiple channels, we need to construct a convolution kernel with the same number of input channels as the input data, so that it can perform cross-correlation with the input data. Assuming that the number of channels for the input data is $c_i$, the number of input channels of the convolution kernel also needs to be $c_i$. If our convolution kernel's window shape is $k_h\\times k_w$, then when $c_i=1$, we can think of our convolution kernel as just a two-dimensional array of shape $k_h\\times k_w$.\n",
    "\n",
    "However, when $c_i>1$, we need a kernel that contains an array of shape $k_h\\times k_w$ for each input channel. Concatenating these $c_i$ arrays together yields a convolution kernel of shape $c_i\\times k_h\\times k_w$. Since the input and convolution kernel each have $c_i$ channels, we can perform a cross-correlation operation on the two-dimensional array of the input and the two-dimensional kernel array of the convolution kernel for each channel, adding the $c_i$ results together (summing over the channels) to yield a two-dimensional array. This is the result of a two-dimensional cross-correlation between multi-channel input data and a multi-input channel convolution kernel.\n",
    "\n",
    "In `Fig. 6.4.1`, we demonstrate an example of a two-dimensional cross-correlation with two input channels. The shaded portions are the first output element as well as the input and kernel array elements used in its computation: $(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$.\n",
    "\n",
    "<img src=\"images/06_06.png\" style=\"width:600px;\"/>\n",
    "\n",
    "To make sure we really understand what is going on here, we can implement cross-correlation operations with multiple input channels ourselves. Notice that all we are doing is performing one cross-correlation operation per channel and then adding up the results using the `add_n` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    # First, traverse along the 0th dimension (channel dimension) of X and K.\n",
    "    # Then, add them together.\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the input array $X$ and the kernel array $K$ corresponding to the values in the above diagram to validate the output of the cross-correlation operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 56.,  72.],\n",
       "       [104., 120.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],\n",
    "              [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
    "K = np.array([[[0, 1], [2, 3]], \n",
    "              [[1, 2], [3, 4]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 Multiple Output Channels\n",
    "Regardless of the number of input channels, so far we always ended up with one output channel. However, as we discussed earlier, it turns out to be essential to have multiple channels at each layer. In the most popular neural network architectures, we actually increase the channel dimension as we go higher up in the neural network, typically downsampling to trade off spatial resolution for greater channel depth. Intuitively, you could think of each channel as responding to some different set of features. Reality is a bit more complicated than the most naive interpretations of this intuition since representations are not learned independent but are rather optimized to be jointly useful. So it may not be that a single channel learns an edge detector but rather that some direction in channel space corresponds to detecting edges.\n",
    "\n",
    "Denote by $c_i$ and $c_o$ the number of input and output channels, respectively, and let $k_h$ and $k_w$ be the height and width of the kernel. To get an output with multiple channels, we can create a kernel array of shape $c_i\\times k_h\\times k_w$ for each output channel. We concatenate them on the output channel dimension, so that the shape of the convolution kernel is $c_o\\times c_i\\times k_h\\times k_w$. In cross-correlation operations, the result on each output channel is calculated from the convolution kernel corresponding to that output channel and takes input from all channels in the input array.\n",
    "\n",
    "We implement a cross-correlation function to calculate the output of multiple channels as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # Traverse along the 0th dimension of K, and each time, perform\n",
    "    # cross-correlation operations with input X. All of the results are merged\n",
    "    # together using the stack function\n",
    "    return np.stack([corr2d_multi_in(X, k) for k in K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a convolution kernel with 3 output channels by concatenating the kernel array $K$ with $K+1$ (plus one for each element in $K$) and $K+2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 2, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = np.stack((K, K + 1, K + 2))\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we perform cross-correlation operations on the input array $X$ with the kernel array $K$. Now the output contains 3 channels. The result of the first channel is consistent with the result of the previous input array $X$ and the multi-input channel, single-output channel kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 56.,  72.],\n",
       "        [104., 120.]],\n",
       "\n",
       "       [[ 76., 100.],\n",
       "        [148., 172.]],\n",
       "\n",
       "       [[ 96., 128.],\n",
       "        [192., 224.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 $1\\times 1$ Convolutional Layer\n",
    "At first, a $1 \\times 1$ convolution, i.e., $k_h = k_w = 1$, does not seem to make much sense. After all, a convolution correlates adjacent pixels. A $1 \\times 1$ convolution obviously does not. Nonetheless, they are popular operations that are sometimes included in the designs of complex deep networks. Let us see in some detail what it actually does.\n",
    "\n",
    "Because the minimum window is used, the $1\\times 1$ convolution loses the ability of larger convolutional layers to recognize patterns consisting of interactions among adjacent elements in the height and width dimensions. The only computation of the $1\\times 1$ convolution occurs on the channel dimension.\n",
    "\n",
    "`Fig. 6.4.2` shows the cross-correlation computation using the $1\\times 1$ convolution kernel with 3 input channels and 2 output channels. Note that the inputs and outputs have the same height and width. Each element in the output is derived from a linear combination of elements at the same position in the input image. You could think of the $1\\times 1$ convolutional layer as constituting a fully-connected layer applied at every single pixel location to transform the $c_i$ corresponding input values into $c_o$ output values. Because this is still a convolutional layer, the weights are tied across pixel location. Thus the $1\\times 1$ convolutional layer requires $c_o\\times c_i$ weights (plus the bias terms).\n",
    "\n",
    "<img src=\"images/06_07.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Let us check whether this works in practice: we implement the $1 \\times 1$ convolution using a fully-connected layer. The only thing is that we need to make some adjustments to the data shape before and after the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape(c_i, h * w)\n",
    "    K = K.reshape(c_o, c_i)\n",
    "    Y = np.dot(K, X)  # Matrix multiplication in the fully connected layer\n",
    "    return Y.reshape(c_o, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing $1\\times 1$ convolution, the above function is equivalent to the previously implemented cross-correlation function `corr2d_multi_in_out`. Let us check this with some reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.uniform(size=(3, 3, 3))\n",
    "K = np.random.uniform(size=(2, 3, 1, 1))\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "\n",
    "np.abs(Y1 - Y2).sum() < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ Multiple channels can be used to extend the model parameters of the convolutional layer.\n",
    "+ The $1\\times 1$ convolutional layer is equivalent to the fully-connected layer, when applied on a per pixel basis.\n",
    "+ The $1\\times 1$ convolutional layer is typically used to adjust the number of channels between network layers and to control model complexity.\n",
    "\n",
    "##### Exercises\n",
    "1. Assume that we have two convolutional kernels of size $k_1$ and $k_2$ respectively (with no nonlinearity in between).\n",
    "    + Prove that the result of the operation can be expressed by a single convolution.\n",
    "    + What is the dimensionality of the equivalent single convolution?\n",
    "    + Is the converse true?\n",
    "2. Assume an input shape of $c_i\\times h\\times w$ and a convolution kernel with the shape $c_o\\times c_i\\times k_h\\times k_w$, padding of $(p_h, p_w)$, and stride of $(s_h, s_w)$.\n",
    "    + What is the computational cost (multiplications and additions) for the forward computation?\n",
    "    + What is the memory footprint?\n",
    "    + What is the memory footprint for the backward computation?\n",
    "    + What is the computational cost for the backward computation?\n",
    "3. By what factor does the number of calculations increase if we double the number of input channels $c_i$ and the number of output channels $c_o$? What happens if we double the padding?\n",
    "4. If the height and width of the convolution kernel is $k_h=k_w=1$, what is the complexity of the forward computation?\n",
    "5. Are the variables Y1 and Y2 in the last example of this section exactly the same? Why?\n",
    "6. How would you implement convolutions using matrix multiplication when the convolution window is not $1\\times 1$?\n",
    "\n",
    "\n",
    "## 6.5 Pooling\n",
    "Often, as we process images, we want to gradually reduce the spatial resolution of our hidden representations, aggregating information so that the higher up we go in the network, the larger the receptive field (in the input) to which each hidden node is sensitive.\n",
    "\n",
    "Often our ultimate task asks some global question about the image, e.g., *does it contain a cat*? So typically the nodes of our final layer should be sensitive to the entire input. By gradually aggregating information, yielding coarser and coarser maps, we accomplish this goal of ultimately learning a global representation, while keeping all of the advantages of convolutional layers at the intermediate layers of processing.\n",
    "\n",
    "Moreover, when detecting lower-level features, such as edges (as discussed in `Section 6.2`), we often want our representations to be somewhat invariant to translation. For instance, if we take the image $X$ with a sharp delineation between black and white and shift the whole image by one pixel to the right, i.e., $Z[i, j] = X[i, j+1]$, then the output for the new image $Z$ might be vastly different. The edge will have shifted by one pixel and with it all the activations. In reality, objects hardly ever occur exactly at the same place. In fact, even with a tripod and a stationary object, vibration of the camera due to the movement of the shutter might shift everything by a pixel or so (high-end cameras are loaded with special features to address this problem).\n",
    "\n",
    "This section introduces pooling layers, which serve the dual purposes of mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations.\n",
    "\n",
    "### 6.5.1 Maximum Pooling and Average Pooling\n",
    "Like convolutional layers, pooling operators consist of a fixed-shape window that is slid over all regions in the input according to its stride, computing a single output for each location traversed by the fixed-shape window (sometimes known as the `pooling window`). However, unlike the cross-correlation computation of the inputs and kernels in the convolutional layer, the pooling layer contains no parameters (there is no `filter`). Instead, pooling operators are deterministic, typically calculating either the maximum or the average value of the elements in the pooling window. These operations are called `maximum pooling` (`max pooling` for short) and `average pooling`, respectively.\n",
    "\n",
    "In both cases, as with the cross-correlation operator, we can think of the pooling window as starting from the top left of the input array and sliding across the input array from left to right and top to bottom. At each location that the pooling window hits, it computes the maximum or average value of the input subarray in the window (depending on whether `max` or `average` pooling is employed).\n",
    "\n",
    "<img src=\"images/06_08.png\" style=\"width:600px;\"/>\n",
    "\n",
    "The output array in `Fig. 6.5.1` above has a height of 2 and a width of 2. The four elements are derived from the maximum value of $\\text{max}$:\n",
    "$$ \\max(0, 1, 3, 4)=4,\\\\ \\max(1, 2, 4, 5)=5,\\\\ \\max(3, 4, 6, 7)=7,\\\\ \\max(4, 5, 7, 8)=8.\\\\ $$\n",
    "\n",
    "A pooling layer with a pooling window shape of $p \\times q$ is called a $p \\times q$ pooling layer. The pooling operation is called $p \\times q$ pooling.\n",
    "\n",
    "Let us return to the object edge detection example mentioned at the beginning of this section. Now we will use the output of the convolutional layer as the input for $2\\times 2$ maximum pooling. Set the convolutional layer input as $X$ and the pooling layer output as $Y$. Whether or not the values of $X[i, j]$ and $X[i, j+1]$ are different, or $X[i, j+1]$ and $X[i, j+2]$ are different, the pooling layer outputs all include $Y[i, j]=1$. That is to say, using the $2\\times 2$ maximum pooling layer, we can still detect if the pattern recognized by the convolutional layer moves no more than one element in height and width.\n",
    "\n",
    "In the code below, we implement the forward computation of the pooling layer in the `pool2d` function. This function is similar to the `corr2d` function in `Section 6.2`. However, here we have no kernel, computing the output as either the max or the average of each region in the input.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = np.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = np.max(X[i: i + p_h, j: j + p_w])\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the input array $X$ in the above diagram to validate the output of the two-dimensional maximum pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5.],\n",
       "       [7., 8.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time, we experiment with the average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3.],\n",
       "       [5., 6.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 Padding and Stride\n",
    "As with convolutional layers, pooling layers can also change the output shape. And as before, we can alter the operation to achieve a desired output shape by padding the input and adjusting the stride. We can demonstrate the use of padding and strides in pooling layers via the two-dimensional maximum pooling layer `MaxPool2D` shipped in `MXNet` `Gluon`'s `nn` module. We first construct an input data of shape $(1, 1, 4, 4)$, where the first two dimensions are batch and channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(16).reshape(1, 1, 4, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the stride in the `MaxPool2D` class has the same shape as the pooling window. Below, we use a pooling window of shape $(3, 3)$, so we get a stride shape of $(3, 3)$ by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[10.]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2D(3)\n",
    "# Because there are no model parameters in the pooling layer, we do not need\n",
    "# to call the parameter initialization function\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stride and padding can be manually specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 5.,  7.],\n",
       "         [13., 15.]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2D(3, padding=1, strides=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can specify an arbitrary rectangular pooling window and specify the padding and stride for height and width, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  3.],\n",
       "         [ 8., 11.],\n",
       "         [12., 15.]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2D((2, 3), padding=(1, 2), strides=(2, 3))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3 Multiple Channels\n",
    "When processing multi-channel input data, the pooling layer pools each input channel separately, rather than adding the inputs of each channel by channel as in a convolutional layer. This means that the number of output channels for the pooling layer is the same as the number of input channels. Below, we will concatenate arrays X and X+1 on the channel dimension to construct an input with 2 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]],\n",
       "\n",
       "        [[ 1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.],\n",
       "         [ 9., 10., 11., 12.],\n",
       "         [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X, X + 1), axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of output channels is still 2 after pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 5.,  7.],\n",
       "         [13., 15.]],\n",
       "\n",
       "        [[ 6.,  8.],\n",
       "         [14., 16.]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2D(3, padding=1, strides=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ Taking the input elements in the pooling window, the maximum pooling operation assigns the maximum value as the output and the average pooling operation assigns the average value as the output.\n",
    "+ One of the major functions of a pooling layer is to alleviate the excessive sensitivity of the convolutional layer to location.\n",
    "+ We can specify the padding and stride for the pooling layer.\n",
    "+ Maximum pooling, combined with a stride larger than 1 can be used to reduce the resolution.\n",
    "+ The pooling layer's number of output channels is the same as the number of input channels.\n",
    "\n",
    "##### Exercises\n",
    "1. Can you implement average pooling as a special case of a convolution layer? If so, do it.\n",
    "2. Can you implement max pooling as a special case of a convolution layer? If so, do it.\n",
    "3. What is the computational cost of the pooling layer? Assume that the input to the pooling layer is of size $c\\times h\\times w$, the pooling window has a shape of $p_h\\times p_w$ with a padding of $(p_h, p_w)$ and a stride of $(s_h, s_w)$.\n",
    "4. Why do you expect maximum pooling and average pooling to work differently?\n",
    "5. Do we need a separate minimum pooling layer? Can you replace it with another operation?\n",
    "6. Is there another operation between average and maximum pooling that you could consider (hint: recall the softmax)? Why might it not be so popular?\n",
    "\n",
    "\n",
    "## 6.6 Convolutional Neural Networks (LeNet)\n",
    "We now have all the ingredients required to assemble a fully-functional convolutional neural network. In our first encounter with image data, we applied a multilayer perceptron (`Section 4.2`) to pictures of clothing in the `Fashion-MNIST` dataset. To make this data amenable to multilayer perceptrons, we first flattened each image from a $28\\times28$ matrix into a fixed-length $784$-dimensional vector, and thereafter processed them with fully-connected layers. Now that we have a handle on convolutional layers, we can retain the spatial structure in our images. As an additional benefit of replacing dense layers with convolutional layers, we will enjoy more parsimonious models (requiring far fewer parameters).\n",
    "\n",
    "In this section, we will introduce `LeNet`, among the first published convolutional neural networks to capture wide attention for its performance on computer vision tasks. The model was introduced (and named for) `Yann Lecun`, then a researcher at `AT&T Bell Labs`, for the purpose of recognizing handwritten digits in images `LeNet5`. This work represented the culmination of a decade of research developing the technology. In 1989, `LeCun` published the first study to successfully train convolutional neural networks via `backpropagation`.\n",
    "\n",
    "At the time `LeNet` achieved outstanding results matching the performance of `Support Vector Machines` (`SVM`), then a dominant approach in supervised learning. `LeNet` was eventually adapted to recognize digits for processing deposits in ATM machines. To this day, some ATMs still run the code that `Yann` and his colleague `Leon Bottou` wrote in the 1990s!\n",
    "\n",
    "### 6.6.1 LeNet\n",
    "At a high level, LeNet consists of two parts: \n",
    "1. a convolutional encoder consisting of two convolutional layers\n",
    "2. a dense block consisting of three fully-connected layers\n",
    "\n",
    "The architecture is summarized in `Fig. 6.6.1`.\n",
    "\n",
    "<img src=\"images/06_09.png\" style=\"width:600px;\"/>\n",
    "\n",
    "The basic units in each convolutional block are a convolutional layer, a sigmoid activation function, and a subsequent average pooling operation. Note that while `ReLU`s and `max-pooling` work better, these discoveries had not yet been made in the 90s. Each convolutional layer uses a $5\\times 5$ kernel and a sigmoid activation function. These layers map spatially arranged inputs to a number of 2D feature maps, typically increasing the number of channels. The first convolutional layer has 6 output channels, while th second has 16. Each $2\\times2$ pooling operation (stride 2) reduces dimensionality by a factor of $4$ via spatial downsampling. The convolutional block emits an output with size given by $(\\text{batch size}, \\text{channel}, \\text{height}, \\text{width})$.\n",
    "\n",
    "In order to pass output from the convolutional block to the fully-connected block, we must flatten each example in the minibatch. In other words, we take this 4D input and transform it into the 2D input expected by fully-connected layers: as a reminder, the 2D representation that we desire has uses the first dimension to index examples in the minibatch and the second to give the flat vector representation of each example. `LeNet`'s fully-connected layer block has three fully-connected layers, with 120, 84, and 10 outputs, respectively. Because we are still performing classification, the 10-dimensional output layer corresponds to the number of possible output classes.\n",
    "\n",
    "While getting to the point where you truly understand what is going on inside `LeNet` may have taken a bit of work, hopefully the following code snippet will convince you that implementing such models with modern deep learning libraries is remarkably simple. We need only to instantiate a `Sequential Block` and chain together the appropriate layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(channels=6, kernel_size=5, padding=2, activation='sigmoid'),\n",
    "        nn.AvgPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),\n",
    "        nn.AvgPool2D(pool_size=2, strides=2),\n",
    "        # Dense will transform the input of the shape (batch size, channel, height, width) into \n",
    "        # the input of the shape (batch size, channel * height * width) automatically by default\n",
    "        nn.Dense(120, activation='sigmoid'),\n",
    "        nn.Dense(84, activation='sigmoid'),\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took a small liberty with the original model, removing the `Gaussian` activation in the final layer. Other than that, this network matches the original `LeNet5` architecture.\n",
    "\n",
    "By passing a single-channel (black and white) $28 \\times 28$ image through the net and printing the output shape at each layer, we can inspect the model to make sure that its operations line up with what we expect from `Fig. 6.6.2`.\n",
    "\n",
    "<img src=\"images/06_10.png\" style=\"width:250px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv5 output shape:\t (1, 6, 28, 28)\n",
      "pool4 output shape:\t (1, 6, 14, 14)\n",
      "conv6 output shape:\t (1, 16, 10, 10)\n",
      "pool5 output shape:\t (1, 16, 5, 5)\n",
      "dense0 output shape:\t (1, 120)\n",
      "dense1 output shape:\t (1, 84)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.uniform(size=(1, 1, 28, 28))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the height and width of the representation at each layer throughout the convolutional block is reduced (compared to the previous layer). The first convolutional layer uses $2$ pixels of padding to compensate for the the reduction in height and width that would otherwise result from using a $5 \\times 5$ kernel. In contrast, the second convolutional layer foregoes padding, and thus the height and width are both reduced by $4$ pixels. As we go up the stack of layers, the number of channels increases layer-over-layer from 1 in the input to 6 after the first convolutional layer and 16 after the second layer. However, each pooling layer halves the height and width. Finally, each fully-connected layer reduces dimensionality, finally emitting an output whose dimension matches the number of classes.\n",
    "\n",
    "### 6.6.2 Data Acquisition and Training\n",
    "Now that we have implemented the model, let's run an experiment to see how `LeNet` fares on `Fashion-MNIST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While convolutional networks have few parameters, they can still be more expensive to compute than similarly deep multilayer perceptrons because each parameter participates in many more multiplications. If you have access to a GPU, this might be a good time to put it into action to speed up training.\n",
    "\n",
    "For evaluation, we need to make a slight modification to the `evaluate_accuracy` function that we described in `Section 3.6`. Since the full dataset lives on the CPU, we need to copy it to the GPU before we can compute our models. This is accomplished via the `as_in_ctx` function described in `Section 5.6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, ctx=None):  #@save\n",
    "    if not ctx:  # Query the first device the first parameter is on\n",
    "        ctx = list(net.collect_params().values())[0].list_ctx()[0]\n",
    "    metric = d2l.Accumulator(2)  # num_corrected_examples, num_examples\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.as_in_ctx(ctx), y.as_in_ctx(ctx)\n",
    "        metric.add(d2l.accuracy(net(X), y), y.size)\n",
    "    return metric[0]/metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to update our training function to deal with GPUs. Unlike the `train_epoch_ch3` defined in `Section 3.6`, we now need to move each batch of data to our designated context (hopefully, the GPU) prior to making the forward and backward passes.\n",
    "\n",
    "The training function `train_ch6` is also similar to `train_ch3` defined in `Section 3.6`. Since we will be implementing networks with many layers going forward, we will rely primarily on `Gluon`. The following train function assumes a `Gluon` model as input and is optimized accordingly. We initialize the model parameters on the device indicated by `ctx` using the `Xavier` initializer. Just as with MLPs, our loss function is `cross-entropy`, and we minimize it via `minibatch stochastic gradient descent`. Since each epoch takes tens of seconds to run, we visualize the training loss more frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_ch6(net, train_iter, test_iter, num_epochs, lr, ctx=d2l.try_gpu()):\n",
    "    net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs], legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer = d2l.Timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)  # train_loss, train_acc, num_examples\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            # Here is the only difference compared to train_epoch_ch3\n",
    "            X, y = X.as_in_ctx(ctx), y.as_in_ctx(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step(X.shape[0])\n",
    "            metric.add(l.sum(), d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_loss, train_acc = metric[0]/metric[2], metric[1]/metric[2]\n",
    "            if (i+1) % 50 == 0:\n",
    "                animator.add(epoch + i/len(train_iter), (train_loss, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch+1, (None, None, test_acc))\n",
    "    print('loss %.3f, train acc %.3f, test acc %.3f' % (train_loss, train_acc, test_acc))\n",
    "    print('%.1f examples/sec on %s' % (metric[2]*num_epochs/timer.sum(), ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.486, train acc 0.816, test acc 0.798\n",
      "2483.8 examples/sec on cpu(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr, num_epochs = 0.9, 10\n",
    "train_ch6(net, train_iter, test_iter, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "+ A `ConvNet` is a network that employs convolutional layers.\n",
    "+ In a `ConvNet`, we interleave convolutions, nonlinearities, and (often) pooling operations.\n",
    "+ These convolutional blocks are typically arranged so that they gradually decrease the spatial resolution of the representations, while increasing the number of channels.\n",
    "+ In traditional `ConvNets`, the representations encoded by the convolutional blocks are processed by one (or more) dense layers prior to emitting output.\n",
    "+ `LeNet` was arguably the first successful deployment of such a network.\n",
    "\n",
    "##### Exercises\n",
    "1. Replace the average pooling with max pooling. What happens?\n",
    "2. Try to construct a more complex network based on LeNet to improve its accuracy.\n",
    "    + Adjust the convolution window size.\n",
    "    + Adjust the number of output channels.\n",
    "    + Adjust the activation function (ReLU?).\n",
    "    + Adjust the number of convolution layers.\n",
    "    + Adjust the number of fully connected layers.\n",
    "    + Adjust the learning rates and other training details (initialization, epochs, etc.)\n",
    "3. Try out the improved network on the original MNIST dataset.\n",
    "4. Display the activations of the first and second layer of LeNet for different inputs (e.g., sweaters, coats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
