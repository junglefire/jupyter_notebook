{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 导入matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 导入TF 2.0并设置GPU\n",
    "import tensorflow as tf\n",
    "# 导入Keras库\n",
    "from tensorflow import keras\n",
    "# 设置使用GPU显存的比例\n",
    "# 对于RNN类型的模型，用GPU没有太大优势\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config)) \n",
    "\n",
    "# 导入TF相关模块\n",
    "from tensorflow.keras import layers, losses, optimizers, Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding, SimpleRNN, LSTM, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 循环神经网络\n",
    "卷积神经网络利用数据的局部相关性和权值共享的思想大大减少了网络的参数量，非常适合于图片这种具有空间(Spatial)局部相关性的数据。\n",
    "\n",
    "自然界的信号除了具有空间维度之外，还有一个时间(Temporal)维度，比如文本、语音信号、股市参数等。这类数据并不一定具有局部相关性，同时数据在时间维度上的长度也是可变的，卷积神经网络并不擅长处理此类数据。\n",
    "\n",
    "本章介绍的循环神经网络可以较好地解决此类问题。\n",
    "\n",
    "### 11.7 RNN短时记忆\n",
    "循环神经网络除了训练困难，还有一个更严重的问题，那就是`短时记忆`(Short-term memory)。考虑一个长句子：\n",
    "\n",
    "> 今天天气太美好了，尽管路上发生了一件不愉快的事情，…，我马上调整好状态，开开心心地准备迎接美好的一天。\n",
    "\n",
    "根据我们的理解，之所以能够`开开心心地准备迎接美好的一天`，在于句子最开始处点名了`今天天气太美好了`。可见人类是能够很好地理解长句子的，但是，循环神经网络在处理较长的句子时，往往只能够理解有限长度内的信息，而对于位于较长范围类的有用信息往往不能够很好的利用起来。我们把这种现象叫做`短时记忆`。\n",
    "\n",
    "针对这个问题，1997年，`Jürgen Schmidhuber`提出了`长短时记忆网络`(Long Short-Term Memory，简称`LSTM`)。LSTM相对于基础的RNN网络，记忆能力更强，更擅长处理较长的序列信号数据。LSTM广泛应用在序列预测、NLP等任务中，几乎取代了基础的RNN模型。\n",
    "\n",
    "\n",
    "## 11.8 LSTM原理\n",
    "基础的RNN网络结构如`图11.13`所示，上一个时间戳的状态向量$h_{t-1}$与当前时间戳的输入$x_t$经过线性变换后，通过激活函数`tanh`后得到新的状态向量$h_t$。相对于基础的RNN网络只有一个状态向量$h_t$ ，`LSTM`新增了一个状态向量$C_t$，同时引入了门控(Gate)机制，通过门控单元来控制信息的遗忘和刷新，如`图11.14`所示。\n",
    "\n",
    "<img src=\"images/11_14.png\" style=\"width:450px;\"/>\n",
    "\n",
    "在`LSTM`中，有两个状态向量$c$和$h$，其中$c$作为`LSTM`的内部状态向量，可以理解为`LSTM`的内存状态向量`Memory`，而$h$表示`LSTM`的输出向量。相对于基础的RNN来说，`LSTM`把内部`Memory`和输出分开为两个变量，同时利用三个门控：\n",
    "+ 输入门(Input Gate)\n",
    "+ 遗忘门(Forget Gate)\n",
    "+ 输出门(Output Gate)\n",
    "\n",
    "控制内部信息的流动。\n",
    "\n",
    "门控机制可以理解为控制数据流通量的一种手段，类比于水阀门：当水阀门全部打开时，水流畅通无阻地通过；当水阀门全部关闭时，水流完全被隔断。在`LSTM`中，阀门开和程度利用门控值向量$g$表示，如`图11.15`所示。\n",
    "\n",
    "<img src=\"images/11_15.png\" style=\"width:250px;\"/>\n",
    "\n",
    "通过$\\sigma{(g)}$激活函数将门控制压缩到$[0,1]$之间区间：\n",
    "+ 当$\\sigma{(g)} = 0$时，门控全部关闭，输出$o = 0$\n",
    "+ 当$\\sigma{(g)} = 1$时，门控全部打开，输出$o = x$\n",
    "\n",
    "通过门控机制可以较好地控制数据的流量程度。\n",
    "\n",
    "### 11.8.1 遗忘门\n",
    "遗忘门作用于`LSTM`状态向量$c$上面，用于控制上一个时间戳的记忆$c_{t-1}$对当前时间戳的影响。遗忘门的控制变量$g_{f}$由\n",
    "+ $g_f = \\sigma(W_f[h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "产生，如`图11.16`所示。\n",
    "\n",
    "<img src=\"images/11_16.png\" style=\"width:350px;\"/>\n",
    "\n",
    "其中$W_f$和$b_f$为遗忘门的参数张量，可由反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数：\n",
    "+ 当门控$g_{f} = 1$时，遗忘门全部打开，`LSTM`接受上一个状态$c_{t-1}$的所有信息\n",
    "+ 当门控$g_{f} = 0$时，遗忘门关闭，`LSTM`直接忽略$c_{t-1}$，输出为0的向量\n",
    "\n",
    "这也是遗忘门的名字由来。\n",
    "\n",
    "经过遗忘门后，`LSTM`的状态向量变为$g_fc_{t−1}$。\n",
    "\n",
    "### 11.8.2 输入门\n",
    "输入门用于控制`LSTM`对输入的接收程度。首先通过对当前时间戳的输入$x_t$和上一个时间戳的输出$h_{t-1}$做非线性变换得到新的输入向量$\\widetilde{c}_t$:\n",
    "+ $\\widetilde{c}_t = \\tanh{(W_c[h_{t-1}, x_t] + b_c)}$\n",
    "\n",
    "其中$W_c$和$b_c$为输入门的参数，需要通过反向传播算法自动优化，`tanh`为激活函数，用于将输入标准化到$[−1,1]$区间。$\\widetilde{c}_t$并不会全部刷新进入`LSTM`的`Memory`，而是通过输入门控制接受输入的量。输入门的控制变量同样来自于输入$x_t$和输出$h_{t-1}$：\n",
    "+ $g_i = \\sigma(W_i[h_{t-1}, x_t] + b_i)$\n",
    "\n",
    "其中$W_i$和$b_i$为输入门的参数，需要通过反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数。\n",
    "\n",
    "输入门控制变量$g_i$决定了`LSTM`对当前时间戳的新输入$\\widetilde{c}_t$的接受程度，如`图11.17`所示：\n",
    "\n",
    "<img src=\"images/11_17.png\" style=\"width:350px;\"/>\n",
    "\n",
    "+ 当$g_i = 0$时，`LSTM`不接受任何的新输入$\\widetilde{c}_t$\n",
    "+ 当$g_i = 1$时，`LSTM`全部接受新输入$\\widetilde{c}_t$\n",
    "\n",
    "经过输入门后，待写入`Memory`的向量为$g_i\\widetilde{c}_t$。\n",
    "\n",
    "### 11.8.3 刷新Memory\n",
    "在遗忘门和输入门的控制下，`LSTM`有选择地读取了上一个时间戳的记忆$c_{t-1}$和当前时间戳的新输入$\\widetilde{c}_t$，状态向量$c_t$的刷新方式为：\n",
    "+ $c_t = g_i\\widetilde{c}_t + g_fc_{t-1}$\n",
    "\n",
    "得到的新状态向量$c_t$即为当前时间戳的状态向量，如`图11.17`所示。\n",
    "\n",
    "### 11.8.4 输出门\n",
    "`LSTM`的内部状态向量$c_{t}$并不会直接用于输出，这一点和基础的RNN不一样。基础的RNN网络的状态向量既用于记忆，又用于输出，所以基础的RNN可以理解为状态向量$c$和输出向量$h$是同一个对象。在`LSTM`内部，状态向量并不会全部输出，而是在输出门的作用下有选择地输出。输出门的门控变量$g_o$为：\n",
    "+ $g_o = \\sigma(W_O[h_{t-1},x_t]+b_o)$\n",
    "\n",
    "其中$W_O$和$b_o$为输出门的参数，同样需要通过反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数：\n",
    "+ 当输出门$g_o=0$时，输出关闭，`LSTM`的内部记忆完全被隔断，无法用作输出，此时输出为0的向量\n",
    "+ 当输出门$g_o=1$时，输出完全打开，`LSTM`的状态向量$c_t$全部用于输出\n",
    "\n",
    "`LSTM`的输出由：\n",
    "+ $h_t = g_o\\cdot\\tanh(c_t)$\n",
    "\n",
    "产生，即内存向量$c_t$经过`tanh`激活函数后与输入门作用，得到`LSTM`的输出。由于$g_o \\in [0,1]，\\tanh(c_t) \\in [−1,1]$，因此`LSTM`的输出$t \\in [−1,1]$。\n",
    "\n",
    "<img src=\"images/11_18.png\" style=\"width:350px;\"/>\n",
    "\n",
    "### 11.8.5 小结\n",
    "`LSTM`虽然状态向量和门控数量较多，计算流程相对复杂。但是由于每个门控功能清晰明确，每个状态的作用也比较好理解。这里将典型的门控行为列举出来，并解释其代码的`LSTM`行为，如`表11.1`所示。\n",
    "\n",
    "<img src=\"images/t_11_01.png\" style=\"width:400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.9 LSTM层使用方法\n",
    "`TensorFlow`有两种方式实现`LSTM`网络：\n",
    "+ 使用`LSTMCell`手动完成时间戳上面的循环运算\n",
    "+ 通过`LSTM`层方式一步完成前向运算\n",
    "\n",
    "### 11.9.1 LSTMCell\n",
    "`LSTMCell`的用法和`SimpleRNNCell`基本一致，区别在于`LSTM`的状态变量有两 个，即$[h_t,c_t]$，其中$h_t$为`cell`的输出，$c_t$为`cell`的更新后的状态。\n",
    "\n",
    "首先新建一个状态向量长度$h=64$的`LSTMCell`，其中状态向量$c_t$和输出向量$h_t$的长度都为$h$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140497035233424, 140497035233424, 140496971031136)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([2,80,100])\n",
    "# 得到一个时间戳的输入\n",
    "xt = x[:,0,:] \n",
    "# 创建LSTM Cell\n",
    "cell = layers.LSTMCell(64) \n",
    "# 初始化状态和输出\n",
    "state = [tf.zeros([2,64]),tf.zeros([2,64])]\n",
    "# 前向计算\n",
    "out, state = cell(xt, state) \n",
    "# 查看返回元素的 id\n",
    "id(out),id(state[0]),id(state[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，返回的输出`out`和$c_t$的`id`是相同的，这与基础的`RNN`初衷一致，都是为了格式的统一。 通过在时间戳上展开循环运算，即可完成一次层的前向传播，写法与基础的`RNN`一样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在序列长度维度上解开，循环送入LSTMCell单元\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    # 前向计算\n",
    "    out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出可以仅使用最后一个时间戳上的输出，也可以聚合所有时间戳上的输出向量。\n",
    "\n",
    "### 11.9.2 LSTM层\n",
    "\n",
    "通过`layers.LSTM`层可以方便的一次完成整个序列的运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一层LSTM层，内存向量长度为64\n",
    "layer = layers.LSTM(64)\n",
    "# 序列通过LSTM层，默认返回最后一个时间戳的输出h\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过`LSTM`层前向传播后，默认只会返回最后一个时间戳的输出，如果需要返回每个时间戳上面的输出，需要设置`return_sequences=True`标志："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 80, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建LSTM层时，设置返回每个时间戳上的输出\n",
    "layer = layers.LSTM(64, return_sequences=True)\n",
    "# 前向计算，每个时间戳上的输出自动进行了concat，拼成一个张量\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`out`的`shape`是$[2,80,64]$，包含了80个时间戳。\n",
    "\n",
    "对于多层神经网络，可以通过`Sequential`容器包裹多层`LSTM`层，并设置所有非末层网络`return_sequences=True`，这是因为非末层的`LSTM`层需要上一层在所有时间戳的输出作为输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = keras.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "    layers.LSTM(64)\n",
    "])\n",
    "\n",
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.10 GRU简介\n",
    "`LSTM`具有更长的记忆能力，不容易出现梯度弥散现象，在大部分序列任务上面都取得了比基础的RNN模型更好的性能表现。但是`LSTM`结构相对较复杂，计算代价较高，模型参数量较大。因此，科学家们尝试简化`LSTM`内部的计算流程，特别是减少门控数量。研究发现，遗忘门是`LSTM`中最重要的门控，甚至发现只有遗忘门的简化版网络在多个基准数据集上面优于标准`LSTM`网络。\n",
    "\n",
    "在众多的简化版`LSTM`中，`门控循环网络`(Gated Recurrent Unit，简称`GRU`)应用最广泛。`GRU`把内部状态向量和输出向量合并，统一为状态向量，门控数量也减少到2个：`复位门`(Reset Gate)和`更新门`(Update Gate)，如`图11.19`：\n",
    "\n",
    "<img src=\"images/11_19.png\" style=\"width:300px;\"/>\n",
    "\n",
    "### 11.10.1 复位门\n",
    "复位门用于控制上一个时间戳的状态$h_{t-1}$进入`GRU`的量。门控向量$g_r$由当前时间戳输入$x_t$和上一时间戳状态$h_{t-1}$变换得到，关系如下：\n",
    "+ $g_r = \\sigma(W_r[h_{t-1},x_t]+b_r)$\n",
    "\n",
    "其中$W_r$和$b_r$为复位门的参数，由反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数。门控向量$g_r$只控制状态$h_{t-1}$，而不会控制输入$x_t$：\n",
    "+  $\\tilde{h_t} = \\tanh(W_h[g_rh_{t-1},x_t]+b_h)$\n",
    "\n",
    "当$g_r = 0$时，新输入$\\tilde{h_t}$全部来自于输入$x_t$，不接受$h_{t-1}$，此时相当于复位$h_{t-1}$。当$g_r = 1$时，$h_{t-1}$和输入$x_t$共同产生新输入$\\tilde{h_t}$，如`图11.20`所示。\n",
    "\n",
    "<img src=\"images/11_20.png\" style=\"width:350px;\"/>\n",
    "\n",
    "### 11.10.2 更新门\n",
    "更新门用控制上一时间戳状态$h_{t-1}$和新输入$\\tilde{h_t}$对新状态向量$h_t$的影响程度。更新门控向量$g_z$由\n",
    "+ $g_z = \\sigma(W_z[h_{t-1}, x_t] + b_z)$\n",
    "\n",
    "得到，其中$W_z$和$b_z$为更新门的参数，由反向传播算法自动优化，$\\sigma$为激活函数，一般使用`Sigmoid`函数。$g_z$用与控制新输入$\\tilde{h_t}$信号，$1-g_z$用于控制状态$h_{t-1}$信号：\n",
    "+ $h_t = (1-g_z)h_{t-1}+g_z\\tilde{h_t}$\n",
    "\n",
    "可以看到，$\\tilde{h_t}$和$h_{t-1}$对$h_t$的更新量处于相互竞争、此消彼长的状态：\n",
    "+ 当更新门$g_z=0$时，$h_t$全部来自上一时间戳状态$h_{t-1}$\n",
    "+ 当更新门$g_z = 1$时，$h_t$全部来自新输入$\\tilde{h_t}$\n",
    "\n",
    "<img src=\"images/11_21.png\" style=\"width:350px;\"/>\n",
    "\n",
    "### 11.10.3 GRU使用方法\n",
    "`TensorFlow`中，也有`Cell`方式和层方式实现`GRU`网络。`GRUCell`和`SimpleRNNCell`非常类似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化状态向量，GRU只有一个\n",
    "h = [tf.zeros([2,64])]\n",
    "# 新建GRU Cell，向量长度为64\n",
    "cell = layers.GRUCell(64)\n",
    "# 在时间戳维度上解开，循环通过cell\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, h = cell(xt, h)\n",
    "# 输出形状\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`layers.GRU`类可以方便创建一层`GRU`网络层，通过`Sequential`容器可以堆叠多层`GRU`层的网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = keras.Sequential([\n",
    "    layers.GRU(64, return_sequences=True),\n",
    "    layers.GRU(64)\n",
    "])\n",
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.11 LSTM/GRU情感分类问题再战\n",
    "首先准备数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) 218 (25000,)\n",
      "(25000,) 68 (25000,)\n"
     ]
    }
   ],
   "source": [
    "# 批量大小\n",
    "batchsz = 128 \n",
    "# 词汇表大小N_vocab\n",
    "total_words = 10000 \n",
    "# 句子最大长度s，大于的句子部分将截断，小于的将填充\n",
    "max_review_len = 80 \n",
    "# 词向量特征长度f\n",
    "embedding_len = 100 \n",
    "# 加载IMDB数据集，此处的数据采用数字编码，一个数字代表一个单词\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n",
    "print(x_train.shape, len(x_train[0]), y_train.shape)\n",
    "print(x_test.shape, len(x_test[0]), y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 80) tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "# 截断和填充句子，使得等长，此处长句子保留句子后面的部分，短句子在前面填充\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n",
    "\n",
    "# 构建数据集，打散，批量，并丢掉最后一个不够batchsz的batch\n",
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.batch(batchsz, drop_remainder=True)\n",
    "print('x_train shape:', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# 数字编码表\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# for k,v in word_index.items():\n",
    "#     print(k,v)\n",
    "#%%\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.11.1 LSTM模型\n",
    "首先是`Cell`方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.5124 - accuracy: 0.6454Epoch 1/50\n",
      "195/195 [==============================] - 6s 31ms/step - loss: 0.3664 - accuracy: 0.8392\n",
      "195/195 [==============================] - 25s 130ms/step - loss: 0.5117 - accuracy: 0.6468 - val_loss: 0.3664 - val_accuracy: 0.8392\n",
      "Epoch 2/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.3530 - accuracy: 0.8451Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.3550 - accuracy: 0.8431\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.3534 - accuracy: 0.8451 - val_loss: 0.3550 - val_accuracy: 0.8431\n",
      "Epoch 3/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8719Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.3577 - accuracy: 0.8432\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.3061 - accuracy: 0.8719 - val_loss: 0.3577 - val_accuracy: 0.8432\n",
      "Epoch 4/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.8883Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.3602 - accuracy: 0.8457\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.2845 - accuracy: 0.8883 - val_loss: 0.3602 - val_accuracy: 0.8457\n",
      "Epoch 5/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.8960Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.3711 - accuracy: 0.8445\n",
      "195/195 [==============================] - 12s 59ms/step - loss: 0.2637 - accuracy: 0.8960 - val_loss: 0.3711 - val_accuracy: 0.8445\n",
      "Epoch 6/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9040Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.3631 - accuracy: 0.8454\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.2491 - accuracy: 0.9040 - val_loss: 0.3631 - val_accuracy: 0.8454\n",
      "Epoch 7/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9069Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.4336 - accuracy: 0.8379\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.2384 - accuracy: 0.9069 - val_loss: 0.4336 - val_accuracy: 0.8379\n",
      "Epoch 8/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9127Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.4030 - accuracy: 0.8355\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.2228 - accuracy: 0.9127 - val_loss: 0.4030 - val_accuracy: 0.8355\n",
      "Epoch 9/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.9193Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.3881 - accuracy: 0.8402\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.2112 - accuracy: 0.9193 - val_loss: 0.3881 - val_accuracy: 0.8402\n",
      "Epoch 10/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1986 - accuracy: 0.9260Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.4248 - accuracy: 0.8368\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.1984 - accuracy: 0.9260 - val_loss: 0.4248 - val_accuracy: 0.8368\n",
      "Epoch 11/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9288Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.4984 - accuracy: 0.8318\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.1907 - accuracy: 0.9288 - val_loss: 0.4984 - val_accuracy: 0.8318\n",
      "Epoch 12/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.1800 - accuracy: 0.9324Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.4346 - accuracy: 0.8339\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.1802 - accuracy: 0.9325 - val_loss: 0.4346 - val_accuracy: 0.8339\n",
      "Epoch 13/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.9364Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.4651 - accuracy: 0.8325\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.1704 - accuracy: 0.9364 - val_loss: 0.4651 - val_accuracy: 0.8325\n",
      "Epoch 14/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9421Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.4771 - accuracy: 0.8313\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.1617 - accuracy: 0.9421 - val_loss: 0.4771 - val_accuracy: 0.8313\n",
      "Epoch 15/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.1507 - accuracy: 0.9435Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.5926 - accuracy: 0.8230\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.1504 - accuracy: 0.9435 - val_loss: 0.5926 - val_accuracy: 0.8230\n",
      "Epoch 16/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9492Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.5476 - accuracy: 0.8273\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.1385 - accuracy: 0.9492 - val_loss: 0.5476 - val_accuracy: 0.8273\n",
      "Epoch 17/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.1314 - accuracy: 0.9516Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.5681 - accuracy: 0.8254\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.1314 - accuracy: 0.9516 - val_loss: 0.5681 - val_accuracy: 0.8254\n",
      "Epoch 18/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9540Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.5576 - accuracy: 0.8129\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.1250 - accuracy: 0.9540 - val_loss: 0.5576 - val_accuracy: 0.8129\n",
      "Epoch 19/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9567Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.5918 - accuracy: 0.8198\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.1133 - accuracy: 0.9568 - val_loss: 0.5918 - val_accuracy: 0.8198\n",
      "Epoch 20/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9610Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.6751 - accuracy: 0.8196\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.1073 - accuracy: 0.9610 - val_loss: 0.6751 - val_accuracy: 0.8196\n",
      "Epoch 21/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.9629Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.6804 - accuracy: 0.8124\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0985 - accuracy: 0.9630 - val_loss: 0.6804 - val_accuracy: 0.8124\n",
      "Epoch 22/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0907 - accuracy: 0.9668Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.7220 - accuracy: 0.8173\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0911 - accuracy: 0.9668 - val_loss: 0.7220 - val_accuracy: 0.8173\n",
      "Epoch 23/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9700Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.7633 - accuracy: 0.8182\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0835 - accuracy: 0.9700 - val_loss: 0.7633 - val_accuracy: 0.8182\n",
      "Epoch 24/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9730Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.7562 - accuracy: 0.8115\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0752 - accuracy: 0.9730 - val_loss: 0.7562 - val_accuracy: 0.8115\n",
      "Epoch 25/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9778Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.7341 - accuracy: 0.8127\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0697 - accuracy: 0.9778 - val_loss: 0.7341 - val_accuracy: 0.8127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9753Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.7841 - accuracy: 0.8077\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.0662 - accuracy: 0.9753 - val_loss: 0.7841 - val_accuracy: 0.8077\n",
      "Epoch 27/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0598 - accuracy: 0.9775Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.9759 - accuracy: 0.8137\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.0595 - accuracy: 0.9776 - val_loss: 0.9759 - val_accuracy: 0.8137\n",
      "Epoch 28/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0548 - accuracy: 0.9803Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.8539 - accuracy: 0.8188\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.0548 - accuracy: 0.9803 - val_loss: 0.8539 - val_accuracy: 0.8188\n",
      "Epoch 29/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0507 - accuracy: 0.9827Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.9302 - accuracy: 0.8159\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.0510 - accuracy: 0.9827 - val_loss: 0.9302 - val_accuracy: 0.8159\n",
      "Epoch 30/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0483 - accuracy: 0.9828Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.9425 - accuracy: 0.8127\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.0481 - accuracy: 0.9828 - val_loss: 0.9425 - val_accuracy: 0.8127\n",
      "Epoch 31/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0419 - accuracy: 0.9847Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 0.9962 - accuracy: 0.8147\n",
      "195/195 [==============================] - 11s 58ms/step - loss: 0.0418 - accuracy: 0.9847 - val_loss: 0.9962 - val_accuracy: 0.8147\n",
      "Epoch 32/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0430 - accuracy: 0.9845Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.9764 - accuracy: 0.8070\n",
      "195/195 [==============================] - 11s 59ms/step - loss: 0.0432 - accuracy: 0.9845 - val_loss: 0.9764 - val_accuracy: 0.8070\n",
      "Epoch 33/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0377 - accuracy: 0.9859Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.9941 - accuracy: 0.8101\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.0379 - accuracy: 0.9859 - val_loss: 0.9941 - val_accuracy: 0.8101\n",
      "Epoch 34/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9889Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.9677 - accuracy: 0.8091\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.0337 - accuracy: 0.9889 - val_loss: 0.9677 - val_accuracy: 0.8091\n",
      "Epoch 35/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0342 - accuracy: 0.9856Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.2266 - accuracy: 0.8104\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0340 - accuracy: 0.9856 - val_loss: 1.2266 - val_accuracy: 0.8104\n",
      "Epoch 36/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0288 - accuracy: 0.9893Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.1926 - accuracy: 0.8095\n",
      "195/195 [==============================] - 12s 60ms/step - loss: 0.0287 - accuracy: 0.9893 - val_loss: 1.1926 - val_accuracy: 0.8095\n",
      "Epoch 37/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0291 - accuracy: 0.9913Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.3501 - accuracy: 0.8008\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0288 - accuracy: 0.9913 - val_loss: 1.3501 - val_accuracy: 0.8008\n",
      "Epoch 38/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9912Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.1857 - accuracy: 0.8089\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0268 - accuracy: 0.9912 - val_loss: 1.1857 - val_accuracy: 0.8089\n",
      "Epoch 39/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0247 - accuracy: 0.9909Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.2860 - accuracy: 0.8048\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0248 - accuracy: 0.9909 - val_loss: 1.2860 - val_accuracy: 0.8048\n",
      "Epoch 40/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9912Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.2206 - accuracy: 0.8125\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0250 - accuracy: 0.9912 - val_loss: 1.2206 - val_accuracy: 0.8125\n",
      "Epoch 41/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9928Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.2759 - accuracy: 0.8071\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0225 - accuracy: 0.9928 - val_loss: 1.2759 - val_accuracy: 0.8071\n",
      "Epoch 42/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9919Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.3078 - accuracy: 0.7991\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0206 - accuracy: 0.9919 - val_loss: 1.3078 - val_accuracy: 0.7991\n",
      "Epoch 43/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9946Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.2343 - accuracy: 0.8052\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0197 - accuracy: 0.9946 - val_loss: 1.2343 - val_accuracy: 0.8052\n",
      "Epoch 44/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9932Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.3686 - accuracy: 0.8111\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0202 - accuracy: 0.9932 - val_loss: 1.3686 - val_accuracy: 0.8111\n",
      "Epoch 45/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9947Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.4680 - accuracy: 0.8114\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0175 - accuracy: 0.9947 - val_loss: 1.4680 - val_accuracy: 0.8114\n",
      "Epoch 46/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9933Epoch 1/50\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 1.5757 - accuracy: 0.8099\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0174 - accuracy: 0.9933 - val_loss: 1.5757 - val_accuracy: 0.8099\n",
      "Epoch 47/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9954Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.4340 - accuracy: 0.8111\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 1.4340 - val_accuracy: 0.8111\n",
      "Epoch 48/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9956Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.5023 - accuracy: 0.8065\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0141 - accuracy: 0.9956 - val_loss: 1.5023 - val_accuracy: 0.8065\n",
      "Epoch 49/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9947Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.3502 - accuracy: 0.8021\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0160 - accuracy: 0.9947 - val_loss: 1.3502 - val_accuracy: 0.8021\n",
      "Epoch 50/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0172 - accuracy: 0.9954Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.4090 - accuracy: 0.8091\n",
      "195/195 [==============================] - 12s 61ms/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 1.4090 - val_accuracy: 0.8091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 3s 16ms/step - loss: 1.4090 - accuracy: 0.8091\n"
     ]
    }
   ],
   "source": [
    "class MyRNN(keras.Model):\n",
    "    # Cell方式构建多层网络\n",
    "    def __init__(self, units):\n",
    "        super(MyRNN, self).__init__()\n",
    "        # [b, 64]，构建Cell初始化状态向量，重复使用\n",
    "        self.state0 = [tf.zeros([batchsz, units]),tf.zeros([batchsz, units])]\n",
    "        self.state1 = [tf.zeros([batchsz, units]),tf.zeros([batchsz, units])]\n",
    "        # 词向量编码 [b, 80] => [b, 80, 100]\n",
    "        self.embedding = layers.Embedding(total_words, embedding_len, input_length=max_review_len)\n",
    "        # 构建2个Cell\n",
    "        self.rnn_cell0 = layers.LSTMCell(units, dropout=0.5)\n",
    "        self.rnn_cell1 = layers.LSTMCell(units, dropout=0.5)\n",
    "        # 构建分类网络，用于将CELL的输出特征进行分类，2分类\n",
    "        # [b, 80, 100] => [b, 64] => [b, 1]\n",
    "        self.outlayer = Sequential([\n",
    "        \tlayers.Dense(units),\n",
    "        \tlayers.Dropout(rate=0.5),\n",
    "        \tlayers.ReLU(),\n",
    "        \tlayers.Dense(1)])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs # [b, 80]\n",
    "        # embedding: [b, 80] => [b, 80, 100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
    "        state0 = self.state0\n",
    "        state1 = self.state1\n",
    "        for word in tf.unstack(x, axis=1): # word: [b, 100] \n",
    "            out0, state0 = self.rnn_cell0(word, state0, training) \n",
    "            out1, state1 = self.rnn_cell1(out0, state1, training)\n",
    "        # 末层最后一个输出作为分类网络的输入: [b, 64] => [b, 1]\n",
    "        x = self.outlayer(out1,training)\n",
    "        # p(y is pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob\n",
    "\n",
    "def main():\n",
    "    units = 64 # RNN状态向量长度f\n",
    "    epochs = 50 # 训练epochs\n",
    "\n",
    "    model = MyRNN(units)\n",
    "    # 装配\n",
    "    # Keras在编译模型阶段，区分训练状态和非训练状态，二者的逻辑是不一样的。\n",
    "    # 比如说，模型使用了Dropout，在训练时要随机失活部分神经元，而在正式运行，所有神经元都要保留的。\n",
    "    # 如果是训练状态，则在编译参数里增加一个选项即可：\n",
    "    # experimental_run_tf_function = False\n",
    "    # 否则会报错：\n",
    "    # _SymbolicException: Inputs to eager execution function cannot be Keras symbolic...\n",
    "    model.compile(optimizer = optimizers.RMSprop(0.001),\n",
    "                  loss = losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function = False)\n",
    "    # 训练和验证\n",
    "    model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "    # 测试\n",
    "    model.evaluate(db_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.11.2 GRU模型\n",
    "首先是`Cell`方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5501 - accuracy: 0.6116Epoch 1/50\n",
      "195/195 [==============================] - 7s 35ms/step - loss: 0.3693 - accuracy: 0.8348\n",
      "195/195 [==============================] - 32s 162ms/step - loss: 0.5494 - accuracy: 0.6126 - val_loss: 0.3693 - val_accuracy: 0.8348\n",
      "Epoch 2/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.3672 - accuracy: 0.8359Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.3447 - accuracy: 0.8476\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.3665 - accuracy: 0.8360 - val_loss: 0.3447 - val_accuracy: 0.8476\n",
      "Epoch 3/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.3231 - accuracy: 0.8659Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.3516 - accuracy: 0.8457\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.3231 - accuracy: 0.8659 - val_loss: 0.3516 - val_accuracy: 0.8457\n",
      "Epoch 4/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2900 - accuracy: 0.8823Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.3668 - accuracy: 0.8445\n",
      "195/195 [==============================] - 13s 68ms/step - loss: 0.2901 - accuracy: 0.8823 - val_loss: 0.3668 - val_accuracy: 0.8445\n",
      "Epoch 5/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.2692 - accuracy: 0.8938 ETA: 0s - loss: 0.2690 Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.4414 - accuracy: 0.8336\n",
      "195/195 [==============================] - 13s 68ms/step - loss: 0.2689 - accuracy: 0.8938 - val_loss: 0.4414 - val_accuracy: 0.8336\n",
      "Epoch 6/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9001Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.3700 - accuracy: 0.8445\n",
      "195/195 [==============================] - 13s 69ms/step - loss: 0.2498 - accuracy: 0.9001 - val_loss: 0.3700 - val_accuracy: 0.8445\n",
      "Epoch 7/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.9105Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.3859 - accuracy: 0.8446\n",
      "195/195 [==============================] - 14s 70ms/step - loss: 0.2335 - accuracy: 0.9105 - val_loss: 0.3859 - val_accuracy: 0.8446\n",
      "Epoch 8/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9166Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.3866 - accuracy: 0.8429\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.2205 - accuracy: 0.9166 - val_loss: 0.3866 - val_accuracy: 0.8429\n",
      "Epoch 9/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9209Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.4167 - accuracy: 0.8409\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.2099 - accuracy: 0.9209 - val_loss: 0.4167 - val_accuracy: 0.8409\n",
      "Epoch 10/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1974 - accuracy: 0.9253Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.5515 - accuracy: 0.8331\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1970 - accuracy: 0.9253 - val_loss: 0.5515 - val_accuracy: 0.8331\n",
      "Epoch 11/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9290Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.4177 - accuracy: 0.8395\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1891 - accuracy: 0.9290 - val_loss: 0.4177 - val_accuracy: 0.8395\n",
      "Epoch 12/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1739 - accuracy: 0.9354Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.5397 - accuracy: 0.8352\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1734 - accuracy: 0.9354 - val_loss: 0.5397 - val_accuracy: 0.8352\n",
      "Epoch 13/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1650 - accuracy: 0.9383Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.4775 - accuracy: 0.8371\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1648 - accuracy: 0.9383 - val_loss: 0.4775 - val_accuracy: 0.8371\n",
      "Epoch 14/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9412Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.4778 - accuracy: 0.8354\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1539 - accuracy: 0.9413 - val_loss: 0.4778 - val_accuracy: 0.8354\n",
      "Epoch 15/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.9474Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.4940 - accuracy: 0.8286\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1440 - accuracy: 0.9474 - val_loss: 0.4940 - val_accuracy: 0.8286\n",
      "Epoch 16/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9517Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.5462 - accuracy: 0.8348\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1333 - accuracy: 0.9517 - val_loss: 0.5462 - val_accuracy: 0.8348\n",
      "Epoch 17/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9527Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.5141 - accuracy: 0.8308\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1239 - accuracy: 0.9527 - val_loss: 0.5141 - val_accuracy: 0.8308\n",
      "Epoch 18/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9578Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.5490 - accuracy: 0.8294\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1168 - accuracy: 0.9579 - val_loss: 0.5490 - val_accuracy: 0.8294\n",
      "Epoch 19/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9598Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.6289 - accuracy: 0.8298\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1086 - accuracy: 0.9598 - val_loss: 0.6289 - val_accuracy: 0.8298\n",
      "Epoch 20/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.1002 - accuracy: 0.9649Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.6352 - accuracy: 0.8278\n",
      "195/195 [==============================] - 14s 71ms/step - loss: 0.1003 - accuracy: 0.9649 - val_loss: 0.6352 - val_accuracy: 0.8278\n",
      "Epoch 21/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.9651Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.7437 - accuracy: 0.8250\n",
      "195/195 [==============================] - 14s 72ms/step - loss: 0.0952 - accuracy: 0.9652 - val_loss: 0.7437 - val_accuracy: 0.8250\n",
      "Epoch 22/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9680Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.7210 - accuracy: 0.8197\n",
      "195/195 [==============================] - 13s 67ms/step - loss: 0.0877 - accuracy: 0.9680 - val_loss: 0.7210 - val_accuracy: 0.8197\n",
      "Epoch 23/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0819 - accuracy: 0.9710Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.6755 - accuracy: 0.8225\n",
      "195/195 [==============================] - 13s 67ms/step - loss: 0.0819 - accuracy: 0.9710 - val_loss: 0.6755 - val_accuracy: 0.8225\n",
      "Epoch 24/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9736Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.9308 - accuracy: 0.8069\n",
      "195/195 [==============================] - 13s 67ms/step - loss: 0.0730 - accuracy: 0.9736 - val_loss: 0.9308 - val_accuracy: 0.8069\n",
      "Epoch 25/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9736Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.9206 - accuracy: 0.8176\n",
      "195/195 [==============================] - 13s 67ms/step - loss: 0.0714 - accuracy: 0.9736 - val_loss: 0.9206 - val_accuracy: 0.8176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0661 - accuracy: 0.9772Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.7020 - accuracy: 0.8205\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.0668 - accuracy: 0.9772 - val_loss: 0.7020 - val_accuracy: 0.8205\n",
      "Epoch 27/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9786Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 0.8931 - accuracy: 0.8158\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0611 - accuracy: 0.9786 - val_loss: 0.8931 - val_accuracy: 0.8158\n",
      "Epoch 28/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9801Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.8520 - accuracy: 0.8145\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.0579 - accuracy: 0.9801 - val_loss: 0.8520 - val_accuracy: 0.8145\n",
      "Epoch 29/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0508 - accuracy: 0.9824Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.8845 - accuracy: 0.8175\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0506 - accuracy: 0.9824 - val_loss: 0.8845 - val_accuracy: 0.8175\n",
      "Epoch 30/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0435 - accuracy: 0.9854Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.0585 - accuracy: 0.8168\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0435 - accuracy: 0.9854 - val_loss: 1.0585 - val_accuracy: 0.8168\n",
      "Epoch 31/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9847Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 0.9253 - accuracy: 0.8159\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0458 - accuracy: 0.9847 - val_loss: 0.9253 - val_accuracy: 0.8159\n",
      "Epoch 32/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9864Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.0935 - accuracy: 0.8143\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0424 - accuracy: 0.9864 - val_loss: 1.0935 - val_accuracy: 0.8143\n",
      "Epoch 33/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0356 - accuracy: 0.9869Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.0610 - accuracy: 0.8143\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0358 - accuracy: 0.9869 - val_loss: 1.0610 - val_accuracy: 0.8143\n",
      "Epoch 34/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0340 - accuracy: 0.9879Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.1077 - accuracy: 0.8059\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0342 - accuracy: 0.9880 - val_loss: 1.1077 - val_accuracy: 0.8059\n",
      "Epoch 35/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0350 - accuracy: 0.9876Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.1741 - accuracy: 0.8092\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0349 - accuracy: 0.9876 - val_loss: 1.1741 - val_accuracy: 0.8092\n",
      "Epoch 36/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0308 - accuracy: 0.9904Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.0802 - accuracy: 0.8109\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0307 - accuracy: 0.9904 - val_loss: 1.0802 - val_accuracy: 0.8109\n",
      "Epoch 37/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9902Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.3740 - accuracy: 0.8068\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0278 - accuracy: 0.9903 - val_loss: 1.3740 - val_accuracy: 0.8068\n",
      "Epoch 38/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9912Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.2813 - accuracy: 0.8121\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.0262 - accuracy: 0.9913 - val_loss: 1.2813 - val_accuracy: 0.8121\n",
      "Epoch 39/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0254 - accuracy: 0.9913Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.3161 - accuracy: 0.8051\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0253 - accuracy: 0.9913 - val_loss: 1.3161 - val_accuracy: 0.8051\n",
      "Epoch 40/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0205 - accuracy: 0.9941Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.3811 - accuracy: 0.8099\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0204 - accuracy: 0.9941 - val_loss: 1.3811 - val_accuracy: 0.8099\n",
      "Epoch 41/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0206 - accuracy: 0.9925Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.5476 - accuracy: 0.8053\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0205 - accuracy: 0.9925 - val_loss: 1.5476 - val_accuracy: 0.8053\n",
      "Epoch 42/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9942Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.5153 - accuracy: 0.8096\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.0175 - accuracy: 0.9942 - val_loss: 1.5153 - val_accuracy: 0.8096\n",
      "Epoch 43/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9935Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.5220 - accuracy: 0.8073\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0194 - accuracy: 0.9935 - val_loss: 1.5220 - val_accuracy: 0.8073\n",
      "Epoch 44/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9937Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.6602 - accuracy: 0.8046\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0163 - accuracy: 0.9937 - val_loss: 1.6602 - val_accuracy: 0.8046\n",
      "Epoch 45/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9948Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.5129 - accuracy: 0.8089\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0169 - accuracy: 0.9948 - val_loss: 1.5129 - val_accuracy: 0.8089\n",
      "Epoch 46/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9953Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.5987 - accuracy: 0.8104\n",
      "195/195 [==============================] - 13s 65ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 1.5987 - val_accuracy: 0.8104\n",
      "Epoch 47/50\n",
      "193/195 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9952Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.6333 - accuracy: 0.8072\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0155 - accuracy: 0.9952 - val_loss: 1.6333 - val_accuracy: 0.8072\n",
      "Epoch 48/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9957Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.5555 - accuracy: 0.8028\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 1.5555 - val_accuracy: 0.8028\n",
      "Epoch 49/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9961Epoch 1/50\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 1.8063 - accuracy: 0.8002\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 1.8063 - val_accuracy: 0.8002\n",
      "Epoch 50/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9966Epoch 1/50\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 1.4869 - accuracy: 0.8115\n",
      "195/195 [==============================] - 13s 66ms/step - loss: 0.0128 - accuracy: 0.9966 - val_loss: 1.4869 - val_accuracy: 0.8115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 3s 17ms/step - loss: 1.4869 - accuracy: 0.8115\n"
     ]
    }
   ],
   "source": [
    "class MyRNN(keras.Model):\n",
    "    # Cell方式构建多层网络\n",
    "    def __init__(self, units):\n",
    "        super(MyRNN, self).__init__()\n",
    "        # [b, 64]，构建Cell初始化状态向量，重复使用\n",
    "        self.state0 = [tf.zeros([batchsz, units])]\n",
    "        self.state1 = [tf.zeros([batchsz, units])]\n",
    "        # 词向量编码 [b, 80] => [b, 80, 100]\n",
    "        self.embedding = layers.Embedding(total_words, embedding_len, input_length=max_review_len)\n",
    "        # 构建2个Cell\n",
    "        self.rnn_cell0 = layers.GRUCell(units, dropout=0.5)\n",
    "        self.rnn_cell1 = layers.GRUCell(units, dropout=0.5)\n",
    "        # 构建分类网络，用于将CELL的输出特征进行分类，2分类\n",
    "        # [b, 80, 100] => [b, 64] => [b, 1]\n",
    "        self.outlayer = Sequential([\n",
    "        \tlayers.Dense(units),\n",
    "        \tlayers.Dropout(rate=0.5),\n",
    "        \tlayers.ReLU(),\n",
    "        \tlayers.Dense(1)])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs # [b, 80]\n",
    "        # embedding: [b, 80] => [b, 80, 100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
    "        state0 = self.state0\n",
    "        state1 = self.state1\n",
    "        for word in tf.unstack(x, axis=1): # word: [b, 100] \n",
    "            out0, state0 = self.rnn_cell0(word, state0, training) \n",
    "            out1, state1 = self.rnn_cell1(out0, state1, training)\n",
    "        # 末层最后一个输出作为分类网络的输入: [b, 64] => [b, 1]\n",
    "        x = self.outlayer(out1, training)\n",
    "        # p(y is pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob\n",
    "\n",
    "def main():\n",
    "    units = 64 # RNN状态向量长度f\n",
    "    epochs = 50 # 训练epochs\n",
    "    model = MyRNN(units)\n",
    "    # 装配\n",
    "    model.compile(optimizer = optimizers.RMSprop(0.001),\n",
    "                  loss = losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function = False)\n",
    "    # 训练和验证\n",
    "    model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "    # 测试\n",
    "    model.evaluate(db_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.12 预训练的词向量\n",
    "在情感分类任务时，`Embedding`层是从零开始训练的。实际上，对于文本处理任务来说，领域知识大部分是共享的，因此我们能够利用在其它任务上训练好的词向量。\n",
    "\n",
    "我们以预训练的`GloVe`词向量为例，演示如何利用预训练的词向量模型提升任务性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 400000, 88588)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "GLOVE_DIR = r'/home/alex/datasets/glove'\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "len(embeddings_index), len(embeddings_index.keys()), len(word_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GloVe.6B`版本共存储了40万个词汇的向量表。前面实战中我们只考虑最多1万个常见的词汇，我们根据词汇的数字编码表依次从GloVe模型中获取其词向量，并写入对应位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9793 (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_WORDS = total_words\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, embedding_len))\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # print(word,embedding_vector)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在获得了词汇表数据后，利用词汇表初始化`Embedding`层即可，并设置`Embedding`层不参与梯度优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5232Epoch 1/50\n",
      "195/195 [==============================] - 20s 101ms/step - loss: 0.6777 - accuracy: 0.5843s - loss: 0.6777  - ETA: 1s - loss: 0.6776 - ac - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accu\n",
      "195/195 [==============================] - 76s 388ms/step - loss: 0.6881 - accuracy: 0.5234 - val_loss: 0.6777 - val_accuracy: 0.5843\n",
      "Epoch 2/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6787 - accuracy: 0.5657Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.6769 - accuracy: 0.5683\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.6787 - accuracy: 0.5658 - val_loss: 0.6769 - val_accuracy: 0.5683\n",
      "Epoch 3/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6709 - accuracy: 0.5781Epoch 1/50\n",
      "195/195 [==============================] - 20s 103ms/step - loss: 0.6607 - accuracy: 0.5964\n",
      "195/195 [==============================] - 75s 387ms/step - loss: 0.6708 - accuracy: 0.5781 - val_loss: 0.6607 - val_accuracy: 0.5964\n",
      "Epoch 4/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6727 - accuracy: 0.5839Epoch 1/50\n",
      "195/195 [==============================] - 20s 103ms/step - loss: 0.6617 - accuracy: 0.5872\n",
      "195/195 [==============================] - 75s 386ms/step - loss: 0.6725 - accuracy: 0.5838 - val_loss: 0.6617 - val_accuracy: 0.5872\n",
      "Epoch 5/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6720 - accuracy: 0.5706Epoch 1/50\n",
      "195/195 [==============================] - 20s 103ms/step - loss: 0.6624 - accuracy: 0.5776\n",
      "195/195 [==============================] - 75s 387ms/step - loss: 0.6719 - accuracy: 0.5706 - val_loss: 0.6624 - val_accuracy: 0.5776\n",
      "Epoch 6/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6648 - accuracy: 0.5829Epoch 1/50\n",
      "195/195 [==============================] - 20s 103ms/step - loss: 0.6623 - accuracy: 0.5649\n",
      "195/195 [==============================] - 75s 387ms/step - loss: 0.6647 - accuracy: 0.5828 - val_loss: 0.6623 - val_accuracy: 0.5649\n",
      "Epoch 7/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6567 - accuracy: 0.5903Epoch 1/50\n",
      "195/195 [==============================] - 20s 104ms/step - loss: 0.6572 - accuracy: 0.5655\n",
      "195/195 [==============================] - 76s 388ms/step - loss: 0.6567 - accuracy: 0.5904 - val_loss: 0.6572 - val_accuracy: 0.5655\n",
      "Epoch 8/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6492 - accuracy: 0.5895Epoch 1/50\n",
      "195/195 [==============================] - 20s 103ms/step - loss: 0.6223 - accuracy: 0.6413\n",
      "195/195 [==============================] - 76s 388ms/step - loss: 0.6490 - accuracy: 0.5896 - val_loss: 0.6223 - val_accuracy: 0.6413\n",
      "Epoch 9/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6388 - accuracy: 0.6175Epoch 1/50\n",
      "195/195 [==============================] - 20s 103ms/step - loss: 0.6289 - accuracy: 0.6183\n",
      "195/195 [==============================] - 76s 388ms/step - loss: 0.6388 - accuracy: 0.6176 - val_loss: 0.6289 - val_accuracy: 0.6183\n",
      "Epoch 10/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6370 - accuracy: 0.6157Epoch 1/50\n",
      "195/195 [==============================] - 20s 103ms/step - loss: 0.6239 - accuracy: 0.6287\n",
      "195/195 [==============================] - 76s 388ms/step - loss: 0.6368 - accuracy: 0.6158 - val_loss: 0.6239 - val_accuracy: 0.6287\n",
      "Epoch 11/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6303 - accuracy: 0.6303Epoch 1/50\n",
      "195/195 [==============================] - 20s 103ms/step - loss: 0.6314 - accuracy: 0.6484\n",
      "195/195 [==============================] - 76s 388ms/step - loss: 0.6304 - accuracy: 0.6303 - val_loss: 0.6314 - val_accuracy: 0.6484\n",
      "Epoch 12/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6233 - accuracy: 0.6337Epoch 1/50\n",
      "195/195 [==============================] - 20s 104ms/step - loss: 0.5982 - accuracy: 0.6693\n",
      "195/195 [==============================] - 76s 388ms/step - loss: 0.6232 - accuracy: 0.6337 - val_loss: 0.5982 - val_accuracy: 0.6693\n",
      "Epoch 13/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6166 - accuracy: 0.6397Epoch 1/50\n",
      "195/195 [==============================] - 20s 103ms/step - loss: 0.5932 - accuracy: 0.6790\n",
      "195/195 [==============================] - 76s 388ms/step - loss: 0.6170 - accuracy: 0.6397 - val_loss: 0.5932 - val_accuracy: 0.6790\n",
      "Epoch 14/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6144 - accuracy: 0.6393Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5800 - accuracy: 0.6772\n",
      "195/195 [==============================] - 76s 387ms/step - loss: 0.6145 - accuracy: 0.6394 - val_loss: 0.5800 - val_accuracy: 0.6772\n",
      "Epoch 15/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.6023 - accuracy: 0.6558Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5573 - accuracy: 0.6967\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.6019 - accuracy: 0.6558 - val_loss: 0.5573 - val_accuracy: 0.6967\n",
      "Epoch 16/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5934 - accuracy: 0.6663Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5729 - accuracy: 0.6599\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.5935 - accuracy: 0.6663 - val_loss: 0.5729 - val_accuracy: 0.6599\n",
      "Epoch 17/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5864 - accuracy: 0.6724Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5689 - accuracy: 0.6826\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.5865 - accuracy: 0.6725 - val_loss: 0.5689 - val_accuracy: 0.6826\n",
      "Epoch 18/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5768 - accuracy: 0.6816Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5573 - accuracy: 0.6804\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.5769 - accuracy: 0.6816 - val_loss: 0.5573 - val_accuracy: 0.6804\n",
      "Epoch 19/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5717 - accuracy: 0.6829Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5563 - accuracy: 0.7036\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.5716 - accuracy: 0.6830 - val_loss: 0.5563 - val_accuracy: 0.7036\n",
      "Epoch 20/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5657 - accuracy: 0.6845Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5468 - accuracy: 0.7141\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.5656 - accuracy: 0.6845 - val_loss: 0.5468 - val_accuracy: 0.7141\n",
      "Epoch 21/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5619 - accuracy: 0.6879Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5492 - accuracy: 0.7159\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.5617 - accuracy: 0.6880 - val_loss: 0.5492 - val_accuracy: 0.7159\n",
      "Epoch 22/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5560 - accuracy: 0.6942Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5397 - accuracy: 0.7038\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.5560 - accuracy: 0.6943 - val_loss: 0.5397 - val_accuracy: 0.7038\n",
      "Epoch 23/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5513 - accuracy: 0.7098Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5317 - accuracy: 0.7196\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.5513 - accuracy: 0.7098 - val_loss: 0.5317 - val_accuracy: 0.7196\n",
      "Epoch 24/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5488 - accuracy: 0.6984Epoch 1/50\n",
      "195/195 [==============================] - 20s 102ms/step - loss: 0.5315 - accuracy: 0.7128\n",
      "195/195 [==============================] - 75s 385ms/step - loss: 0.5488 - accuracy: 0.6984 - val_loss: 0.5315 - val_accuracy: 0.7128\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/195 [============================>.] - ETA: 0s - loss: 0.5414 - accuracy: 0.7051Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5246 - accuracy: 0.7248\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5418 - accuracy: 0.7051 - val_loss: 0.5246 - val_accuracy: 0.7248\n",
      "Epoch 26/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5420 - accuracy: 0.7104Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5510 - accuracy: 0.6737\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5421 - accuracy: 0.7104 - val_loss: 0.5510 - val_accuracy: 0.6737\n",
      "Epoch 27/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5344 - accuracy: 0.7116Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5356 - accuracy: 0.6960\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5344 - accuracy: 0.7116 - val_loss: 0.5356 - val_accuracy: 0.6960\n",
      "Epoch 28/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5307 - accuracy: 0.7176Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5226 - accuracy: 0.7271\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5307 - accuracy: 0.7176 - val_loss: 0.5226 - val_accuracy: 0.7271\n",
      "Epoch 29/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5277 - accuracy: 0.7229Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5649 - accuracy: 0.6606\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5280 - accuracy: 0.7229 - val_loss: 0.5649 - val_accuracy: 0.6606\n",
      "Epoch 30/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5231 - accuracy: 0.7194Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5308 - accuracy: 0.6963\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5230 - accuracy: 0.7195 - val_loss: 0.5308 - val_accuracy: 0.6963\n",
      "Epoch 31/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5210 - accuracy: 0.7220Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5151 - accuracy: 0.7154\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5210 - accuracy: 0.7221 - val_loss: 0.5151 - val_accuracy: 0.7154\n",
      "Epoch 32/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5151 - accuracy: 0.7321Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5231 - accuracy: 0.7044\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5151 - accuracy: 0.7321 - val_loss: 0.5231 - val_accuracy: 0.7044\n",
      "Epoch 33/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5141 - accuracy: 0.7314Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4998 - accuracy: 0.7379\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5141 - accuracy: 0.7315 - val_loss: 0.4998 - val_accuracy: 0.7379\n",
      "Epoch 34/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5067 - accuracy: 0.7405Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4944 - accuracy: 0.7515\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5067 - accuracy: 0.7405 - val_loss: 0.4944 - val_accuracy: 0.7515\n",
      "Epoch 35/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5016 - accuracy: 0.7401Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4993 - accuracy: 0.7329\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5021 - accuracy: 0.7401 - val_loss: 0.4993 - val_accuracy: 0.7329\n",
      "Epoch 36/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.5000 - accuracy: 0.7468Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4903 - accuracy: 0.7502\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.5002 - accuracy: 0.7468 - val_loss: 0.4903 - val_accuracy: 0.7502\n",
      "Epoch 37/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4941 - accuracy: 0.7455Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5003 - accuracy: 0.7404\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4942 - accuracy: 0.7455 - val_loss: 0.5003 - val_accuracy: 0.7404\n",
      "Epoch 38/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4949 - accuracy: 0.7530Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4905 - accuracy: 0.7609\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4949 - accuracy: 0.7530 - val_loss: 0.4905 - val_accuracy: 0.7609\n",
      "Epoch 39/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4826 - accuracy: 0.7520Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4839 - accuracy: 0.7558\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4824 - accuracy: 0.7521 - val_loss: 0.4839 - val_accuracy: 0.7558\n",
      "Epoch 40/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4776 - accuracy: 0.7567Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4808 - accuracy: 0.7518\n",
      "195/195 [==============================] - 74s 380ms/step - loss: 0.4772 - accuracy: 0.7567 - val_loss: 0.4808 - val_accuracy: 0.7518\n",
      "Epoch 41/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4734 - accuracy: 0.7649Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.5009 - accuracy: 0.7290\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4735 - accuracy: 0.7649 - val_loss: 0.5009 - val_accuracy: 0.7290\n",
      "Epoch 42/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4708 - accuracy: 0.7633Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4840 - accuracy: 0.7515\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4704 - accuracy: 0.7633 - val_loss: 0.4840 - val_accuracy: 0.7515\n",
      "Epoch 43/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4645 - accuracy: 0.7636Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4793 - accuracy: 0.7551\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4650 - accuracy: 0.7636 - val_loss: 0.4793 - val_accuracy: 0.7551\n",
      "Epoch 44/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4635 - accuracy: 0.7681Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4757 - accuracy: 0.7612\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4635 - accuracy: 0.7681 - val_loss: 0.4757 - val_accuracy: 0.7612\n",
      "Epoch 45/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4544 - accuracy: 0.7778Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4992 - accuracy: 0.7330\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4543 - accuracy: 0.7778 - val_loss: 0.4992 - val_accuracy: 0.7330\n",
      "Epoch 46/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4577 - accuracy: 0.7733Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4607 - accuracy: 0.7730\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4576 - accuracy: 0.7733 - val_loss: 0.4607 - val_accuracy: 0.7730\n",
      "Epoch 47/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4481 - accuracy: 0.7823Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4602 - accuracy: 0.7763\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4482 - accuracy: 0.7823 - val_loss: 0.4602 - val_accuracy: 0.7763\n",
      "Epoch 48/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.7850Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4664 - accuracy: 0.7671\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4403 - accuracy: 0.7850 - val_loss: 0.4664 - val_accuracy: 0.7671\n",
      "Epoch 49/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4389 - accuracy: 0.7837Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4883 - accuracy: 0.7462\n",
      "195/195 [==============================] - 74s 379ms/step - loss: 0.4391 - accuracy: 0.7837 - val_loss: 0.4883 - val_accuracy: 0.7462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "194/195 [============================>.] - ETA: 0s - loss: 0.4318 - accuracy: 0.7932Epoch 1/50\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4633 - accuracy: 0.7741\n",
      "195/195 [==============================] - 74s 380ms/step - loss: 0.4319 - accuracy: 0.7932 - val_loss: 0.4633 - val_accuracy: 0.7741\n",
      "195/195 [==============================] - 19s 99ms/step - loss: 0.4633 - accuracy: 0.7741\n"
     ]
    }
   ],
   "source": [
    "class MyRNN(keras.Model):\n",
    "    # Cell方式构建多层网络\n",
    "    def __init__(self, units):\n",
    "        super(MyRNN, self).__init__() \n",
    "        # 词向量编码 [b, 80] => [b, 80, 100]\n",
    "        self.embedding = layers.Embedding(total_words, embedding_len,\n",
    "                                          input_length=max_review_len,\n",
    "                                          trainable=False)\n",
    "        self.embedding.build(input_shape=(None,max_review_len))\n",
    "        # self.embedding.set_weights([embedding_matrix])\n",
    "        # 构建RNN\n",
    "        self.rnn = keras.Sequential([\n",
    "            layers.LSTM(units, dropout=0.5, return_sequences=True),\n",
    "            layers.LSTM(units, dropout=0.5)\n",
    "        ])\n",
    "        # 构建分类网络，用于将CELL的输出特征进行分类，2分类\n",
    "        # [b, 80, 100] => [b, 64] => [b, 1]\n",
    "        self.outlayer = Sequential([\n",
    "            layers.Dense(32),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.ReLU(),\n",
    "            layers.Dense(1)])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs # [b, 80]\n",
    "        # embedding: [b, 80] => [b, 80, 100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
    "        x = self.rnn(x)\n",
    "        # 末层最后一个输出作为分类网络的输入: [b, 64] => [b, 1]\n",
    "        x = self.outlayer(x,training)\n",
    "        # p(y is pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob\n",
    "\n",
    "def main():\n",
    "    units = 512 # RNN状态向量长度f\n",
    "    epochs = 50 # 训练epochs\n",
    "\n",
    "    model = MyRNN(units)\n",
    "    # 装配\n",
    "    model.compile(optimizer = optimizers.Adam(0.001),\n",
    "                  loss = losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function = False)\n",
    "    # 训练和验证\n",
    "    model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "    # 测试\n",
    "    model.evaluate(db_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它部分均保持一致。我们可以简单地比较通过预训练的 GloVe 模型初始化的 Embedding 层的训练结果和随机初始化的 Embedding 层的训练结果，在训练完 50 个 Epochs 后，预训 练模型的准确率达到了 84.7%，提升了约 2%。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
