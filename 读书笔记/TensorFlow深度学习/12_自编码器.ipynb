{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "\n",
    "# 加载TF模块\n",
    "from tensorflow.keras import layers,Sequential,losses,optimizers,datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. 自编码器\n",
    "前面我们介绍了在给出样本及其的标签的情况下，神经网络如何学习的算法，这类算法需要学习的是在给定样本$x$下的条件概率$P(y|x)$。现在获取海量的样本数据是相对容易的，困难的是获取这些数据所对应的标签信息。例如机器翻译，除了收集源语言的对话文本外，还需要待翻译的目标语言文本数据。数据的标注工作目前主要还是依赖人的`先验知识`(Prior Knowledge)来完成。深度学习所需要的数据规模一般非常大，这种强依赖人工完成数据标注的方式代价较高，而且不可避免地引入标注人员的主观先验偏差。\n",
    "\n",
    "面对海量的无标注数据$x$，有没有办法能够从中学习到数据的分布$P(x)$的算法？这就是本章要介绍的`无监督学习`(Unsupervised Learning)算法。特别地，如果算法把$x$作为监督信号来学习，这类算法称为`自监督学习`(Self-supervised Learning)，本章要介绍的自编码器算法就是属于自监督学习范畴。\n",
    "\n",
    "### 12.1 自编码器原理\n",
    "让我们来考虑`监督学习`中神经网络的功能：\n",
    "+ $\\displaystyle o = f_{\\theta}, x \\in R^{d_{in}}, o \\in R^{d_{out}}$\n",
    "\n",
    "$d_{in}$是输入的特征向量长度，$d_{out}$是网络输出的向量长度。对于分类问题，网络模型通过把长度为$d_{in}$输入特征向量$x$变换到长度为$d_{out}$的输出向量$o$，这个过程可以看成是`特征降维`。最常见的降维算法有`主成分分析法`(Principal components analysis，简称PCA)，通过对协方差矩阵进行特征分解而得到数据的主要成分，但是`PCA`本质上是一种线性变换，提取特征的能力极为有限。\n",
    "\n",
    "能不能利用神经网络的强大非线性表达能力去学习到低维的数据表示呢？问题的关键在于，训练神经网络一般需要一个显式的标签数据(或监督信号)，但是无监督的数据没有额外的标注信息，只有数据$x$本身。\n",
    "\n",
    "于是，我们尝试着利用数据$x$本身作为监督信号来指导网络的训练，即希望神经网络能够学习到映射$f_{\\theta}:x\\to x$。我们把网络$f_{\\theta}$切分为两个部分：\n",
    "+ 前面的子网络尝试学习映射关系:$g_{\\theta1}:x\\to z$\n",
    "+ 后面的子网络尝试学习映射关系:$h_{\\theta2}:z\\to x$\n",
    "\n",
    "如`图12.1`所示：\n",
    "\n",
    "<img src=\"images/12_01.png\" style=\"width:200px;\"/>\n",
    "\n",
    "我们把$g_{\\theta1}$看成一个数据`编码`(Encode)的过程，把高维度的输入$x$编码成低维度的隐变量$z$(Latent Variable，或隐藏变量)，称为`Encoder`网络(编码器)；$h_{\\theta2}$看成数据`解码`(Decode)的过程，把编码过后的输入$z$解码为高维度的$x$，称为`Decoder`网络(解码器)。\n",
    "\n",
    "编码器和解码器共同完成了输入数据$x$的编码和解码过程，我们把整个网络模型$f_{\\theta}$叫做`自编码器`(Auto-Encoder)。如果使用深层神经网络来参数化$g_{\\theta1}$和$h_{\\theta2}$函数，则称为`深度自编码器`(Deep Auto-encoder)，如`图12.2`所示。\n",
    "\n",
    "<img src=\"images/12_02.png\" style=\"width:500px;\"/>\n",
    "\n",
    "自编码器能够将输入变换到隐藏向量$z$，并通过解码器`重建`(Reconstruct，或恢复)出$x$。我们希望解码器的输出能够完美地或者近似恢复出原来的输入，即$\\bar{x} \\approx x$，那么，自编码器的优化目标可以写成：\n",
    "+ $\\text{Minimize} \\mathcal{L} = \\text{dist}(x,\\bar{x})$\n",
    "+ $\\bar{x} = h_{\\theta2}\\big(g_{\\theta1}(x)\\big)$\n",
    "\n",
    "其中$\\text{dist}(x,\\bar{x})$表示$x$和$\\bar{x}$的距离度量，称为`重建误差函数`。最常见的度量方法有欧氏距离：\n",
    "+ $\\mathcal{L} = \\displaystyle\\sum_{i}(x_i-\\bar{x}_i)^2$\n",
    "\n",
    "它和均方误差原理上是等价的。自编码器网络和普通的神经网络并没有本质的区别，只不过训练的监督信号由标签$y$变成了自身$x$。借助于深层神经网络的非线性特征提取能力，自编码器可以获得良好的数据表示，相对于`PCA`等线性方法，自编码器性能更加优秀。\n",
    "\n",
    "在`图12.3(a)`中，第1行是随机采样自测试集的真实MNIST手写数字图片，第2、3、4行分别是基于长度为30的隐藏向量，使用`自编码器`、`Logistic PCA`和`标准PCA`算法恢复出的重建样本图片；在`图12.3(b)`中，第1行为真实的人像图片，第2、3行分别是基于长度为30的隐藏向量，使用`自编码器`和`标准PCA`算法恢复出的重建样本。可以看到，使用深层神经网络的自编码器重建出图片还原度较高。\n",
    "\n",
    "<img src=\"images/12_03.png\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Fashion MNIST图片重建实战\n",
    "我们基于`Fashsion MNIST`数据集进行图片重建实战。\n",
    "\n",
    "### 12.2.1 Fashion MNIST数据集\n",
    "`Fashion MNIST`包含了10类不同类型的衣服、鞋子、包等灰度图片，图片大小为$28\\times28$，共70000张图片，其中60000张用于训练集，10000张用于测试集，如`图12.4`所示。\n",
    "\n",
    "<img src=\"images/12_04.png\" style=\"width:350px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 512\n",
    "h_dim = 20\n",
    "lr = 1e-3\n",
    "\n",
    "# 加载Fashion MNIST图片数据集\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 归一化\n",
    "x_train, x_test = x_train.astype(np.float32)/255., x_test.astype(np.float32)/255.\n",
    "\n",
    "# 只需要通过图片数据即可构建数据集对象，不需要标签\n",
    "train_db = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_db = train_db.shuffle(batchsz * 5).batch(batchsz)\n",
    "\n",
    "# 构建测试集对象\n",
    "test_db = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "test_db = test_db.batch(batchsz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.2 编码器/解码器\n",
    "我们利用编码器将输入图片$x \\in R^{784}$降维到较低维度的隐藏向量：$h \\in R^{20}$，并基于隐藏向量$h$利用解码器重建图片。自编码器模型如`图12.5`所示：\n",
    "\n",
    "<img src=\"images/12_05.png\" style=\"width:500px;\"/>\n",
    "\n",
    "编码器由3层全连接层网络组成，输出节点数分别为256、128、20；解码器同样由3层全连接网络组成，输出节点数分别为128、256、784。\n",
    "\n",
    "首先是编码器子网络的实现。利用3层的神经网络将长度为784的图片向量数据依次降维到256、128，最后降维到$h_{dim}$维度，每层使用`ReLU`激活函数，最后一层不使用激活函数。\n",
    "\n",
    "然后再来创建解码器子网络，这里基于隐藏向量$h_{dim}$依次升维到128、256、784长度，除最后一层，激活函数使用`ReLU`函数。解码器的输出为784长度的向量，通过`Reshape`操作恢复为图片矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(keras.Model):\n",
    "    # 自编码器模型类，包含了Encoder和Decoder 2个子网络\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        # Encoders\n",
    "        self.encoder = Sequential([\n",
    "            layers.Dense(256, activation=tf.nn.relu),\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(64, activation=tf.nn.relu),\n",
    "            layers.Dense(h_dim)\n",
    "        ])\n",
    "        # Decoders\n",
    "        self.decoder = Sequential([\n",
    "            layers.Dense(64, activation=tf.nn.relu),\n",
    "            layers.Dense(128, activation=tf.nn.relu),\n",
    "            layers.Dense(256, activation=tf.nn.relu),\n",
    "            layers.Dense(784)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # [b, 784] => [b, 20]\n",
    "        h = self.encoder(inputs)\n",
    "        # [b, 20] => [b, 784]\n",
    "        x_hat = self.decoder(h)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.5 网络训练\n",
    "自编码器的训练过程与分类器的基本一致，通过误差函数计算出重建向量$\\bar{x}$与原始输入向量$x$之间的距离，再利用`TensorFlow`的自动求导机制同时求出`encoder`和`decoder`的梯度，循环更新即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      multiple                  243412    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    multiple                  244176    \n",
      "=================================================================\n",
      "Total params: 487,588\n",
      "Trainable params: 487,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建网络对象\n",
    "model = AE()\n",
    "# 指定输入大小\n",
    "model.build(input_shape=(4, 784))\n",
    "# 打印网络信息\n",
    "model.summary()\n",
    "# 创建优化器，并设置学习率\n",
    "optimizer = optimizers.Adam(lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里固定训练100个`Epoch`，每次通过前向计算获得重建图片向量，并利用`tf.nn.sigmoid_cross_entropy_with_logits`损失函数计算重建图片与原始图片直接的误差，实际上利用`MSE`误差函数也是可行的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 117 0.33658719062805176\n",
      "1 117 0.3122255504131317\n",
      "2 117 0.30539190769195557\n",
      "3 117 0.3164972960948944\n",
      "4 117 0.29966506361961365\n",
      "5 117 0.3028705418109894\n",
      "6 117 0.29469549655914307\n",
      "7 117 0.28039780259132385\n",
      "8 117 0.29256802797317505\n",
      "9 117 0.3064577281475067\n",
      "10 117 0.28608107566833496\n",
      "11 117 0.2972445785999298\n",
      "12 117 0.27093005180358887\n",
      "13 117 0.29275402426719666\n",
      "14 117 0.2827445864677429\n",
      "15 117 0.29868945479393005\n",
      "16 117 0.2867629826068878\n",
      "17 117 0.29393208026885986\n",
      "18 117 0.285297691822052\n",
      "19 117 0.2784631550312042\n",
      "20 117 0.28337258100509644\n",
      "21 117 0.27688172459602356\n",
      "22 117 0.2894863188266754\n",
      "23 117 0.282692015171051\n",
      "24 117 0.2930589020252228\n",
      "25 117 0.2911820709705353\n",
      "26 117 0.2721794843673706\n",
      "27 117 0.27247190475463867\n",
      "28 117 0.2602325677871704\n",
      "29 117 0.27335622906684875\n",
      "30 117 0.27941298484802246\n",
      "31 117 0.28101953864097595\n",
      "32 117 0.28674423694610596\n",
      "33 117 0.29272356629371643\n",
      "34 117 0.2892412841320038\n",
      "35 117 0.273915559053421\n",
      "36 117 0.2757524251937866\n",
      "37 117 0.2720988690853119\n",
      "38 117 0.28338170051574707\n",
      "39 117 0.2744380831718445\n",
      "40 117 0.282144159078598\n",
      "41 117 0.2936348021030426\n",
      "42 117 0.2748746871948242\n",
      "43 117 0.27740001678466797\n",
      "44 117 0.2714516818523407\n",
      "45 117 0.26371094584465027\n",
      "46 117 0.2796797752380371\n",
      "47 117 0.2682528495788574\n",
      "48 117 0.2655391991138458\n",
      "49 117 0.27111390233039856\n",
      "50 117 0.28892019391059875\n",
      "51 117 0.2645397186279297\n",
      "52 117 0.2766726613044739\n",
      "53 117 0.2669178545475006\n",
      "54 117 0.2747781574726105\n",
      "55 117 0.2655070126056671\n",
      "56 117 0.27162155508995056\n",
      "57 117 0.27373814582824707\n",
      "58 117 0.25926199555397034\n",
      "59 117 0.28336817026138306\n",
      "60 117 0.2815343737602234\n",
      "61 117 0.2704349160194397\n",
      "62 117 0.28400206565856934\n",
      "63 117 0.27507495880126953\n",
      "64 117 0.27009037137031555\n",
      "65 117 0.2661428153514862\n",
      "66 117 0.2690109312534332\n",
      "67 117 0.2787996530532837\n",
      "68 117 0.2770515978336334\n",
      "69 117 0.2547883689403534\n",
      "70 117 0.289934366941452\n",
      "71 117 0.24718159437179565\n",
      "72 117 0.26774486899375916\n",
      "73 117 0.2695326507091522\n",
      "74 117 0.27216771245002747\n",
      "75 117 0.28467586636543274\n",
      "76 117 0.26442697644233704\n",
      "77 117 0.2573782503604889\n",
      "78 117 0.27226904034614563\n",
      "79 117 0.27814245223999023\n",
      "80 117 0.26108112931251526\n",
      "81 117 0.2783409059047699\n",
      "82 117 0.2689497470855713\n",
      "83 117 0.2821345031261444\n",
      "84 117 0.2815946340560913\n",
      "85 117 0.26803550124168396\n",
      "86 117 0.29224786162376404\n",
      "87 117 0.2646870017051697\n",
      "88 117 0.27324992418289185\n",
      "89 117 0.262052983045578\n",
      "90 117 0.2829347848892212\n",
      "91 117 0.2948170304298401\n",
      "92 117 0.278665155172348\n",
      "93 117 0.27676400542259216\n",
      "94 117 0.27940642833709717\n",
      "95 117 0.26793429255485535\n",
      "96 117 0.281540185213089\n",
      "97 117 0.28134316205978394\n",
      "98 117 0.28001928329467773\n",
      "99 117 0.26936963200569153\n"
     ]
    }
   ],
   "source": [
    "def save_images(imgs, name):\n",
    "    new_im = Image.new('L', (280, 280))\n",
    "    index = 0\n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = imgs[index]\n",
    "            im = Image.fromarray(im, mode='L')\n",
    "            new_im.paste(im, (i, j))\n",
    "            index += 1\n",
    "    new_im.save(name)\n",
    "\n",
    "# 训练100个Epoch\n",
    "for epoch in range(100): \n",
    "    # 遍历训练集\n",
    "    for step, x in enumerate(train_db): \n",
    "        # 打平，[b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        # 构建梯度记录器\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 前向计算获得重建的图片\n",
    "            x_rec_logits = model(x)\n",
    "            # 计算重建图片与输入之间的损失函数\n",
    "            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_rec_logits)\n",
    "            # 计算均值\n",
    "            rec_loss = tf.reduce_mean(rec_loss)\n",
    "        # 自动求导，包含了2个子网络的梯度\n",
    "        grads = tape.gradient(rec_loss, model.trainable_variables)\n",
    "        # 自动更新，同时更新 2 个子网络\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # 间隔性打印训练误差\n",
    "    print(epoch, step, float(rec_loss))\n",
    "    # 重建图片，从测试集采样一批图片\n",
    "    x = next(iter(test_db))\n",
    "    logits = model(tf.reshape(x, [-1, 784]))\n",
    "    x_hat = tf.sigmoid(logits)\n",
    "    # [b, 784] => [b, 28, 28]\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28])\n",
    "    # [b, 28, 28] => [2b, 28, 28]\n",
    "    x_concat = tf.concat([x, x_hat], axis=0)\n",
    "    x_concat = x_hat\n",
    "    x_concat = x_concat.numpy() * 255.\n",
    "    x_concat = x_concat.astype(np.uint8)\n",
    "    # save_images(x_concat, '/home/alex/temp/rec_epoch_%d.png'%epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图片重建的效果如`图12.6`、`图12.7`、`图12.8`所示，其中每张图片的左边5列为真实图片，右边 5 列为对应的重建图片。可以看到，随着训练的进行，重建图片边缘越来越清晰。\n",
    "\n",
    "<img src=\"images/12_06.png\" style=\"width:600px;\"/>\n",
    "\n",
    "## 12.3 自编码器变种\n",
    "一般而言，自编码器网络的训练较为稳定，但是由于损失函数是直接度量重建样本与真实样本的底层特征之间的距离，而不是评价重建样本的逼真度和多样性等抽象指标，因此在某些任务上效果一般，如图片重建，容易出现重建图片边缘模糊，逼真度相对真实图片仍有不小差距。为了尝试让自编码器学习到数据的真实分布，产生了一系列的自编码器变种网络。本节介绍几种典型的自编码器变种模型。\n",
    "\n",
    "### 12.3.1 Denoising Auto-Encoder\n",
    "为了防止神经网络记忆住输入数据的底层特征，`Denoising Auto-Encoders`给输入数据添加随机的噪声扰动，如给输入$x$添加采样自高斯分布的噪声$\\epsilon$：\n",
    "+ $\\tilde{x} = x + \\epsilon, \\epsilon \\approx \\mathcal{N}(0, \\text{var})$\n",
    "\n",
    "添加噪声后，网络需要从$\\tilde{x}$学习到数据的真实隐藏变量$z$，并还原出原始的输入$x$，如`图12.9`所示：\n",
    "\n",
    "<img src=\"images/12_09.png\" style=\"width:600px;\"/>\n",
    "\n",
    "模型的优化目标为：\n",
    "+ $\\displaystyle\\theta^{*} = \\underbrace{\\arg\\min}_{\\theta}\\text{dist}\\bigg(h_{\\theta2}\\big(g_{\\theta1}(\\tilde{x}),x\\big)\\bigg)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2 Dropout Auto-Encoder\n",
    "自编码器网络同样面临过拟合的风险，`Dropout Auto-Encoder`通过随机断开网络的连接防止过拟合。`Dropout Auto-Encoder`实现非常简单，通过在网络层中插入`Dropout`层即可实现网络连接的随机断开。\n",
    "\n",
    "### 12.3.3 Adversarial Auto-Encoder\n",
    "为了能够方便地从某个已知的先验分布中$p(z)$采样隐藏变量$z$，方便利用$p(z)$来重建输入，`对抗自编码器`(Adversarial Auto-Encoder)利用额外的`判别器网络`(Discriminator，简称D网络)来判定降维的隐藏变量$z$是否采样自先验分布$p(z)$，如`图12.10`所示。\n",
    "\n",
    "<img src=\"images/12_10.png\" style=\"width:500px;\"/>\n",
    "\n",
    "判别器网络的输出为一个属于$[0,1]$区间的变量，表征隐藏向量是否采样自先验分布$p(z)$：\n",
    "+ 所有采样自先验分布$p(z)$的$z$标注为真\n",
    "+ 采样自编码器的条件概率$q(z|x)$的$z$标注为假\n",
    "\n",
    "通过这种方式训练，除了可以重建样本，还可以约束条件概率分布$q(z|x)$逼近先验分布$p(z)$。\n",
    "\n",
    "对抗自编码器是从下一章要介绍的生成对抗网络算法衍生而来，在学习完对抗生成网络后可以加深对对抗自编码器的理解。\n",
    "\n",
    "\n",
    "## 12.4 变分自编码器\n",
    "基本的自编码器本质上是学习输入$x$和隐藏变量$z$之间映射关系，它是一个`判别模型`(Discriminative model)，并不是`生成模型`(Generative model)。那么能不能将自编码器调整为生成模型，方便地生成样本呢？\n",
    "\n",
    "给定隐藏变量的分布$P(z)$，如果可以学习到条件概率分布$P(x|z)$，则通过对联合概率分布$P(x,z) = P(x|z)P(z)$进行采样，生成不同的样本。`变分自编码器`(Variational AutoEncoders，简称`VAE`)就可以实现此目的，如`图12.11`所示。\n",
    "\n",
    "<img src=\"images/12_11.png\" style=\"width:500px;\"/>\n",
    "\n",
    "如果从神经网络的角度来理解的话，`VAE`和前面的自编码器一样，非常直观好理解；但是`VAE`的理论推导稍复杂，接下来我们先从神经网络的角度去阐述`VAE`，再从概率角度去推导`VAE`。\n",
    "\n",
    "从神经网络的角度来看，`VAE`相对于自编码器模型，同样具有编码器和解码器两个子网络。解码器接受输入$x$，输出为隐变量$z$；解码器负责将隐变量$z$解码为重建的$x$。不同的是，`VAE`模型对隐变量$z$的分布有显式地约束，希望隐变量$z$符合预设的先验分布$P(z)$。因此，在损失函数的设计上，除了原有的重建误差项外，还添加了隐变量$z$分布的约束项。\n",
    "\n",
    "**关于`变分自编码器`后续内容用到再说，本章就读到这里吧！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
